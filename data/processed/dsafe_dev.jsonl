{"id": "1032_2", "doc_id": "1032", "text": "2. SCOPE\nAgency adoption of AI poses many challenges, some novel and specific to AI and some well-known. While agencies must give due attention to all aspects of AI, this memorandum is more narrowly scoped to address a subset of AI risks, as well as governance and innovation issues that are directly tied to agencies’ use of AI. The risks addressed in this memorandum result from any reliance on AI outputs to inform, influence, decide, or execute agency decisions or actions, which could undermine the efficacy, safety, equitableness, fairness, transparency, accountability, appropriateness, or lawfulness of such decisions or actions.\nThis memorandum does not address issues that are present regardless of whether AI is used versus any other software, such as issues with respect to Federal information and information systems in general. In addition, this memorandum does not supersede other, more general Federal policies that apply to AI but are not focused specifically on AI, such as policies that relate to enterprise risk management, information resources management, privacy, accessibility, Federal statistical activities, IT, or cybersecurity.\nAgencies must continue to comply with applicable OMB policies in other domains relevant to AI, and to coordinate compliance across the agency with all appropriate officials. All agency responsible officials retain their existing authorities and responsibilities established in other laws and policies.\na. Covered Agencies. Except as specifically noted, this memorandum applies to all agencies defined in 44 U.S.C. § 3502(1).7 As noted in the relevant sections, some requirements in this memorandum apply only to Chief Financial Officers Act (CFO Act) agencies as identified in 31 U.S.C. § 901(b), and other requirements do not apply to elements of the Intelligence Community, as defined in 50 U.S.C. § 3003.\nb. Covered AI. This memorandum provides requirements and recommendations that, as described in more detail below, apply to new and existing AI that is developed, used, or procured by or on behalf of covered agencies. This memorandum does not, by contrast, govern:\ni. agencies’ regulatory actions designed to prescribe law or policy regarding non-agency uses of AI;\nii. agencies’ evaluations of particular AI applications because the AI provider is the target or potential target of a regulatory enforcement, law enforcement, or national security action;8\niii. agencies’ development of metrics, methods, and standards to test and measure AI, where such metrics, methods, and standards are for use by the general public or the government as a whole, rather than to test AI for a particular agency application9; or\niv. agencies’ use of AI to carry out basic research or applied research, except where the purpose of such research is to develop particular AI applications within the agency.\nThe requirements and recommendations of this memorandum apply to system functionality that implements or is reliant on AI, rather than to the entirety of an information system that incorporates AI. As noted in the relevant sections, some requirements in this memorandum apply only in specific circumstances in which agencies use AI, such as when the AI may impact rights or safety.\nc. Applicability to National Security Systems. This memorandum does not cover AI when it is being used as a component of a National Security System.10", "tags": ["Risk factors: Safety", "Risk factors: Transparency", "Risk factors: Reliability", "Applications: Government: military and public safety"], "source": "https://www.whitehouse.gov/wp-content/uploads/2024/03/M-24-10-Advancing-Governance-Innovation-and-Risk-Management-for-Agency-Use-of-Artificial-Intelligence.pdf", "official_name": "Advancing Governance, Innovation, and Risk Management for Agency Use of Artificial Intelligence", "label": "safe"}
{"id": "1032_8", "doc_id": "1032", "text": "d. AI Sharing and Collaboration\nOpenness, sharing, and reuse of AI significantly enhance both innovation and transparency, and must also be done responsibly to avoid undermining the rights, safety, and security of the public. Agencies must share their AI code, models, and data, and do so in a manner that facilitates re-use and collaboration Government-wide and with the public, subject to applicable law, governmentwide guidance, and the following considerations:\ni. Sharing and Releasing AI Code and Models. Agencies must proactively share their custom-developed code20—including models and model weights—for AI applications in active use and must release and maintain that code as open source software on a public repository,21 unless:\nA. the sharing of the code is restricted by law or regulation, including patent or intellectual property law, the Export Asset Regulations, the International Traffic in Arms Regulations, and Federal laws and regulations governing classified information;\nB. the sharing of the code would create an identifiable risk to national security, confidentiality of Government information, individual privacy, or the rights or safety of the public;\nC. the agency is prevented by a contractual obligation from doing so; or\nD. the sharing of the code would create an identifiable risk to agency mission,\nprograms, or operations, or to the stability, security, or integrity of an agency’s systems or personnel.\nAgencies should prioritize sharing custom-developed code, such as commonly used packages or functions, that has the greatest potential for re-use by other agencies or the public.\nii. Sharing and Releasing AI Data Assets. Data used to develop and test AI is likely to constitute a “data asset” for the purposes of implementing the Open, Public, Electronic\nand Necessary (OPEN) Government Data Act,22 and agencies must, if required by that Act and pursuant to safety and security considerations in Section 4.7 of Executive Order 14110, release such data assets publicly as open government data assets.23 When sharing AI data assets, agencies should promote data interoperability, including by coordinating internally and with other relevant agencies on interoperability criteria and using standardized data formats where feasible and appropriate.\niii. Partial Sharing and Release. Where some portion of an AI project’s code, models, or data cannot be shared or released publicly pursuant to subsections (i) and (ii) of this section, the rest should still be shared or released where practicable, such as by releasing the data used to evaluate a model even if the model itself cannot be safely released, or by sharing a model within the Federal Government even if the model cannot be publicly released. Where code, models, or data cannot be released without restrictions on who can access it, agencies should also, where practicable, share them through Federally controlled infrastructure that allows controlled access by entities outside the Federal Government, such as via the National AI Research Resource.\niv. Procuring AI for Sharing and Release. When procuring custom-developed code for AI, data to train and test AI, and enrichments to existing data (such as labeling services), agencies are encouraged to do so in a manner that allows for the sharing and public release of the relevant code, models, and data.\nv. Unintended Disclosure of Data from AI Models. When agencies are deciding whether to share and release AI models and model weights, they should assess the risk that the models can be induced to reveal sensitive details of the data used to develop them. Agencies’ assessment of risk should include a model-specific risk analysis.24", "tags": ["Risk factors: Safety", "Risk factors: Bias", "Risk factors: Security", "Applications: Government: military and public safety", "Strategies: Input controls: Data circulation", "Strategies: Input controls: Data use"], "source": "https://www.whitehouse.gov/wp-content/uploads/2024/03/M-24-10-Advancing-Governance-Innovation-and-Risk-Management-for-Agency-Use-of-Artificial-Intelligence.pdf", "official_name": "Advancing Governance, Innovation, and Risk Management for Agency Use of Artificial Intelligence", "label": "safe"}
{"id": "1054_1", "doc_id": "1054", "text": "Directive on Automated Decision-Making\n1. Effective date\n1.1 This directive takes effect on April 1, 2019, with compliance required by no later than April 1, 2020.\n1.2 This directive applies to all automated decision systems developed or procured after April 1, 2020. However,\n1.2.1 existing systems developed or procured prior to April 25, 2023 will have until April 25, 2024 to fully transition to the requirements in subsections 6.2.3, 6.3.1, 6.3.4, 6.3.5 and 6.3.6 in this directive;\n1.2.2 new systems developed or procured after April 25, 2023 will have until October 25, 2023 to meet the requirements in this directive.\n1.3 This directive will be reviewed every two years, and as determined by the Chief Information Officer of Canada.\n1. Authorities\n\t2.1 This directive is issued pursuant to the same authority indicated in section 2 of the Policy on Service and Digital.\n1. Definitions\n\t3.1 Definitions to be used in the interpretation of this directive are listed in Appendix A.\n1. Objectives and expected results\n4.1 The objective of this directive is to ensure that automated decision systems are deployed in a manner that reduces risks to clients, federal institutions and Canadian society, and leads to more efficient, accurate, consistent and interpretable decisions made pursuant to Canadian law.\n4.2 The expected results of this directive are as follows:\n4.2.1 Decisions made by federal institutions are data-driven, responsible and comply with procedural fairness and due process requirements.\n4.2.2 Impacts of algorithms on administrative decisions are assessed and negative outcomes are reduced, when encountered.\n4.2.3 Data and information on the use of automated decision systems in federal institutions are made available to the public, where appropriate.\n1. Scope\n5.1 This directive applies to any system, tool, or statistical model used to make an administrative decision or a related assessment about a client.\n5.2 This directive applies only to automated decision systems in production and excludes systems operating in test environments.", "tags": ["Applications: Government: other applications/unspecified", "Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Transparency"], "source": "https://www.tbs-sct.canada.ca/pol/doc-eng.aspx?id=32592", "official_name": "Directive on Automated Decision-Making (Canada)", "label": "safe"}
{"id": "1054_3", "doc_id": "1054", "text": "6.2 Transparency\nProviding notice before decisions\n\t6.2.1 Providing notice through all service delivery channels in use that the decision rendered will be undertaken in whole or in part by an automated decision system, as prescribed in Appendix C.\n\t6.2.2 Providing notices prominently and in plain language, pursuant to the Canada.ca Content Style Guide.\nProviding explanations after decisions\n6.2.3 Providing a meaningful explanation to affected individuals of how and why the decision was made, as prescribed in Appendix C.\nAccess to components\n\t6.2.4 Determining the appropriate licence for software components, including consideration of open source software in accordance with the measures specified in the Government of Canada Enterprise Architecture Framework.\n\t6.2.5 If using a proprietary licence, ensuring that:\n\t6.2.5.1 All released versions of proprietary software components used for automated decision systems are delivered to, and safeguarded by, the department.\n\t6.2.5.2 The Government of Canada retains the right to access and test automated decision systems, including all released versions of proprietary software components, in case it is necessary for a specific audit, investigation, inspection, examination, enforcement action, or judicial proceeding, subject to safeguards against unauthorized disclosure.\n\t6.2.5.3 As part of this access, the Government of Canada retains the right to authorize external parties to review and audit these components as necessary.\nRelease of source code\n\t6.2.6 Releasing custom source code owned by the Government of Canada in accordance with the measures specified in the Government of Canada Enterprise Architecture Framework, unless:\n\t6.2.6.1 the source code is processing data classified as Secret, Top Secret or Protected C; or\n\t6.2.6.2 disclosure would otherwise be exempted or excluded under the Access to Information Act, if the Access to Information Act were to apply.\n\t6.2.7 Determining the appropriate access restrictions to the released source code.\nDocumenting decisions\n\t6.2.8 Documenting the decisions of automated decision systems in accordance with the Directive on Service and Digital, and in support of the monitoring (6.3.2), data governance (6.3.4) and reporting requirements (6.5.1).", "tags": ["Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: In standard form", "Risk factors: Security", "Risk factors: Security: Dissemination", "Strategies: Evaluation", "Strategies: Evaluation: External auditing"], "source": "https://www.tbs-sct.canada.ca/pol/doc-eng.aspx?id=32592", "official_name": "Directive on Automated Decision-Making (Canada)", "label": "safe"}
{"id": "1125_4", "doc_id": "1125", "text": "Chapter 3. The Norms of Research and Development\n\n\n10. Strengthen the awareness of self-discipline. Strengthen self-discipline in activities related to AI research and development, actively integrate AI ethics into every phase of technology research and development, consciously carry out self-censorship, strengthen self-management, and do not engage in AI research and development that violates ethics and morality.\n\n\n11. Improve data quality. In the phases of data collection, storage, use, processing, transmission, provision, disclosure, etc., strictly abide by data-related laws, standards and norms. Improve the completeness, timeliness, consistency, normativeness and accuracy of data.\n\n\n12. Enhance safety, security and transparency. In the phases of algorithm design, implementation, and application, etc.,  improve transparency, interpretability, understandability, reliability, and controllability, enhance the resilience, adaptability, and the ability of anti-interference of AI systems, and gradually realize verifiable, auditable, supervisable, traceable, predictable and trustworthy AI.\n\n\n13. Avoid bias and discrimination. During the process of data collection and algorithm development, strengthen ethics review, fully consider the diversity of demands, avoid potential data and algorithmic bias, and strive to achieve inclusivity, fairness and non-discrimination of AI systems.", "tags": ["Risk factors: Interpretability and explainability", "Strategies: Input controls", "Strategies: Input controls: Data use", "Risk factors: Transparency", "Risk factors: Bias", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy"], "source": "https://ai-ethics-and-governance.institute/2021/09/27/the-ethical-norms-for-the-new-generation-artificial-intelligence-china/", "official_name": "Ethical Norms for the New Generation Artificial Intelligence", "label": "safe"}
{"id": "1356_18", "doc_id": "1356", "text": "D.\tRequirements that biometric systems have certain properties:\n1.\tUsing a configurable minimum similarity threshold for candidate results;\n2.\tEnforcing minimum quality criteria for input biometric data or samples used for biometric systems, including data used in reference galleries or training datasets for the system. These criteria should be based on standards set by independent bodies, such as the International Organization for Standardization (ISO)/International Electrotechnical Commission (IEC)’s 29794-5:201;\n3.\tFor a given probe image or data input in use cases using a one-to-many identification search, returning a list of candidate matches above the minimum similarity threshold alongside similarity scores whenever possible; and\n4.\tMaintaining detailed logs of use for auditing and compliance, including capturing input and output data in ways that incorporate appropriate protections for PII and other data throughout the information life cycle, and limiting retention and restricting reuse of PII for other purposes; system configuration parameters; resulting candidate matches, scores, and ordering; and other information as appropriate.\n\niii.\tComply with Civil Rights Laws to Avoid Unlawful Bias, Unlawful Discrimination, and Harmful Outcomes. Many AI systems rely on vast amounts of data. These tools have the potential to produce outcomes that result in unlawful discrimination. Discrimination may come from different sources, including problems with data model opacity and access, and with system design and use. Consistent with the risk management requirements of OMB Memorandum M-24-10, agencies should address risks that procured AI may generate unlawful bias, unlawful discrimination, or harmful outcomes, and require vendors to identify potential AI biases and mitigation strategies to address biases.", "tags": ["Strategies: Performance requirements", "Strategies: Evaluation: External auditing", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Bias", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment"], "source": "https://www.whitehouse.gov/wp-content/uploads/2024/10/M-24-18-AI-Acquisition-Memorandum.pdf", "official_name": "Advancing the Responsible Acquisition of Artificial Intelligence in Government ", "label": "safe"}
{"id": "1356_19", "doc_id": "1356", "text": "c.\t Practices for Managing Performance and Risk for Acquired AI.\n\ni.\tUse Performance-Based Acquisition Techniques that Enable Proactive Risk Management. When acquiring AI, agencies should leverage performance-based approaches and techniques, including through the use of best practices delineated in Appendix I(a) of this memorandum, to strengthen their ability to effectively plan for, identify, and manage risk. Performance-based requirements allow agencies to understand and evaluate vendor claims about their proposed use of AI systems or services prior to contract award, acquire AI capabilities that address their needs, and perform post-award monitoring. Focusing acquisition on achieving desired performance outcomes directly facilitates an agency’s ability to ensure agency needs are met by defining metrics to maintain and improve performance of the AI system or service.\n\nii.\tEnsure Performance Justifies Use. The performance-based requirements identified by an agency, including safeguards against inaccurate outputs, confabulations, drift, and other risks specific to AI, must ensure, to the greatest extent practicable, that the AI system or service to be acquired will be appropriate for the expected use contexts.\n\niii.\tDetermine Appropriate Intellectual Property Rights and Ownership. Consistent with applicable laws and governmentwide policy, an agency must include appropriate contractual terms that clearly delineate the respective ownership and intellectual property (IP) rights of the Government and the contractor. Careful consideration of respective IP licensing rights is even more important when an agency procures an AI system or service, including where agency information is used to train, fine-tune, and develop the AI system.", "tags": ["Strategies: Performance requirements", "Strategies: Evaluation"], "source": "https://www.whitehouse.gov/wp-content/uploads/2024/10/M-24-18-AI-Acquisition-Memorandum.pdf", "official_name": "Advancing the Responsible Acquisition of Artificial Intelligence in Government ", "label": "safe"}
{"id": "1356_20", "doc_id": "1356", "text": "To that end, agencies must develop an approach to IP that considers what rights and deliverables are necessary for the agency to successfully accomplish its mission, protects Federal information used by vendors in the development and operation of AI systems and services for the Federal Government, considers the exploration of open-source development practices of AI code, avoids vendor lock-in, and avoids unnecessary costs. Agencies must scrutinize terms of service and licensing terms, including those that specify what information, models, and transformed agency data should be provided as deliverables, to ensure that they clearly articulate the scope of rights needed by the Government over its own data and any derived products. Furthermore, agencies should conduct careful due diligence to the supply chain of a vendor’s data. Best practices include the following:\nA.\tNegotiating the appropriate scope of licensing rights and other rights that are necessary to accomplish the Government’s mission in the long term while avoiding vendor lock-in. This includes strategically selecting the appropriate FAR or agency supplemental clauses and making affirmative decisions about which alternates to these clauses are necessary. For example, as part of its acquisition planning, an agency may determine it needs unlimited rights to certain contractor deliverables based on its long-term approach to IP. Another agency may determine it requires assignment of copyright to deliverables specified in the contract. In all circumstances, agencies must consider its mission, long-term needs, and larger enterprise architecture while avoiding vendor lock-in and maximizing competition;\nB.\tEnsuring the contract clearly defines the process and timeline for delivery of components needed to operate and monitor the AI system, including as appropriate: data; inputs to the development, testing, and operation process; models; software; other technical components; and documentation as described in the agency’s technical requirements. Contracts should ensure such components and their foundational code remain available for the acquiring agency to access and use for as long as it may be necessary (e.g., to re-train the model);\nC.\tEnsuring complete and timely delivery of information necessary to fulfill requirements of OMB Memorandum M-24-10, including incident reporting. This information should be provided in a machine-readable and native format for ingestion and analysis;\nD.\tRequiring appropriate handling, access, and use of agency information, such as original input data, prompts, processed data, output data, weights, and models, at least in part by providing clear parameters to ensure that such information must only be collected and retained by a vendor when reasonably necessary to serve the intended purposes of the contract; and\nE.\tOpting out of or prohibiting the contractor from using agency data to train AI without an agency’s consent. The contract should permanently prohibit the use of inputted agency data and outputted results to further train publicly or commercially available AI algorithms, including generative AI, consistent with applicable law.", "tags": ["Strategies: Input controls", "Strategies: Input controls: Data use", "Strategies: Disclosure"], "source": "https://www.whitehouse.gov/wp-content/uploads/2024/10/M-24-18-AI-Acquisition-Memorandum.pdf", "official_name": "Advancing the Responsible Acquisition of Artificial Intelligence in Government ", "label": "safe"}
{"id": "1356_23", "doc_id": "1356", "text": "B.\tSoftware Controls to Secure Models. Contract requirements should address vendor compliance with all relevant agency requirements on protecting data, including software controls for privacy and security. Where practicable, vendors should provide detailed documentation of the training procedure used for the model to demonstrate the model’s authenticity, provenance, and security. Trained model artifacts should also be available for agency evaluation and review.\nC.\tEvaluating the Model Design and Any Data Used in Model Training. Agencies should work to ensure contract requirements language establishes baseline expectations of model training using agency data to better understand how a model is expected to iterate using agency data. This is especially critical for capturing additional risks unique to AI, such as data poisoning or data leakage, which might occur when training a model with agency data inputs.\n\nvi.\tOngoing Cost Management. Agencies must leverage early-stage budgeting and financial planning (e.g., in cost estimates) to estimate costs for the ongoing operations and maintenance of AI systems, including post-award oversight, risk management, maintenance, and corrective action to ensure the AI system continues to operate appropriately and as intended.", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Strategies: Evaluation"], "source": "https://www.whitehouse.gov/wp-content/uploads/2024/10/M-24-18-AI-Acquisition-Memorandum.pdf", "official_name": "Advancing the Responsible Acquisition of Artificial Intelligence in Government ", "label": "safe"}
{"id": "1356_24", "doc_id": "1356", "text": "d.\t Practices for Managing Risk and Performance for Rights-Impacting AI and Safety-Impacting AI.\n\nAs noted earlier in this memorandum, for existing contracts for AI systems or services where agency use is rights-impacting or safety-impacting, agencies must update contract terms as needed to comply with applicable requirements from Section 5(c) of OMB Memorandum M- 24-10. This includes updates to contractual terms where an AI system or service was initially acquired for a use that did not impact rights or safety, but subsequently is used or planned to be used in a manner that does impact rights or safety. Agencies must cease use of AI systems or services that impact rights or safety in cases where required risk management practices cannot be sufficiently implemented, as determined by the agency.\ni.\tIdentify When Solicitations Require Compliance for Rights-Impacting and Safety- Impacting AI. Where practicable, agencies must disclose in solicitations whether the planned use is rights-impacting or safety-impacting. In cases where an agency intends to procure AI capacity without full awareness of potential future use cases, the agency should decide during acquisition planning whether to require that any awards support use cases involving rights impacting or safety-impacting AI, and plan accordingly.", "tags": ["Risk factors: Safety", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Strategies: Disclosure"], "source": "https://www.whitehouse.gov/wp-content/uploads/2024/10/M-24-18-AI-Acquisition-Memorandum.pdf", "official_name": "Advancing the Responsible Acquisition of Artificial Intelligence in Government ", "label": "safe"}
{"id": "1356_25", "doc_id": "1356", "text": "ii.\tIncorporate Transparency Requirements into Contractual Terms and Solicitations to Obtain Necessary Information and Access. Agencies must ensure that vendors provide them with the information and documentation necessary to monitor the performance of an AI system or service and implement applicable requirements of OMB Memorandum M-24-10. This may include information about the AI’s functionality and use that may be publicly posted in the agency’s AI use case inventory.\n\nThe level of transparency agencies must require of a vendor, both in the solicitation and evaluation process and through resulting contractual obligations, should be commensurate with the risk and impact of the use case for which the AI system or service will be used. Furthermore, careful consideration should be given to the range of potential agency use cases for the acquired AI system or service, and how the information required to facilitate compliance may depend on whether vendors are developers or deployers of an AI system or service. Agencies must consider whether any or all of the following categories of information must be provided by the vendor to satisfy the requirements of OMB Memorandum M-24-10 or to meet the agency’s objectives:\nA.\tPerformance metrics, including real-world performance for specific sub-groups and demographic groups to surface discriminatory outcomes;\nB.\tInformation about the training data, including the source, provenance, selection, quality, and appropriateness and fitness-for-purpose of the training data, the input features used, time period across which training data was collected, and any filters used;\nC.\tInformation about programmatic evaluations of the AI system or service, including the methodology, design, data, and results of how the evaluation of the program delivering the AI system or service was conducted.;\nD.\tInformation about testing and validation data, including the source, provenance, quality, and appropriateness and fitness-for-purpose of the testing and validation data, the time period across which it was collected, and the extent of overlap or other possible lack of independence from training data;\nE.\tInformation about how input data is used, transformed, and retained by the AI and whether such data is accessible to the vendor;\nF.\tInformation about the AI model(s) integrated into an AI system or service, including the model’s version, capabilities, and mitigations, to the extent it is available to the vendor;\nG.\tThe intended purpose of the AI system or service, known or likely unintended consequences that may occur when deployed for the intended purpose, and known limitations; and\nH.\tData protection metrics or assurance indicators for data in transit and at rest in AI systems.\nIf the agency must obtain any of this information to satisfy legal or policy requirements, then the agency must incorporate requirements for the submission of that information into solicitation and/or contract documents.", "tags": ["Risk factors: Transparency", "Strategies: Performance requirements", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In deployment"], "source": "https://www.whitehouse.gov/wp-content/uploads/2024/10/M-24-18-AI-Acquisition-Memorandum.pdf", "official_name": "Advancing the Responsible Acquisition of Artificial Intelligence in Government ", "label": "safe"}
{"id": "1356_26", "doc_id": "1356", "text": "iii.\tDelineate Responsibilities for Ongoing Testing and Monitoring and Build Evaluations into Vendor Contract Performance. OMB Memorandum M-24-10 generally requires agencies to institute ongoing procedures to monitor degradation of the functionality of AI systems or services and to detect changes in their impact on rights and safety. However, there are instances when a vendor is best equipped to carry out those activities on the agency’s behalf, and so is required under a contract to closely monitor and evaluate the performance and risks of an AI system. In such instances, agencies must still provide oversight and require sufficient information from a vendor to determine compliance with OMB Memorandum M-24-10. Agencies must ensure that contractual terms provide the ability to regularly monitor and evaluate (e.g., on a quarterly or bi- annual basis, based on the needs of the program) performance and risks throughout the duration of the contract. To do so:\nA.\tAgencies must use data defined by the agency (e.g., agency validation and testing datasets) when conducting independent evaluations to ensure the AI system or service is fit for purpose. To the extent practicable, the data used when conducting independent evaluations should not be accessible to the vendor, and should be as similar as possible to the data used when the system is deployed;\nB.\tContracts must require vendors to provide agencies with sufficient access and time to conduct any required testing in a real-world context, including testing carried out by others on behalf of or under agreement with the agency. Alternatively, agencies may require a vendor to regularly provide the results of an AI system or service’s testing in a real-world operational context and the benchmarks used, with sufficient detail such that the testing could be independently verified or reproduced, if practicable;\nC.\tContracts must not prohibit agencies from disclosing how they conduct testing and the results of testing;\nD.\tContracts must detail the examination, testing, and validation procedures the vendor is responsible for and the frequency with which they need to be carried out;\nE.\tWhere appropriate, agency contracts for AI systems or services must also include terms that require vendors to provide the government with the results of performance testing for algorithmic discrimination, including demographic and bias testing, demographic characteristics of groups the performance testing has been conducted on, or third-party evaluations and assessments providing an equivalent level of detail. Alternatively, agencies may require a vendor to provide the results of performance testing to address these issues; and\nF.\tAgencies must also consider how testing and monitoring, including as part of post-award management, impacts financial planning and budgeting requirements in Sections 3(a)(ii) and 4(c)(vi) of this memorandum.", "tags": ["Risk factors: Safety", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Risk factors: Bias"], "source": "https://www.whitehouse.gov/wp-content/uploads/2024/10/M-24-18-AI-Acquisition-Memorandum.pdf", "official_name": "Advancing the Responsible Acquisition of Artificial Intelligence in Government ", "label": "safe"}
{"id": "1356_27", "doc_id": "1356", "text": "iv.\tSet Criteria for Risk Mitigation and Prioritize Performance Improvement. Agencies must have the ability, throughout the entire lifecycle of the contract, to update risk mitigation options and prioritize performance improvement of the AI system or service. To do so, agencies should consider:\n \nA.\tContractual terms that require vendors to regularly monitor an AI system’s performance and rectify any unwanted system behavior, such as retraining the model or adding additional mitigations to the system, based on performance or event-based triggers. To the extent practicable, the cadence of reporting, review, and updating should be determined prior to award of the contract;\nB.\tContractual terms that require vendors to meet performance standards before deploying a new version of its AI system or service in performance of an agency contract or for a vendor to roll-back to a previous version if a new version fails to meet performance standards and requirements;\nC.\tIncentivizing improved model performance through performance-based contracting (see Appendix I of this memorandum) and incentive contracts;30\nD.\tContractual terms requiring vendors to participate in program evaluations sponsored by the agency to assess implementation and effectiveness, as an additional incentive to assess and improve the intended use case of an AI system, or service; and\nE.\tContractual language that requires vendors to document tools, techniques, coding methods, and testing results, as a means of promoting interoperability and mitigating vendor lock in.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Strategies: Performance requirements", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Evaluation: Conformity assessment"], "source": "https://www.whitehouse.gov/wp-content/uploads/2024/10/M-24-18-AI-Acquisition-Memorandum.pdf", "official_name": "Advancing the Responsible Acquisition of Artificial Intelligence in Government ", "label": "safe"}
{"id": "1356_32", "doc_id": "1356", "text": "ii.\tMitigate Inappropriate Use. When procuring general use enterprise-wide generative AI, agencies must consider including contractual requirements that ensure vendors provide appropriate protections, where practicable, against the AI systems or services being used in ways that are contrary to law and policy. This may include providing methods for monitoring how the general use enterprise-wide generative AI is used in the agency and guidance for how to monitor such use effectively, as well as potentially implementing technical safeguards against the AI being used in prohibited or otherwise sensitive contexts, such as refusing prompts asking for prohibited outputs.", "tags": ["Harms: Detrimental content", "Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring"], "source": "https://www.whitehouse.gov/wp-content/uploads/2024/10/M-24-18-AI-Acquisition-Memorandum.pdf", "official_name": "Advancing the Responsible Acquisition of Artificial Intelligence in Government ", "label": "safe"}
{"id": "1356_34", "doc_id": "1356", "text": "iv.\tTesting Performance to Identify the Best Fit for Agency Mission. When procuring general use enterprise-wide generative AI, agencies must take appropriate steps to provide an objective and empirical basis for competitively selecting the solution or solutions that provide the most value to the agency, including taking steps to compare the performance, cost, safety, security, and trustworthiness of available solutions under reasonably expected usage conditions.\nA.\tAgencies must consider a range of alternatives to a proposed solution and must not privilege proprietary solutions over open-source alternatives solely on the basis of their proprietary nature, but instead make a decision based on comprehensive evaluation. It is important not to assume that proprietary solutions are inherently more secure or reliable than open-source alternatives.\nB.\tTo compare performance of general use enterprise-wide generative AI solutions prior to the procurement, agencies may estimate the most common general categories of beneficial tasks that the agency will use the AI system for, relying as appropriate on the Federal AI Use Case Inventory and any additional resources provided by OMB, GSA, or NIST. Agencies may then assess multiple available solutions based on their evaluated performance on these categories of tasks.", "tags": ["Risk factors: Safety", "Risk factors: Security", "Risk factors: Reliability"], "source": "https://www.whitehouse.gov/wp-content/uploads/2024/10/M-24-18-AI-Acquisition-Memorandum.pdf", "official_name": "Advancing the Responsible Acquisition of Artificial Intelligence in Government ", "label": "safe"}
{"id": "1356_47", "doc_id": "1356", "text": "a.\t Take Advantage of Performance-Based Acquisition Techniques.\n\nAgencies are strongly encouraged to use performance-based requirements that allow agencies to understand and evaluate vendor claims about their AI systems or services prior to contract award, acquire AI capabilities that address their needs, and better perform post-award monitoring. Performance based techniques include:\n\ni.\tStatements of Objectives (SOOs) and Performance Work Statements (PWS), which provide agencies with more flexibility to acquire AI systems or services that meet agencies’ outcome-oriented needs but may not meet unnecessary or overly-limiting requirements in Statements of Work (SOW).\n\nii.\tQuality Assurance Surveillance Plans (QASP), which can help agencies overcome challenges in defining relevant performance metrics pre-solicitation and can enable a more collaborative process for negotiating a QASP that meets agency needs and objectives. Government personnel should be prepared to assume a more active role in performance monitoring.\n\niii.\tContract incentives to improve the performance for AI systems or services and their interoperability such that other agencies and vendors can improve upon, replicate, and further develop existing federal AI source code. Incentives can be based on metrics and provisions in QASPs. When determining whether to include performance-based incentives, agencies must carefully consider whether the established metrics are correctly tied to desired business and societal outcomes, and whether they can adequately measure baseline performance of the AI systems or services.", "tags": ["Strategies: Performance requirements", "Strategies: Evaluation"], "source": "https://www.whitehouse.gov/wp-content/uploads/2024/10/M-24-18-AI-Acquisition-Memorandum.pdf", "official_name": "Advancing the Responsible Acquisition of Artificial Intelligence in Government ", "label": "safe"}
{"id": "1387_4", "doc_id": "1387", "text": "High-Impact AI Use Cases\n\nSome AI activities in military, intelligence, or broader defense contexts that benefit U.S. national security objectives may simultaneously introduce significant new risks. Risks from the use of AI can arise in the event of AI failure, but can also manifest, for example, from ineffective outputs or if the AI was used in a context for which it was not intended. Such high-impact AI activities require sufficient safeguards to mitigate risk. For the purposes of this AI Framework, high-impact AI uses include AI whose output serves as a principal basis for a decision or action that could exacerbate or create significant risks to national security, international norms, democratic values, human rights, civil rights, civil liberties, privacy, or safety, such as the high-impact activities identified in the non-exhaustive list below. Agencies shall review each existing or planned use of AI to determine whether it matches this definition. Consistent with these goals, AI use is presumed to be high impact if the AI use controls or significantly influences the outcomes of any of the following activities:\n\nTracking or identifying individuals in real time, based solely on biometrics, for military or law enforcement action.\n\nClassifying an individual as a known or suspected terrorist, insider threat, or other national security threat in order to inform decisions or actions that could affect their safety, liberty, employment, immigration status, ability to enter or remain in the United States, or Constitutionally-protected rights and freedoms.\n\nDetermining an individual’s immigration classification, including related to refuge or asylum, or other entry or admission into the United States.\n\nThe activities referenced in Appendix I of OMB Memorandum M-24-10, when those activities: Occur within the United States; Impact U.S. persons; or Bear on immigration processes or other entry or admission into the United States.\n\nDesigning, developing, testing, managing, or decommissioning sensitive chemical or biological, radiological, or nuclear materials, devices, and/or systems (including chemical and biological data sources) that could be at risk of being unintentionally weaponizable.\n\nOperating or deploying malicious software designed to allow AI to automatically and without human oversight write or rewrite code in a way that risks unintended performance or operation, spread autonomously, or cause physical damage to or disruption of critical infrastructure.\n\nUsing AI as a sole means of producing and disseminating finished intelligence analysis. AI used in autonomous or semi-autonomous weapon systems are covered by the policies articulated in Department of Defense Directive 3000.09 and successor or related policies.", "tags": ["Risk factors: Bias", "Risk factors: Reliability", "Harms: Violation of civil or human rights, including privacy", "Harms: Harm to infrastructure", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Post-market monitoring", "Applications: Government: military and public safety", "Applications: Security", "Applications: Medicine, life sciences and public health", "Applications: Government: judicial and law enforcement", "Risk factors: Privacy", "Risk factors: Safety", "Harms: Harm to health/safety", "Strategies: Tiering: Tiering based on impact"], "source": "https://ai.gov/wp-content/uploads/2024/10/NSM-Framework-to-Advance-AI-Governance-and-Risk-Management-in-National-Security.pdf", "official_name": "Framework to Advance AI Governance and Risk Management in National Security", "label": "safe"}
{"id": "1387_5", "doc_id": "1387", "text": "AI Use Cases Impacting Federal Personnel For the purposes of this AI Framework, Federal personnel-impacting AI use cases include AI whose output serves as a significant basis for a decision or action resulting in a legal, material binding, or similarly significant effect on individual military service members, individuals in the Federal civilian workforce, or individuals to whom the agency has extended an offer of employment. Agencies shall review each existing or planned use of AI to determine whether it matches this definition. Consistent with these goals, AI is automatically presumed to impact Federal personnel if it is used to control or significantly influence the outcomes of any of the following activities: Making hiring decisions, including determining pay or benefits packages; Determining whether to promote, demote, or terminate employees; and Determining job performance, physical health, or mental health diagnoses or outcomes for U.S. Government personnel.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Post-market monitoring", "Applications: Government: other applications/unspecified", "Strategies: Tiering", "Applications: Government: benefits and welfare"], "source": "https://ai.gov/wp-content/uploads/2024/10/NSM-Framework-to-Advance-AI-Governance-and-Risk-Management-in-National-Security.pdf", "official_name": "Framework to Advance AI Governance and Risk Management in National Security", "label": "safe"}
{"id": "1387_8", "doc_id": "1387", "text": "Risk and Impact Assessments and Ensuring Effective Human Oversight High-impact AI use necessitates additional safeguards. \n\nThe practices in this AI Framework represent a minimum baseline for managing risk from the use of high-impact AI. Covered agencies must, taking into consideration the broad risk factors outlined in section 4.2(c) of the AI NSM, identify additional context-specific risks that are associated with their AI use and addressthem as appropriate, including by adding minimum risk management practices. All updates tohigh-impact AI use and associated minimum risk management practices must be provided to the APNSA.6Within 180 days of the issuance of this AI Framework, covered agencies shall begin following these practices before using new or existing high-impact AI:\n\nComplete an AI risk and impact assessment, including at least identifying the intended purpose for the AI, its expected benefits, and its potential risks. The AI risk and impact assessment should include: The intended purpose for the AI and its expected benefit, supported by metrics or qualitative analysis, as appropriate. The analysis should demonstrate an expected positive outcome, and it should demonstrate that the AI is better suited to accomplish the relevant task as compared to alternative strategies.\n\nThe potential risks of using the AI, as well as what, if any, additional mitigation measures, beyond these minimum practices, the agency will take to help reduce these risks. Agencies should document and assess the possible failure modes of the AI. The expected benefits of the AI functionality should be considered against its potential risks, and if the benefits do not meaningfully outweigh the unmitigated risks, agencies should not use the AI.\n\nThe quality and appropriateness of the relevant data. Agencies must assess the quality, to the extent practicable, of the data used in the AI’s design, development, training, testing, and operations, and the data’s fitness to the AI’s intended purpose. If a covered agency cannot access the data used to train, evaluate, and operate a given AI system, it must obtain sufficient descriptive information from the AI or data provider. At a minimum, covered agencies must document to the extent practicable: (i) the general provenance and quality of the data for the AI’s intended purpose for commercially-acquired models; (ii) how the data are relevant to the task being automated and are expected to be useful for AI development, testing, and operation; (iii) whether the data are sufficient to address the range of real-world inputs the AI system might encounter; (iv) whether the data come from a reliable source; and (v) how errors caused by the AI will be adequately measured and reasonably limited.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Risk factors: Bias", "Risk factors: Reliability", "Strategies: Tiering: Tiering based on impact", "Strategies: Tiering", "Strategies: Disclosure", "Strategies: Disclosure: About inputs"], "source": "https://ai.gov/wp-content/uploads/2024/10/NSM-Framework-to-Advance-AI-Governance-and-Risk-Management-in-National-Security.pdf", "official_name": "Framework to Advance AI Governance and Risk Management in National Security", "label": "safe"}
{"id": "1678_1", "doc_id": "1678", "text": "15 CFR part 702 is proposed to be amended as follows:\n\nPART 702—INDUSTRIAL BASE SURVEYS—DATA COLLECTIONS\n\n1. The authority citation for 15 CFR part 702 is revised to read as follows:\n\nAuthority: 50 U.S.C. 4501 et seq.; E.O. 13603, 77 FR 16651, 3 CFR, 2012 Comp., p. 225; E.O. 14110, 88 FR 75191, 3 CFR, 2023 Comp., p. 657.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Licensing, registration, and certification", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Reliability", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Harms: Discrimination", "Risk factors: Reliability: Robustness", "Risk factors: Reliability", "Risk factors: Safety", "Harms: Harm to health/safety", "Applications: Medicine, life sciences and public health", "Harms: Financial loss"], "source": "https://www.federalregister.gov/documents/2024/09/11/2024-20529/establishment-of-reporting-requirements-for-the-development-of-advanced-artificial-intelligence", "official_name": "Establishment of Reporting Requirements for the Development of Advanced Artificial Intelligence Models and Computing Clusters ", "label": "safe"}
{"id": "1678_2", "doc_id": "1678", "text": "2. Section 702.7 is added to read as follows: \n\n§ 702.7\n\nSpecial requirements for on-going reporting regarding the development of advanced artificial intelligence models and computing clusters.\n\n(a) Reporting requirements.\n\n(1) Covered U.S. persons are required to submit a notification to the Department by emailing ai_reporting@bis.doc.gov on a quarterly basis as defined in paragraph (a)(2) of this section if the covered U.S. person engages in, or plans, within six months, to engage in `applicable activities,' defined as follows:\n\n(i) Conducting any AI model training run using more than 10^26 computational operations (e.g., integer or floating-point operations); or\n\n(ii) Acquiring, developing, or coming into possession of a computing cluster that has a set of machines transitively connected by data center networking of greater than 300 Gbit/s and having a theoretical maximum greater than 10^20 computational operations (e.g., integer or floating-point operations) per second (OP/s) for AI training, without sparsity.\n\nNote 1 to paragraph (a)(1): Consistent with industry conventions, one multiply-accumulate computation, D = A × B + C, should be counted as two operations.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Licensing, registration, and certification"], "source": "https://www.federalregister.gov/documents/2024/09/11/2024-20529/establishment-of-reporting-requirements-for-the-development-of-advanced-artificial-intelligence", "official_name": "Establishment of Reporting Requirements for the Development of Advanced Artificial Intelligence Models and Computing Clusters ", "label": "safe"}
{"id": "1678_3", "doc_id": "1678", "text": "(2) Timing of notifications and response to BIS questions\n\n(i) Notification of applicable activities.\nCovered U.S. persons subject to the reporting requirements in paragraph (a)(1) of this section must notify BIS of `applicable activities' via email each quarter, identifying any `applicable activities' planned in the six months following notification. Quarterly notification dates are as follows: Q1—April 15; Q2—July 15; Q3—October 15; Q4—January 15. For example, in a notification due on April 15, a covered U.S. person should include all activities planned until October 15 of the same year.\n\n(ii) Response to BIS questions.\n\nFollowing a notification of `applicable activities' by a covered U.S. person, the covered U.S. person will receive questions from BIS. The covered U.S. person must respond to all questions within 30 calendar days of receiving the request.\n\n(iii) Corrections.\n\nIf any notification of `applicable activities' or response to BIS questions filed under this section is incomplete when filed, BIS will notify the covered U.S. person and require a revised resubmission within 14 calendar days after BIS provides notice of incompletion. BIS will continue to require revisions within 14 calendar days of notification if a resubmission remains incomplete.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Licensing, registration, and certification", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Reliability", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Harms: Discrimination", "Risk factors: Reliability: Robustness", "Risk factors: Reliability", "Risk factors: Safety", "Harms: Harm to health/safety", "Applications: Medicine, life sciences and public health", "Harms: Financial loss"], "source": "https://www.federalregister.gov/documents/2024/09/11/2024-20529/establishment-of-reporting-requirements-for-the-development-of-advanced-artificial-intelligence", "official_name": "Establishment of Reporting Requirements for the Development of Advanced Artificial Intelligence Models and Computing Clusters ", "label": "safe"}
{"id": "1678_4", "doc_id": "1678", "text": "(iv) Clarification questions.\n\nIf, after receipt of responses described in paragraph (a)(2)(ii) of this section, BIS has additional questions to clarify those responses, the covered U.S. person will provide additional responses to such additional questions within seven (7) calendar days. If the covered U.S. person needs additional time to provide an additional response, it can request an extension from BIS.\n\n(v) Affirmation of no applicable activities.\n\nFor each of the seven quarters following the quarter covered by a notification of `applicable activities,' if the covered U.S. person has no `applicable activities' (i.e., an “applicable activity” that meets the criteria under paragraph (a)(1)(i) or (ii) of this section) to report, they must submit an affirmation of no applicable activities by emailing ai_reporting@bis.doc.gov on the quarterly notification date. If the covered U.S. person submits an affirmation of no applicable activities for seven consecutive quarters, they need not provide BIS with any affirmation thereafter until they have `applicable activities' to report.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Licensing, registration, and certification", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Reliability", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Harms: Discrimination", "Risk factors: Reliability: Robustness", "Risk factors: Reliability", "Risk factors: Safety", "Harms: Harm to health/safety", "Applications: Medicine, life sciences and public health", "Harms: Financial loss"], "source": "https://www.federalregister.gov/documents/2024/09/11/2024-20529/establishment-of-reporting-requirements-for-the-development-of-advanced-artificial-intelligence", "official_name": "Establishment of Reporting Requirements for the Development of Advanced Artificial Intelligence Models and Computing Clusters ", "label": "safe"}
{"id": "1678_5", "doc_id": "1678", "text": "(b) Content, form, and manner of response to BIS questions.\n\n(1) All information submitted under this section shall be filed with BIS in the form and manner that BIS will prescribe in instructions sent to the covered U.S. person after BIS has received a notification of `applicable activities.'\n\n(2) BIS will send questions to the covered U.S. person which must address, but may not be limited to, the following topics:\n\n(i) Any ongoing or planned activities related to training, developing, or producing dual-use foundation models, including the physical and cybersecurity protections taken to assure the integrity of that training process against sophisticated threats;\n\n(ii) The ownership and possession of the model weights of any dual-use foundation models, and the physical and cybersecurity measures taken to protect those model weights;\n\n(iii) The results of any developed dual-use foundation model's performance in relevant AI red-team testing, including a description of any associated measures the company has taken to meet safety objectives, such as mitigations to improve performance on these red-team tests and strengthen overall model security; and\n\n(iv) Other information pertaining to the safety and reliability of dual-use foundation models, or activities or risks that present concerns to U.S. national security.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Reliability", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About inputs"], "source": "https://www.federalregister.gov/documents/2024/09/11/2024-20529/establishment-of-reporting-requirements-for-the-development-of-advanced-artificial-intelligence", "official_name": "Establishment of Reporting Requirements for the Development of Advanced Artificial Intelligence Models and Computing Clusters ", "label": "safe"}
{"id": "1678_6", "doc_id": "1678", "text": "(c) Definitions.\n\nFor purposes of the reports required by paragraph (a) of this section, apply the following definitions.\n\nAI red-teaming means a structured testing effort to find flaws and vulnerabilities in an AI system, often in a controlled environment and in collaboration with developers of AI. In the context of AI, red-teaming is most often performed by dedicated “red teams” that adopt adversarial methods to identify flaws and vulnerabilities, such as harmful or discriminatory outputs from an AI system, unforeseen or undesirable system behaviors, limitations, or potential risks associated with the misuse of the system.\n\nAI model means a component of an information system that implements AI technology and uses computational, statistical, or machine-learning techniques to produce outputs from a given set of inputs.\n\nAI system means any data system, software, hardware, application, tool, or utility that operates in whole or in part using AI.\n\nArtificial intelligence or AI has the meaning set forth in 15 U.S.C. 9401(3).\n\nCompany means a corporation, partnership, association, or any other organized group of persons, or legal successor or representative thereof. This definition is not limited to commercial or for-profit organizations. For example, the term “any other organized group of persons” may encompass academic institutions, research centers, or any group of persons who are organized in some manner. The term “corporation” is not limited to publicly traded corporations or corporations that exist for the purpose of making a profit.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Harms: Discrimination", "Risk factors: Reliability: Robustness", "Risk factors: Reliability"], "source": "https://www.federalregister.gov/documents/2024/09/11/2024-20529/establishment-of-reporting-requirements-for-the-development-of-advanced-artificial-intelligence", "official_name": "Establishment of Reporting Requirements for the Development of Advanced Artificial Intelligence Models and Computing Clusters ", "label": "safe"}
{"id": "1678_8", "doc_id": "1678", "text": "Large-scale computing cluster means a cluster of computing hardware that meets the technical thresholds provided by the Department in paragraph (a)(1) of this section.\n\nModel weights means the numerical parameters used in the layers of a neural network.\n\nTraining or training run refers to any process by which an AI model learns from data using computing power. Training includes but is not limited to techniques employed during pre-training like unsupervised learning and employed during fine tuning like reinforcement learning from human feedback.\n\nUnited States (U.S.) includes the 50 states, the District of Columbia, Puerto Rico, Guam, American Samoa, the U.S. Virgin Islands, and the Northern Mariana Islands.\n\nThea D. Rozman Kendler,\nAssistant Secretary for Export Administration.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Licensing, registration, and certification", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Reliability", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Harms: Discrimination", "Risk factors: Reliability: Robustness", "Risk factors: Reliability", "Risk factors: Safety", "Harms: Harm to health/safety", "Applications: Medicine, life sciences and public health", "Harms: Financial loss"], "source": "https://www.federalregister.gov/documents/2024/09/11/2024-20529/establishment-of-reporting-requirements-for-the-development-of-advanced-artificial-intelligence", "official_name": "Establishment of Reporting Requirements for the Development of Advanced Artificial Intelligence Models and Computing Clusters ", "label": "safe"}
{"id": "177_1", "doc_id": "177", "text": "§ 870.1420 Coronary artery disease risk indicator using acoustic heart signals.\n\n(a) Identification.\n\nA coronary artery disease risk indicator using acoustic heart signals is a device that records heart sounds including murmurs and vibrations to calculate a patient-specific risk of presence of coronary artery disease, as an aid in cardiac analysis and diagnosis.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In standard form", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/06/01/2022-11699/medical-devices-cardiovascular-devices-classification-of-the-coronary-artery-disease-risk-indicator", "official_name": "21 CFR § 870.1420 (\"Coronary artery disease risk indicator using acoustic heart signals\")", "label": "safe"}
{"id": "177_2", "doc_id": "177", "text": "(b) Classification.\n\nClass II (special controls). The special controls for this device are:\n\n(1) Clinical performance testing must fulfill the following:\n\n(i) Testing must include a discussion of the patient population and any statistical techniques used for analyzing the data; and\n\n(ii) Testing must be representative of the intended use population for the device. Any selection criteria or sample limitations must be fully described and justified.\n\n(2) Acoustic performance testing must evaluate microphone sensitivity, sound acquisition bandwidth, and amplitude accuracy. The acoustic sensor specifications and mechanism used to capture heart sounds must be described.\n\n(3) A scientific justification for the validity of the algorithm(s) must be provided. This justification must fulfill the following:\n\n(i) All inputs and outputs of the algorithm must be fully described;\n\n(ii) The procedure for segmenting, characterizing, and classifying the acoustic signal must be fully described; and\n\n(iii) This justification must include verification of the algorithm calculations and validation using an independent data set.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In standard form", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/06/01/2022-11699/medical-devices-cardiovascular-devices-classification-of-the-coronary-artery-disease-risk-indicator", "official_name": "21 CFR § 870.1420 (\"Coronary artery disease risk indicator using acoustic heart signals\")", "label": "safe"}
{"id": "177_3", "doc_id": "177", "text": "(4) The patient-contacting components of the device must be demonstrated to be biocompatible.\n\n(5) Software verification, validation, and hazard analysis must be performed.\n\n(6) Human factors/usability testing must demonstrate that the user can correctly use the device, including device placement, based solely on reading the directions for use.\n\n(7) Performance data must demonstrate the electromagnetic compatibility and electrical safety of the device.\n\n(8) Labeling must include the following:\n\n(i) A description of what the device measures and outputs to the user;\n\n(ii) Instructions for proper placement of the device;\n\n(iii) Instructions on care and cleaning of the device;\n\n(iv) Warnings identifying sensor acquisition factors that may impact measurement results and instructions for mitigating these factors; and\n\n(v) The expected performance of the device for all intended use populations and environments.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In standard form", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/06/01/2022-11699/medical-devices-cardiovascular-devices-classification-of-the-coronary-artery-disease-risk-indicator", "official_name": "21 CFR § 870.1420 (\"Coronary artery disease risk indicator using acoustic heart signals\")", "label": "safe"}
{"id": "1808_1", "doc_id": "1808", "text": "JSP 936 V1.1\nDependable Artificial Intelligence (AI) in Defence\nPart 1: Directive\n\n[Introductory material omitted. Figures and footnotes omitted throughout.]\n\n1 Introduction\n\nPolicy\n\n1. The Defence AI Strategy [1] and associated policy statement [2] (known as the Ambitious, Safe, Responsible, or ASR, document) set out how MOD will adopt and deploy AI in ways that are both effective and aligned to the UK’s democratic values.\n\n2. A holistic approach should be adopted when considering the AI Strategy; for example, it is closely linked to the MOD’s strategies for data [3] and digital backbone [4].\n\n3. In keeping with broad consensus, the MOD adopts the UK National AI Strategy [5] position that no single definition of AI is suitable across its range of applications. Therefore, a general characterisation is made for AI as follows:\n‘Machines that perform tasks normally requiring human intelligence, especially when the machines learn from data how to do those tasks.’\nNote that Machine Learning (ML) is a subset of AI but has become so prevalent that AI is often referred to as AI/ML. ML is not further characterised here as it is encompassed in the characterisation above.\n\n4. UK Government has also set out a legal definition in the National Security and Investments Act (see [6] for more information) which has greater clarity but is arguably less helpful for practical purposes in the context of this document: ‘Technology designed to approximate cognitive abilities including reasoning, perception, communication, learning, planning, problem solving, abstract thinking or decision making.’\nIt does, however, serve to provide useful perspective with respect to the use of AI in decision-making and these perspectives should be borne in mind when considering the nature of AI in Defence applications.\n\n5. For the purposes of this JSP, the MOD characterisation outlined in paragraph 3 will be adopted. Figure 1 provides an overview of indicative concepts, approaches and techniques that are found in AI. This is deliberately vague and incomplete as the field is complex and fast moving, any attempt at completeness is futile and risks excluding future developments. However, it does serve to characterise the types of technology that are often classed as AI and hence within scope of the JSP.\n\n6. The concept of dependability relates to how much the user can rely on the safe, secure and correct operation of the system within a given context (including mission or task, human involvement, the environment and system constraints). This JSP will not provide a dependability scale but the level of confidence developed in the system must be commensurate with the reliance placed upon it. Adequacy of evidence to support that confidence must be judged by risk owners throughout the lifecycle of the AI. In the Defence sphere, there might be times where there is considerable uncertainty (for example, due to a fast-evolving operating environment). Where this is the case owners of resultant risk must understand that uncertainty and communicate associated risks to all stakeholders involved in its operation. Whilst this JSP highlights new risk dimensions introduced by the unique nature of AI, the management of risk is not part of this JSP and must continue to be practiced as per JSP 892.", "tags": ["Risk factors: Reliability", "Applications: Government: military and public safety", "Strategies: Disclosure"], "source": "https://assets.publishing.service.gov.uk/media/6735fc89f6920bfb5abc7b62/JSP936_Part1.pdf", "official_name": "JSP 936 V1.1 Dependable Artificial Intelligence (AI) in Defence (part 1: directive)", "label": "safe"}
{"id": "1808_7", "doc_id": "1808", "text": "Robotic and Autonomous Systems\n\n30. The term ‘robot’ stems from the Czech word for forced labour (‘robota’). In modern technology it has come to mean a system designed to undertake work that would normally be done by humans (or other living entities). In the context of this JSP, we consider only advanced robotics, that is those requiring the use of AI to perform complex tasks.\n\n31. Similar to the problematic nature of defining AI, there is no overall consensus on what constitutes an Autonomous System (AS).\n\n32. Since this JSP addresses AI across all MOD AS, for the purposes of this JSP, we frame the type of system to which we are referring as follows:\n‘An autonomous system is capable of acting on high-level goal-setting provided by human operators. From these set goals and its perception of its operating environment, such a system is able to take action to bring about a desired state. It is capable of deciding a course of action, from a number of alternatives, without depending on human oversight and control, although these may still be present. Although the overall activity of an autonomous system may be predictable, individual actions may not be. Additionally, autonomous systems may contain machine-learning capabilities which endow them with some abilities for changing their own actions without the intervention of a human.\nAutonomous Systems are contrasted to automated systems that can function with little human involvement, but only perform pre-programmed actions.’\nIn all but the simplest environments, AI is highly likely to be a key component of an AS.\n\n33. Having noted there is no universally accepted definition of AS, we should note that NATO has published the following [9]:\n‘Autonomy is the ability of a system to respond to uncertain situations by independently composing and selecting among different courses of action in order to accomplish goals based on knowledge and a contextual understanding of the world, itself, and the situation. Autonomy is characterised by degrees of self-directed behaviour (levels of autonomy) ranging from fully manual to fully autonomous.’\n\n34. Clearly there is an overlap between robots and autonomous systems but advanced robots do not have to be autonomous and similarly autonomous systems do not need to be robotic. Generally though, they are grouped together due to their similarity.\n\n35. Notably there is a continuum in RAS behavioural capabilities from manually\noperated, through automatic to fully autonomous. JSP 936 is concerned with dependable AI regardless of the overall system level of autonomy; however, higher levels of autonomy typically reduce the potential for human decision-making within the control loop and this must be considered when applying the requirements of this JSP.\n\n36. Many RAS are considered to be or to contain ‘dual-use’ technology. Where a civil-sector RAS containing AI is procured for Defence use, the ‘military delta’ in the\nOperational Design Domain (ODD) must be identified, and the AI tested to ensure that any drop-offs in performance from the possibly benign, designed-for ODD are understood and additional risks identified. The difference in safety and security needs, introduced in the military domain by the increased potential for malign actions carried out by an adversary, should be of particular note.", "tags": ["Applications: Government: military and public safety", "Risk factors: Reliability", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment"], "source": "https://assets.publishing.service.gov.uk/media/6735fc89f6920bfb5abc7b62/JSP936_Part1.pdf", "official_name": "JSP 936 V1.1 Dependable Artificial Intelligence (AI) in Defence (part 1: directive)", "label": "safe"}
{"id": "1808_10", "doc_id": "1808", "text": "Ethical Principles\n\n49. The ASR policy [2] is clear that the MOD is committed to responsibly developing and deploying AI for purposes that are demonstrably beneficial whilst upholding human rights and democratic values. To support that it sets out five key principles9:\na. Human-centricity. The impact of AI-enabled systems on humans must be assessed and considered, for a full range of effects both positive and negative across the entire system lifecycle.\nb. Responsibility. Human responsibility for AI-enabled systems must be clearly established, ensuring accountability for their outcomes, with clearly defined means by which human control is exercised throughout their lifecycles.\nc. Understanding. AI-enabled systems, and their outputs, must be appropriately understood by relevant individuals, with mechanisms to enable this understanding made an explicit part of system design.\nd. Bias and harm mitigation. Those responsible for AI-enabled systems must proactively mitigate the risk of unexpected or unintended biases or harms resulting from these systems, whether through their original rollout, or as they learn, change or are redeployed.\ne. Reliability. AI-enabled systems must be demonstrably reliable, robust and secure.\nAdoption of these principles is key to developing trust in our use of AI-based systems across the range of stakeholders, from the operator to system owners and our wider society. When speaking of trust, we do not suggest an abdication of all of our control to AI-based systems, rather we speak of building reliable systems operating under meaningful human control exercised through context-appropriate human involvement. Whilst it is tempting to treat the principles in isolation, they should be considered in the context of the wider ASR document (for example Legal and Governance aspects are considered separately in the ASR but are related to ethical values) as well as other relevant publications such as JSP 985 Human Security in Defence.\n\n50. In many respects, but not all, the MOD’s AI Ethical Principles can be considered as driving the safe10 adoption of AI across the entire business, from back office functions to front line operations. Organisations must consider these principles as early as\npracticable, for ease of implementation. Teams will almost always need to undertake a balancing and judgement exercise between principles in order to adopt them – what good looks like in terms of meeting the principles will look different for each use case.\nAdditionally, teams will need to consider military requirements and operational effectiveness, recognising that developing AI responsibly by implementing the MOD AI Ethical Principles will ultimately result in more robust, reliable, and effective AI-enabled capabilities, thereby advancing our military edge.\n\n51. Whilst not directly related, [extant internal policy] may be relevant to AI ethics, particularly in relation to the reporting of ethical concerns. Where AI is in use and may impact human well-being, there must be clearly signposted avenues for redress as laid out in [extant internal policy].", "tags": ["Applications: Government: military and public safety", "Risk factors: Bias", "Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Strategies: Performance requirements"], "source": "https://assets.publishing.service.gov.uk/media/6735fc89f6920bfb5abc7b62/JSP936_Part1.pdf", "official_name": "JSP 936 V1.1 Dependable Artificial Intelligence (AI) in Defence (part 1: directive)", "label": "safe"}
{"id": "1808_11", "doc_id": "1808", "text": "Ethical Principles: Human Centricity\n\n52. All humans (e.g. MOD personnel, civilians, targets of military action etc.) interacting with or affected by the development and/or use of an AI-enabled system must be clearly identified. An assessment must then be made of the impact the AI could have on each stakeholder group to ensure that effects are as positive as possible and justified as outweighing negative effects where these may arise.\n\n53. Whilst conducting the assessment of any impact on humans, considerations should include, but not be limited to, the seven factors associated with Human Security (see JSP 985). Summarising JSP 985, these factors are that:\na. Personal/Physical: the potential for unnecessary physical harm should be minimised.\nb. Political: the democratic values of the UK, where there is freedom from repression and the right for freedom of expression, should be upheld.\nc. Economic: quality of life due to economic pressure should be maintained or enhanced where possible.\nd. Cultural/Community: traditional relationships with cultural heritage should be maintained.\ne. Health: illness should be prevented through maintenance of healthy lifestyle.\nf. Food: physical and economic access to food that meets dietary needs should\nbe maintained.\ng. Environmental/Climate11: the impact on the environment should be minimized whilst providing equitable access to natural resources or industrialization (for example, the energy consumption of LLMs is fairly high).\nh. Informational: appropriate access to information that empowers the individual\nshould be provided whilst not being manipulative or controlling.\n\n54. Where the system has more than one mode of operation or ‘level of autonomy’ (see diagram on page 4 of the Defence AI Strategy [1]) the impact analysis must be conducted for all modes and ‘autonomy levels’.\n\n55. The concept of harmful effects is distinct from the intended military effects of certain capabilities. It is necessary to understand the factors set out in paragraph 53 in order to assess the military effectiveness of capability. Even when deploying a military effect it should be clearly demonstrated that the positive benefit of AI use outweighs any wider negative impacts factoring in the information available in the context of use.", "tags": ["Risk factors: Safety", "Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Ecological harm", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Applications: Government: military and public safety", "Strategies: Performance requirements"], "source": "https://assets.publishing.service.gov.uk/media/6735fc89f6920bfb5abc7b62/JSP936_Part1.pdf", "official_name": "JSP 936 V1.1 Dependable Artificial Intelligence (AI) in Defence (part 1: directive)", "label": "safe"}
{"id": "1808_20", "doc_id": "1808", "text": "5 Human/AI Teams\n\nIntroduction\n\n115. The MOD recognises the advantages that the teaming of humans with AI brings are central to the application of AI across Defence (see [1], [2], [9]). These advantages include enhancement of overall effectiveness, optimal use of resources, the practicalities of\n\nintegration and the ease with which we can address issues arising. Fully realising the benefits of AI depends on understanding the relative strengths of humans and machines, and how they best function in combination to achieve the desired outcomes within a particular context of use. This human involvement and teaming approach extends beyond the individual operator interacting with an AI-enabled system to include the wider team of people involved in supporting, training and maintaining the system.\n\n116. As systems become increasingly interconnected, individual human/AI teams are likely to expand and become teams of human/AI teams extending across the Defence enterprise. The organisational consequences of ubiquitous AI will become increasingly important together with the critical role that humans play in supporting resilience, safety and maintaining human accountability.\n\n117. The importance of addressing these human factors applies across all applications of AI from combat systems through to administrative and support systems.\n\n118. Delivering effective human/AI teams is dependent on adopting a Human Centred Design (HCD) approach across the system lifecycle. HCD focusses on identifying user needs, involving users in the design and testing of a developing system solution and applying human factors best practice. A well-managed HCD approach will support projects in optimising system performance, reducing risk, enhancing cost-effectiveness and supporting user adoption of new systems.", "tags": ["Strategies: Performance requirements"], "source": "https://assets.publishing.service.gov.uk/media/6735fc89f6920bfb5abc7b62/JSP936_Part1.pdf", "official_name": "JSP 936 V1.1 Dependable Artificial Intelligence (AI) in Defence (part 1: directive)", "label": "safe"}
{"id": "1808_21", "doc_id": "1808", "text": "Human Centred AI Design\n\n119. A recognised HCD approach, appropriate to the system under development, must be applied across the system lifecycle, including introduction into service and in-service updates.\n\n120. The HCD approach should be appropriately resourced, managed and integrated within the wider software development process.\n\n121. For the development and acquisition of military capability the design approach must\napply the Human Factors Integration (HFI) process mandated in JSP 91\n213.\n\n122. The through life HCD approach adopted must include the following activities:\na. identification of the users14 of the system and understanding their characteristics.\nb. identification of user needs and the development of Human Factors’ related requirements with associated acceptance criteria for inclusion in project documentation.\nc. involvement of users in the development and testing of the system solution; this includes software updates and testing of systems following training/retraining of AI systems.\n\n13 Note that JSP 912, associated Human Factors Integration Defence Standard 00-251 [11] and Technical Guides do not specifically address AI technologies.\n14 ‘Users’ includes: operators, maintainers, support personel and people that come into contact with or are affected by the system.\n\n123. Alongside the involvement of users in the development and testing of the system’s human factors, insights from human sciences should also be applied to the system\nsolution. This should include:\na. the application of established Human Factors principles and accepted best practice to the design of the system.\nb. the use of suitable methods, tools, techniques and data by projects to support design.\nc. the application of principles of transparency, explainability and interpretability to the design of the system and the development of user interfaces that enable the user to understand system behaviour.\n\n124. A key part of the design process of all systems, but one that is critical to those intended to be used within a human/AI teaming approach, is functional analysis and allocation of functions between human and machine.\n\n125. An analysis of the allocation of functions between human and AI agents and AI behaviours across all modes of function and levels of autonomy must be conducted to:\na. provide evidence of compliance with the ethical principles.\nb. ensure that there is no accountability gap, i.e. humans remain accountable for, and in control of, the effects of the system.\nc. understand the roles of humans in ensuring system safety, performance, resilience, and preventing AI bias and how the design of the system supports this.\nd. provide evidence to support broader legal and regulatory compliance arguments (e.g. safety cases and legal reviews).", "tags": ["Risk factors: Bias", "Risk factors: Interpretability and explainability", "Strategies: Evaluation", "Strategies: Performance requirements", "Applications: Government: military and public safety"], "source": "https://assets.publishing.service.gov.uk/media/6735fc89f6920bfb5abc7b62/JSP936_Part1.pdf", "official_name": "JSP 936 V1.1 Dependable Artificial Intelligence (AI) in Defence (part 1: directive)", "label": "safe"}
{"id": "1808_27", "doc_id": "1808", "text": "Architecture\n\n149. The scope of the architecture in the context of this JSP relates to the AI algorithms, data and models.\n\n150. The AI architecture must be traceable to requirements and be able to incorporate the intended behaviours whilst protecting against entry into failure modes identified during hazard analysis (e.g. increased potential for overfitting or underfitting data, handling sparse or missing data etc.). Consideration of this clause should include mitigations for AI failure put into the wider system integration and architecture.\n\nAlgorithm Design\n\n151. Choices made during algorithm design (e.g. hyperparameter settings) should be\njustified and documented including tracing to functional and non-functional requirements.\n\n152. Measures of performance that are used to optimise AI designs should be justified and documented.\n\n153. It can be tempting to optimise AI design to maximise performance against training and test data. However, this risks limitations in data quality and availability driving operational performance. Evidence should be provided that algorithm design is optimised for the performance in the expected operational context; for example by demonstrating the avoidance of overfitting.\n\nAlgorithm Implementation\n\n154. Traceability to requirements should be maintained in the algorithm implementation.\n\n155. All frameworks and/or libraries that are utilised to develop algorithms must be\njustified and their output checked and demonstrated as instantiating the AI requirements.", "tags": ["Strategies: Performance requirements"], "source": "https://assets.publishing.service.gov.uk/media/6735fc89f6920bfb5abc7b62/JSP936_Part1.pdf", "official_name": "JSP 936 V1.1 Dependable Artificial Intelligence (AI) in Defence (part 1: directive)", "label": "safe"}
{"id": "1808_30", "doc_id": "1808", "text": "AI Verification and Validation\n\n169. Verification demonstrates that the AI meets its requirements; validation demonstrates that the AI meets the user’s needs. This is sometimes referred to as ‘did we build it right (verification) and did we build the right thing (validation)’.\n\n170. Verification and validation of the AI must include demonstration across the ODD including boundary conditions and realistic edge cases.\n\n171. Safe behaviours of the AI when exposed to inputs that fall outside of the ODD must\nbe demonstrated.\n\n172. Where AI is updated, for example through exposure to new training data, verification and validation activity must include testing for both existing and new behaviours (e.g. through new test cases and the application of regression testing). This is to provide assurance that the intended outcomes of the update have been achieved and unwanted emergent effects such as catastrophic forgetting and model drift have not occurred.\n\n173. Evidence should be provided to support arguments for the validity of the claims being made for verification and validation sufficiency. This should include, for example, metrics\non test coverage of the ODD, explicit and inferred requirements, internal algorithm/model structural coverage, performance metrics within the tested context etc.", "tags": ["Risk factors: Reliability: Robustness", "Risk factors: Reliability", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation"], "source": "https://assets.publishing.service.gov.uk/media/6735fc89f6920bfb5abc7b62/JSP936_Part1.pdf", "official_name": "JSP 936 V1.1 Dependable Artificial Intelligence (AI) in Defence (part 1: directive)", "label": "safe"}
{"id": "1808_31", "doc_id": "1808", "text": "AI Integration, Use and Modification\n\n174. AI will always operate within some wider system or system of systems context, which may include other AI. Care should be taken to ensure that the AI will perform to its operational requirements in its context of use. This may require an assessment of the AI impact on broader operational governance requirements, such as operational authorities\nand Rules of Engagement.\n\n175. Where performance is demonstrated in a system environment other than the final operational system, differences between the environments must be analysed, understood and mitigated. This includes where AI is transferred from one operational system to another with differing build standards (e.g. different computational hardware or system inputs).\n\n176. When substantive modifications are made to an existing AI-hosting system continued satisfactory performance must be demonstrated. Substantive changes might include, but are not limited to, processors and operating systems etc.\n\n177. The approved ODD, i.e. the environment in which the AI has been demonstrated as achieving acceptable performance, must be clearly defined in the operating context.\n\n178. The management and use of AI may present particular operating risks. These should be considered in wider operating policies and procedures; for example, where necessary AI products must have specific and clearly written Security Operating Procedures (SyOPs) that are proportionate to the risk presented by their use.\n\n179. The operational environment must be monitored at a contextually appropriate rate to ensure that the AI-based system continues to operate within the ODD of the AI.\n\n180. AI performance must be monitored at an appropriate rate to provide assurance that it remains within an acceptable level with respect to the operational requirements placed on the system during use. In cases where AI is supporting important/risky decision-making, its output should have additional checks applied that are undertaken by a relevant subject matter expert.\n\n181. Where AI is intended for modification, processes and procedures must be in place to ensure the continued assurance of acceptable behaviour. These should include steps to ensure that changes to risk are properly understood and managed appropriately.", "tags": ["Applications: Government: military and public safety", "Risk factors: Reliability", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Performance requirements"], "source": "https://assets.publishing.service.gov.uk/media/6735fc89f6920bfb5abc7b62/JSP936_Part1.pdf", "official_name": "JSP 936 V1.1 Dependable Artificial Intelligence (AI) in Defence (part 1: directive)", "label": "safe"}
{"id": "1808_37", "doc_id": "1808", "text": "9 AI Assurance\n\n205. Assurance of AI shares many common features with its ‘traditional software’ counterpart. Traditional approaches should continue to be applied with this section providing an overlay of requirements that address the novel aspects of AI applications.\n\n206. Given the breadth of technologies and development approaches, it is neither possible nor appropriate to be prescriptive on resolving aspects of ‘traditional software’ and AI assurance differences. Typically these differences may be as a result of imprecise requirements, reduced requirement traceability, excessive complexity, unpredictable specific behaviour and adaptive behaviour. This can be thought of as ‘assurance risk’, i.e. the risk associated with the assurance process itself. In some applications these issues will simply present too much risk and in such cases AI must not be adopted. Where ‘assurance risks’ can be tolerated, assurance activities still have to be undertaken that are commensurate with the level of risk posed by incorrect AI outputs.\n\n207. AI assurance should be considered within the context of the system in which it operates. Some systems may be able to tolerate a level of incorrect specific outputs\nprovided the overall intended outcome is acceptable (e.g. safe, secure, ethical etc.). This may be a key difference between traditional software assurance and that for AI. Without its accommodation, the potential gains of AI may be lost unnecessarily.\n\n208. Where AI is replacing extant technology or human decision-making, AI assurance may be able to argue that the specific risk from the AI is no greater than the system being replaced. That is, for example, if AI is being used to plan vehicle trajectories (e.g. route planning) that were previously produced via human thought, then the risk of failure should be no greater than when the human planned them.\n\n209. AI assurance must include assurance of behaviour where excursions from the AI ODD may reasonably be expected to occur. For many Defence applications this may include adversarial action causes. In such cases careful consideration must be made with respect to failure mode behaviour (e.g. options to ‘fail safe’ or ‘fail operational’). The level of confidence sought in the assurance of the AI should be commensurate with the acceptable ethical, safety, security and mission risk associated with the function provided by the AI.\n\n[References omitted.]", "tags": ["Risk factors: Reliability", "Risk factors: Reliability: Robustness", "Applications: Government: military and public safety"], "source": "https://assets.publishing.service.gov.uk/media/6735fc89f6920bfb5abc7b62/JSP936_Part1.pdf", "official_name": "JSP 936 V1.1 Dependable Artificial Intelligence (AI) in Defence (part 1: directive)", "label": "safe"}
{"id": "2099_2", "doc_id": "2099", "text": "Chapter 2\tReviewers\n\nArticle 5 [Reviewers] Institutes of higher education, scientific research institutions, medical and health institutions, and enterprises, among others, are the entities responsible for S&T ethics review management. Work units (单位) engaged in S&T activities in areas such as life sciences, medicine, and artificial intelligence (AI), and whose research involves sensitive S&T ethics areas, shall establish an S&T ethics (review) committee. Other work units with ethical review requirements may establish S&T ethics (review) committees based on their actual situation.\n\nWork units shall provide the S&T ethics (review) committee full-time (or part-time) staff, office space, funding, and other prerequisites, and take effective measures to assure that the S&T ethics (review) committee carries out ethical review work independently.\n\nArticle 6 [Duties of the Committee] The main duties of the S&T ethics (review) committee include:\n\n(1) Formulating and improving the management rules and work norms of the S&T ethics (review) committee;\n\n(2) Providing consultation on S&T ethics and guiding scientific and technical personnel in conducting S&T ethical risk assessments of S&T activities;\n\n(3) Conducting S&T ethics reviews and tracking and supervising the entire process of relevant S&T activities in accordance with requirements;\n\n(4) Determining whether proposed S&T activities fall within the scope of the list defined in Article 31 of these Measures;\n\n(5) Organizing and conducting training for committee members on S&T ethics reviews, and training for scientific and technical personnel on S&T ethics;\n\n(6) Accepting complaint reports involving S&T ethics issues in the unit’s S&T activities;\n\n(7) Carrying out registration and reporting as required by the management department (管理部门), and cooperating with the management department in conducting relevant work involving S&T ethics reviews.", "tags": ["Applications: Medicine, life sciences and public health", "Strategies: Evaluation", "Strategies: Convening", "Strategies: Evaluation: External auditing", "Strategies: New institution"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0611_science_ethics_trial_measures_draft_EN.pdf", "official_name": "(Trial) Measures for Science and Technology Ethics Reviews (Draft for Feedback)", "label": "safe"}
{"id": "2099_3", "doc_id": "2099", "text": "Article 7 [System Construction] The S&T ethics (review) committee shall formulate its own charter, establish rules, norms, and work procedures for review, supervision, secrecy management (保密管理), file management, etc., improve mechanisms for managing conflicts of interest, and assure that S&T ethics reviews are compliant, transparent, and traceable.\n\nArticle 8 [Committee Composition] The S&T ethics (review) committee shall have no fewer than seven members, with one member serving as the committee chair. Committee members shall include peer expert members with relevant S&T backgrounds, members with relevant professional backgrounds such as ethics, law, and sociology, members of different genders, and members who are not from the same organization. In ethnic minority areas, consideration shall be given to including members from minority ethnic groups. Members shall have terms of office not to exceed five years, and may be re-elected.\n\nArticle 9 [Member Requirements] Members of the S&T ethics (review) committee shall have relevant ethics review abilities, be in good standing in terms of scientific research integrity, and comply with the following requirements:\n\n(1) Abide by China’s laws and regulations, relevant S&T ethics rules and norms, and the charter of the S&T ethics (review) committee on which they serve;\n\n(2) Attend S&T ethics review meetings on time and independently express review opinions;\n\n(3) Strictly comply with secrecy protection (保密) provisions, and not disclose or use for other purposes, without permission, the personal private information, technical secrets, undisclosed information, etc., that they come into contact with or learn of in the course of ethics review work;\n\n(4) Comply with the requirements for managing conflicts of interest and recuse oneself in accordance with regulations;\n\n(5) Regularly participate in training on S&T ethics review and receive continuing education;\n\n(6) Cooperate in completing other tasks arranged by the committee.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Risk factors: Transparency", "Risk factors: Privacy"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0611_science_ethics_trial_measures_draft_EN.pdf", "official_name": "(Trial) Measures for Science and Technology Ethics Reviews (Draft for Feedback)", "label": "safe"}
{"id": "2099_4", "doc_id": "2099", "text": "Chapter 3\tReview Procedures\n\nSection I General Review Procedures\n\nArticle 10 [Application for Review] Ethical risk assessments shall be carried out for S&T activities. The S&T ethics (review) committee shall formulate S&T ethical risk assessment procedures and standards in accordance with the requirements of these Measures, so as to guide scientific researchers in conducting ethical risk assessments.\n\nFor S&T activities that fall within the scope of Article 3 of these Measures, the person in charge of the S&T activity shall apply to the S&T ethics (review) committee of their work unit for an ethics review. The application materials shall mainly include:\n\n(1) An overview of the S&T activity, including the name, purpose, significance, necessity, and previous S&T ethics reviews of the S&T activity;\n\n(2) The implementation plan and related materials for the S&T activities, including the design of the S&T activity plan, possible S&T ethical risks and their prevention and control measures and emergency response plans, and the form in which the achievements of S&T activities are to be announced;\n\n(3) The legal qualification materials of the relevant institutions involved in the S&T activity, the relevant research experience of the participants in the S&T activity and their participation in S&T ethics training, the source of funding for the S&T activity, and conflict of interest declarations for the S&T activity;\n\n(4) Informed consent forms, materials explaining the sources of biological samples, information, and data, and materials explaining the sources of laboratory animals;\n\n(5) A written commitment to comply with the requirements of S&T ethics and scientific research integrity; and\n\n(6) Other materials that the S&T ethics (review) committee deems it necessary to submit.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: External auditing"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0611_science_ethics_trial_measures_draft_EN.pdf", "official_name": "(Trial) Measures for Science and Technology Ethics Reviews (Draft for Feedback)", "label": "safe"}
{"id": "2099_7", "doc_id": "2099", "text": "Article 19 [Re-Examination] Where adjustments to the implementation plan for an S&T activity may lead to changes in S&T ethical risks, the person in charge of the S&T activity shall promptly report to the S&T ethics (review) committee and cooperate with said committee in conducting assessments of the risks and benefits. When necessary, the S&T ethics (review) committee may conduct a new ethics review, and the S&T activity must pass the ethics review before its implementation may proceed.\n\nArticle 20 [Review Cooperation] Where multiple work units cooperate to carry out S&T activities, the lead work unit shall establish an S&T ethics review cooperation mechanism to strengthen the coordinated management of S&T ethics reviews and promote mutual recognition of review results. The cooperating work units shall submit the review decision or approval opinion to the lead work unit in a timely manner.\n\nArticle 21 [S&T Activities with International Cooperation] Where S&T activities with international cooperation fall within the scope of Article 3 of these Measures, such activities must pass the S&T ethics reviews prescribed by the countries where the cooperating parties are located before they may be conducted.\n\nArticle 22 [Commissioned Review] Where a unit has not established an S&T ethics (review) committee or where the S&T ethics (review) committee fails to meet the requirements for review work, it shall commission in writing another S&T ethics (review) committee that satisfies the requirements to conduct the ethics review.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Convening", "Strategies: New institution"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0611_science_ethics_trial_measures_draft_EN.pdf", "official_name": "(Trial) Measures for Science and Technology Ethics Reviews (Draft for Feedback)", "label": "safe"}
{"id": "2099_9", "doc_id": "2099", "text": "Section 3 Emergency Review Procedures\n\nArticle 26 [Emergency Review System] The S&T ethics (review) committee shall formulate a system for emergency S&T ethics reviews, clarify the emergency review process and standard operating rules for emergencies such as sudden public security incidents (突发公共事件), and organize and conduct emergency ethics review training.\n\nArticle 27 [Emergency Review Mechanism] The S&T ethics (review) committee shall carry out graded management (分级管理) according to the urgency of S&T activities, and may establish a fast track for S&T ethics reviews in order to conduct emergency reviews in a timely manner. Emergency reviews shall generally be completed within 72 hours.\n\nArticle 28 [Review Requirements] Participants in emergency review meetings should include members with expertise in the relevant field. Where there are no members with expertise in the corresponding field, expert advisors in the relevant field shall be invited to attend in order to provide consultation and guidance.\n\nArticle 29 [Follow-up Management] The S&T ethics (review) committee shall strengthen the follow-up review and process supervision of emergency S&T activity reviews, and provide scientific and technical personnel guidance and advice on S&T ethics in a timely manner.\n\nArticle 30 [Review Supervision] No work unit or individual may use an emergency as an excuse to avoid an S&T ethics review or to lower S&T ethics review standards.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Risk factors: Safety", "Strategies: Convening"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0611_science_ethics_trial_measures_draft_EN.pdf", "official_name": "(Trial) Measures for Science and Technology Ethics Reviews (Draft for Feedback)", "label": "safe"}
{"id": "2099_12", "doc_id": "2099", "text": "Article 46 [Annual Reporting] Before March 31 of each year, the work unit shall submit to the national S&T ethics management information registration platform a report on the S&T ethics (review) committee’s work in the preceding year, and a report on the implementation status of S&T activities included in checklist-style management.\n\nArticle 47 [Complaint Reporting] Any work unit or individual shall be entitled, according to law, to report complaints to the relevant departments and work units regarding conduct in S&T activities that violates S&T ethical norms or requirements.\n\nArticle 48 [Handling of Violations] Where a work unit undertaking S&T activities, or its scientific and technical personnel, violates the provisions of these Measures, and any of the following circumstances exists, the agency with jurisdiction shall impose penalties or take other handling measures in accordance with laws, administrative regulations, and relevant provisions; where property loss or other damage has been caused, civil liability shall be borne according to law; and where a crime has been committed, criminal liability shall be pursued according to law.\n\n(1) Obtaining S&T ethics review approval through fraud or forgery, or falsifying or tampering with documents related to S&T ethics review;\n\n(2) For an S&T activity included in checklist-style management, failure to have the activity pass S&T ethics review and expert reconsideration as required;\n\n(3) Conducting an S&T activity without obtaining S&T ethics review and approval as required;\n\n(4) Conducting an S&T activity beyond the scope approved by the S&T ethics review;\n\n(5) Other actions that violate the provisions of these Measures.\n\nArticle 49 [Handling of Violations] Where the S&T ethics committee or a member thereof violates the provisions of these Measures and any of the following circumstances exist, the agency with jurisdiction shall impose penalties or take other measures in accordance with laws, administrative regulations, and relevant provisions; where property loss or other damage has been caused, civil liability shall be borne according to law; and where a crime has been committed, criminal liability shall be pursued according to law.\n\n(1) Falsification of materials to aid the work units undertaking S&T activities in obtaining S&T ethics review and approval\n\n(2) Engagement in favoritism or irregularities for personal gain, abuse of power, or neglect of duty;\n\n(3) Other violations of the provisions of these Measures.", "tags": ["Incentives: Civil liability", "Incentives: Criminal liability", "Risk factors: Bias"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0611_science_ethics_trial_measures_draft_EN.pdf", "official_name": "(Trial) Measures for Science and Technology Ethics Reviews (Draft for Feedback)", "label": "safe"}
{"id": "2099_13", "doc_id": "2099", "text": "Article 50 [Division of Investigation and Handling Responsibilities, Part One] Institutes of higher education, scientific research institutions, medical and health institutions, enterprises, etc., are the entities with primary responsibility for the internal investigation and handling of S&T ethics violations. They shall promptly and actively investigate S&T ethics violations and pursue liability according to law.\n\nWhere a work unit and the persons responsible for it are suspected of an S&T ethics violation, the investigation and handling shall be done by the main oversight department (主管部门) at the next higher level, and where there is no main oversight department at the next higher level, the provincial-level S&T administrative department for the work unit’s location shall be responsible for organizing the investigation and handling.\n\nArticle 51 [Division of Investigation and Handling Responsibilities, Part Two] Local and relevant main industrial oversight departments shall, in accordance with their respective duties and lines of authority, strengthen guidance and supervision of the investigation and handling of S&T ethics violations in their respective localities and systems, and organize and conduct the investigation and handling of major S&T ethics cases.\n\nArticle 52 [Division of Investigation and Handling Responsibilities, Part Three] Where an S&T ethics violation involves an S&T project established with government funding, the project management department (work unit) shall organize the investigation and handling in accordance with relevant provisions on project management. A work unit undertaking (participating in) a project shall, at the request of the project management department (work unit), actively carry out and cooperate with the investigation, and deal with the person(s) responsible for the violation in accordance with their duties and powers.", "tags": ["Incentives: Civil liability"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0611_science_ethics_trial_measures_draft_EN.pdf", "official_name": "(Trial) Measures for Science and Technology Ethics Reviews (Draft for Feedback)", "label": "safe"}
{"id": "2426_1", "doc_id": "2426", "text": "STATE OF NEW YORK\n   ________________________________________________________________________\n\n                                    6453--B\n\n                           2025-2026 Regular Sessions\n\n               IN ASSEMBLY\n\n                                 March 5, 2025\n                                 ___________\n\n   Introduced  by  M.  of A. BORES, LASHER, SEAWRIGHT, PAULIN, TAPIA, RAGA,\n      SHIMSKY, REYES, EPSTEIN, BURKE, HEVESI, P. CARROLL, ZACCARO,  HYNDMAN,\n      LUPARDO,  KASSAY, LEE, DAVILA, SCHIAVONI, LUNSFORD, K. BROWN, TANNOUS-\n      IS, TORRES, HOOKS, GIBBS, ROMERO, COLTON, CONRAD, MEEKS, GLICK,  CRUZ,\n      CUNNINGHAM,  FORREST,  CHANDLER-WATERMAN, STIRPE, WRIGHT, SIMON, DAIS,\n      JENSEN, ROZIC, GONZALEZ-ROJAS -- read once and referred to the Commit-\n      tee on Science and Technology -- committee discharged,  bill  amended,\n      ordered  reprinted  as  amended  and  recommitted to said committee --\n      reported  and  referred  to  the  Committee  on  Codes  --   committee\n      discharged, bill amended, ordered reprinted as amended and recommitted\n      to said committee\n\n   AN  ACT  to  amend the general business law, in relation to the training\n      and use of artificial intelligence frontier models\n\n      The People of the State of New York, represented in Senate and  Assem-\n   bly, do enact as follows:\n\nSection  1.  Short  title. This act shall be known and may be cited as\nthe \"Responsible AI safety and education act\" or \"RAISE act\".", "tags": ["Risk factors: Transparency", "Strategies: Tiering", "Strategies: Tiering: Tiering based on inputs", "Harms: Harm to health/safety", "Harms: Financial loss", "Applications: Education", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Risk factors: Safety", "Harms: Harm to health/safety", "Harms: Financial loss", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure", "Strategies: Disclosure: About incidents", "Incentives: Civil liability"], "source": "https://nyassembly.gov/leg/?default_fld=&leg_video=&bn=A06453&term=&Text=Y", "official_name": "Responsible AI safety and education act", "label": "safe"}
{"id": "2426_9", "doc_id": "2426", "text": "§  1422.  Violations. 1. The attorney general may bring a civil action\nfor a violation of this article and to recover  all  of  the  following,\ndetermined based on severity of the violation:\n(a)  For  a violation   of section fourteen hundred twenty-one of this\narticle, a civil penalty in an amount not exceeding ten million  dollars\nfor  a  first  violation  and  in an amount not exceeding thirty million\ndollars for any subsequent violation.\n(b) For a violation of section fourteen  hundred  twenty-one  of  this\narticle, injunctive or declaratory relief.", "tags": ["Incentives: Civil liability"], "source": "https://nyassembly.gov/leg/?default_fld=&leg_video=&bn=A06453&term=&Text=Y", "official_name": "Responsible AI safety and education act", "label": "safe"}
{"id": "2426_10", "doc_id": "2426", "text": "2.  Nothing  in this article shall be construed to establish a private\nright of action associated with violations of this article.\n3. Nothing in this subdivision shall be construed to prevent  a  large\ndeveloper  from  asserting that another person, entity, or factor may be\nresponsible for any alleged harm, injury, or  damage  resulting  from  a\ncritical harm or a violation of this article.\n4. This section does not limit the application of other laws.", "tags": ["Risk factors: Transparency", "Strategies: Tiering", "Strategies: Tiering: Tiering based on inputs", "Harms: Harm to health/safety", "Harms: Financial loss", "Applications: Education", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Risk factors: Safety", "Harms: Harm to health/safety", "Harms: Financial loss", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure", "Strategies: Disclosure: About incidents", "Incentives: Civil liability"], "source": "https://nyassembly.gov/leg/?default_fld=&leg_video=&bn=A06453&term=&Text=Y", "official_name": "Responsible AI safety and education act", "label": "safe"}
{"id": "2426_11", "doc_id": "2426", "text": "§  1423. Duties and obligations. The duties and obligations imposed by\nthis article are cumulative with any other duties or obligations imposed\nunder other law and shall not be construed to relieve any party from any\nduties or obligations imposed under other  law  and  do  not  limit  any\nrights or remedies under existing law.\n§  1424.  Scope. This article shall only apply to frontier models that\nare developed, deployed, or operating in whole or in part  in  New  York\nstate.\n§ 1425. Severability. If any clause, sentence, paragraph, subdivision,\nsection or part of this article shall be adjudged by any court of compe-\ntent jurisdiction to be invalid, such judgment shall not affect, impair,\nor invalidate the remainder thereof, but shall be confined in its opera-\ntion  to  the clause, sentence, paragraph, subdivision, section, or part\nthereof directly involved in the  controversy  in  which  such  judgment\nshall have been made.\n§  3.  This  act shall take effect on the ninetieth day after it shall\nhave become a law.", "tags": ["Risk factors: Transparency", "Strategies: Tiering", "Strategies: Tiering: Tiering based on inputs", "Harms: Harm to health/safety", "Harms: Financial loss", "Applications: Education", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Risk factors: Safety", "Harms: Harm to health/safety", "Harms: Financial loss", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure", "Strategies: Disclosure: About incidents", "Incentives: Civil liability"], "source": "https://nyassembly.gov/leg/?default_fld=&leg_video=&bn=A06453&term=&Text=Y", "official_name": "Responsible AI safety and education act", "label": "safe"}
{"id": "2666_5", "doc_id": "2666", "text": "22757.12. \n(a) A large frontier developer shall write, implement, comply with, and clearly and conspicuously publish on its internet website a frontier AI framework that applies to the large frontier developer’s frontier models and describes how the large frontier developer approaches all of the following:\n    (1) Incorporating national standards, international standards, and industry-consensus best practices into its frontier AI framework.\n    (2) Defining and assessing thresholds used by the large frontier developer to identify and assess whether a frontier model has capabilities that could pose a catastrophic risk, which may include multiple-tiered\n\tthresholds.\n    (3) Applying mitigations to address the potential for catastrophic risks based on the results of assessments undertaken pursuant to paragraph (2).\n    (4) Reviewing assessments and adequacy of mitigations as part of the decision to deploy a frontier model or use it extensively internally.\n    (5) Using third parties to assess the potential for catastrophic risks and the effectiveness of mitigations of catastrophic risks.\n    (6) Revisiting and updating the frontier AI framework, including any criteria that trigger updates and how the large frontier developer determines when its frontier models are substantially modified enough to\n\trequire disclosures pursuant to subdivision (c).\n    (7) Cybersecurity practices to secure unreleased model weights from unauthorized modification or transfer by internal or external parties.\n    (8) Identifying and responding to critical safety incidents.\n    (9) Instituting internal governance practices to ensure implementation of these processes.\n    (10) Assessing and managing catastrophic risk resulting from the internal use of its frontier models, including risks resulting from a frontier model circumventing oversight mechanisms.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: External auditing", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Performance requirements", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About incidents", "Strategies: Disclosure: In standard form"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202520260SB53", "official_name": "California Senate Bill 53", "label": "safe"}
{"id": "2666_11", "doc_id": "2666", "text": "22757.15. (a) A large frontier developer that fails to publish or transmit a compliant document required to be published or transmitted under this chapter, makes a statement in violation of subdivision (e) of Section 22757.12, fails to report an incident as required by Section 22757.13, or fails to comply with its own frontier AI framework shall be subject to a civil penalty in an amount dependent upon the severity of the violation that does not exceed one million dollars ($1,000,000) per violation.\n(b) A civil penalty described in this section shall be recovered in a civil action brought only by the Attorney General.\n\n22757.16. The loss of value of equity does not count as damage to or loss of property for the purposes of this chapter.", "tags": ["Incentives: Civil liability", "Incentives: Fines"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202520260SB53", "official_name": "California Senate Bill 53", "label": "safe"}
{"id": "2666_17", "doc_id": "2666", "text": "(e) (1) A large frontier developer shall provide a reasonable internal process through which a covered employee may anonymously disclose information to the large frontier developer if the covered employee\n\tbelieves in good faith that the information indicates that the large frontier developer’s activities present a specific and substantial danger to the public health or safety resulting from a catastrophic\n\trisk or that the large frontier developer violated Chapter 25.1 (commencing with Section 22757.10) of Division 8 of the Business and Professions Code, including a monthly update to the person who made the\n\tdisclosure regarding the status of the large frontier developer’s investigation of the disclosure and the actions taken by the large frontier developer in response to the disclosure.\n    (2) (A) Except as provided in subparagraph (B), the disclosures and responses of the process required by this subdivision shall be shared with officers and directors of the large frontier developer at least once\n\teach quarter.\n\t(B) If a covered employee has alleged wrongdoing by an officer or director of the large frontier developer in a disclosure or response, subparagraph (A) shall not apply with respect to that officer or\n\tdirector.\n(f) The court is authorized to award reasonable attorney’s fees to a plaintiff who brings a successful action for a violation of this section.\n(g) In a civil action brought pursuant to this section, once it has been demonstrated by a preponderance of the evidence that an activity proscribed by this section was a contributing factor in the alleged\n\tprohibited action against the covered employee, the frontier developer shall have the burden of proof to demonstrate by clear and convincing evidence that the alleged action would have occurred for\n\tlegitimate, independent reasons even if the covered employee had not engaged in activities protected by this section.", "tags": ["Incentives: Civil liability", "Strategies: Disclosure"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202520260SB53", "official_name": "California Senate Bill 53", "label": "safe"}
{"id": "2666_18", "doc_id": "2666", "text": "(h) (1) In a civil action or administrative proceeding brought pursuant to this section, a covered employee may petition the superior court in any county wherein the violation in question is alleged to have\n\toccurred, or wherein the person resides or transacts business, for appropriate temporary or preliminary injunctive relief.\n    (2) Upon the filing of the petition for injunctive relief, the petitioner shall cause notice thereof to be served upon the person, and thereupon the court shall have jurisdiction to grant temporary injunctive\n\trelief as the court deems just and proper.\n    (3) In addition to any harm resulting directly from a violation of this section, the court shall consider the chilling effect on other covered employees asserting their rights under this section in determining\n\twhether temporary injunctive relief is just and proper.\n    (4) Appropriate injunctive relief shall be issued on a showing that reasonable cause exists to believe a violation has occurred.\n    (5) An order authorizing temporary injunctive relief shall remain in effect until an administrative or judicial determination or citation has been issued, or until the completion of a review pursuant to\n\tsubdivision (b) of Section 98.74, whichever is longer, or at a certain time set by the court. Thereafter, a preliminary or permanent injunction may be issued if it is shown to be just and proper. Any\n\ttemporary injunctive relief shall not prohibit a frontier developer from disciplining or terminating a covered employee for conduct that is unrelated to the claim of the retaliation.\n(i) Notwithstanding Section 916 of the Code of Civil Procedure, injunctive relief granted pursuant to this section shall not be stayed pending appeal.\n(j) (1) This section does not impair or limit the applicability of Section 1102.5, including with respect to the rights of employees who are not covered employees to report violations of this chapter or Chapter 25.1\n\t(commencing with Section 22757.10) of Division 8 of the Business and Professions Code.\n    (2) The remedies provided by this section are cumulative to each other and the remedies or penalties available under all other laws of this state.", "tags": ["Incentives: Civil liability"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202520260SB53", "official_name": "California Senate Bill 53", "label": "safe"}
{"id": "2670_5", "doc_id": "2670", "text": "Measure 1.1 Drawing up and keeping up-to-date model documentation\n\nSignatories, when placing a general-purpose AI model on the market, will have documented at least\nall the information referred to in the Model Documentation Form below (hereafter this information\nis referred to as the ‘Model Documentation’). Signatories may choose to complete the Model\nDocumentation Form provided in the Appendix to comply with this commitment.\n\nSignatories will update the Model Documentation to reflect relevant changes in the information\ncontained in the Model Documentation, including in relation to updated versions of the same model,\nwhile keeping previous versions of the Model Documentation for a period ending 10 years after the\nmodel has been placed on the market.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About inputs", "Strategies: Licensing, registration, and certification", "Strategies: Disclosure: About evaluation"], "source": "https://digital-strategy.ec.europa.eu/en/policies/contents-code-gpai", "official_name": "General Purpose AI Code of Practice, Transparency Chapter", "label": "safe"}
{"id": "301_1", "doc_id": "301", "text": "A bill\n\nTO AMEND THE SOUTH CAROLINA CODE OF LAWS BY ADDING SECTION 63-5-380 SO AS TO PROHIBIT OPERATORS OF INTERNET-BASED APPLICATIONS FROM USING \"AUTOMATED DECISION SYSTEMS\" TO PLACE CONTENT ON SOCIAL MEDIA PLATFORMS FOR USERS UNDER THE AGE OF EIGHTEEN WHO ARE RESIDENTS OF THE STATE OF SOUTH CAROLINA, TO REQUIRE OPERATORS TO PERFORM AGE-VERIFICATION PRACTICES FOR CERTAIN USERS, TO ESTABLISH THAT A VIOLATION IS AN UNFAIR OR DECEPTIVE ACT OR PRACTICE UNDER THE SOUTH CAROLINA UNFAIR TRADE PRACTICES ACT, AND FOR OTHER PURPOSES.\n\nBe it enacted by the General Assembly of the State of South Carolina:", "tags": ["Strategies: Tiering", "Incentives: Civil liability", "Applications: Broadcasting and media production", "Applications: Arts, sports, leisure, travel, and lifestyle"], "source": "https://www.scstatehouse.gov/sess125_2023-2024/bills/404.htm", "official_name": "A bill to amend the South Carolina Code of Laws by adding Section 63-5-380 so as to prohibit operators of internet-based applications from using \"automated decision systems\" to place content on social media platforms for users under the age of eighteen who are residents of the State of South Carolina, to require operators to perform age-verification practices for certain users, to establish that a violation is an unfair or deceptive act or practice under the South Carolina Unfair Trade Practices Act, and for other purposes.", "label": "safe"}
{"id": "301_2", "doc_id": "301", "text": "SECTION 1.   Article 3, Chapter 5, Title 63 of the S.C. Code is amended by adding:\n\n   Section 63-5-380.   (A)(1) It is unlawful for any operator of a website, an online service, or an online or mobile application, including any social media platform, to utilize an automated decision system for content placement, including feeds, posts, advertisements, or product offerings, for a user under the age of eighteen who is a resident of the State of South Carolina.\n\n      (2) An operator that utilizes an automated decision system for content placement for residents of South Carolina who are eighteen years or older shall perform an age verification through an independent, third-party age-verification service that compares information available from public records to the personal information entered by the user to create an account to establish the individual is eighteen years of age or older, unless the operator employs the following protections to ensure age verification:\n\n         (a) the user creates an online profile or account with personal information including, but not limited to, name, address, and a valid phone number, and that personal information is verified through publicly available records; or\n\n         (b) the user is required to upload a copy of his or her government-issued identification in addition to a current photograph of the user.\n\nAge verification that is required, is shown, and reasonably is relied upon for the user's proof of age pursuant to this item is a defense to an action initiated pursuant to this section.\n\n      (3) Failure of an operator to perform age verification as required by item (2) is prima facie evidence of violation of this section.", "tags": ["Strategies: Tiering", "Incentives: Civil liability", "Applications: Broadcasting and media production", "Applications: Arts, sports, leisure, travel, and lifestyle"], "source": "https://www.scstatehouse.gov/sess125_2023-2024/bills/404.htm", "official_name": "A bill to amend the South Carolina Code of Laws by adding Section 63-5-380 so as to prohibit operators of internet-based applications from using \"automated decision systems\" to place content on social media platforms for users under the age of eighteen who are residents of the State of South Carolina, to require operators to perform age-verification practices for certain users, to establish that a violation is an unfair or deceptive act or practice under the South Carolina Unfair Trade Practices Act, and for other purposes.", "label": "safe"}
{"id": "301_3", "doc_id": "301", "text": "(B)(1) A violation of subsection (A) is considered an unfair or deceptive act or practice under the South Carolina Unfair Trade Practices Act, pursuant to Section 39-5-20, and is enforceable by the Attorney General through an action for an injunction or for civil penalties, or both, pursuant to Sections 39-5-50 and 39-5-110.\n\n      (2) A user who suffers damages as a result of a violation of subsection (A) may bring an action individually to recover actual damages from the operator pursuant to Section 39-5-140.\n\n      (3) For purposes of calculating civil penalties pursuant to Section 39-5-110, each time an operator utilizes an automated decision system in violation of this section is considered a separate violation for each user under the age of eighteen who is a resident of this State.\n\n   (C) For purposes of this section, \"automated decision system\" means a computational process, including one derived from machine learning, statistics, or other data processing or artificial intelligence techniques, that makes a decision or facilitates human decision making and that impacts consumers.\n\nSECTION 2.   This act takes effect upon approval by the Governor.", "tags": ["Strategies: Tiering", "Incentives: Civil liability", "Applications: Broadcasting and media production", "Applications: Arts, sports, leisure, travel, and lifestyle"], "source": "https://www.scstatehouse.gov/sess125_2023-2024/bills/404.htm", "official_name": "A bill to amend the South Carolina Code of Laws by adding Section 63-5-380 so as to prohibit operators of internet-based applications from using \"automated decision systems\" to place content on social media platforms for users under the age of eighteen who are residents of the State of South Carolina, to require operators to perform age-verification practices for certain users, to establish that a violation is an unfair or deceptive act or practice under the South Carolina Unfair Trade Practices Act, and for other purposes.", "label": "safe"}
{"id": "356_6", "doc_id": "356", "text": "Sec. 616.005.  STATE REGULATORY AUTHORITY; DISCIPLINARY ACTION.  \n\n\n(a)  The appropriate state regulatory authority for the provision of counseling, therapy, or other mental health services must recognize as authorized by this state the artificial intelligence mental health services that are provided through an artificial intelligence technology that has been approved under this chapter.\n\n\n(b)  A person providing artificial intelligence mental health services must comply with the standards regarding professional ethics to which a licensed mental health professional providing the service would be held for providing the service without the use of an artificial intelligence technology.\n\n\n(c)  A person who provides artificial intelligence mental health services and who violates a provision of a professional licensing statute that would be applicable to a licensed mental health professional providing the service is subject to disciplinary action as provided by that licensing statute regardless of whether the person is licensed under that statute.", "tags": ["Applications: Medicine, life sciences and public health", "Incentives: Civil liability"], "source": "https://legiscan.com/TX/bill/HB4695/2023", "official_name": "Texas HB 4695 ", "label": "safe"}
{"id": "37_1", "doc_id": "37", "text": "Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)\nArticle 1: In order to stimulate the healthy development and standardized application of generative artificial intelligence (AI), on the basis of the Cybersecurity Law of the People’s Republic of China, the Data Security Law of the People’s Republic of China, the Personal Information Protection Law of the People’s Republic of China, and other such laws and administrative regulations, these Measures are formulated.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Harm to property", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-measures-for-the-management-of-generative-artificial-intelligence-services-draft-for-comment-april-2023/", "official_name": "Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)", "label": "safe"}
{"id": "37_2", "doc_id": "37", "text": "Article 2: These Measures apply to the research, development, and use of products with generative AI functions, and to the provision of services to the public within the [mainland] territory of the People’s Republic of China.\n\nGenerative AI, as mentioned in these Measures, refers to technologies generating text, image, audio, video, code, or other such content based on algorithms, models, or rules.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Harm to property", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-measures-for-the-management-of-generative-artificial-intelligence-services-draft-for-comment-april-2023/", "official_name": "Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)", "label": "safe"}
{"id": "37_3", "doc_id": "37", "text": "Article 3: The State supports indigenous innovation, broad application, and international cooperation in foundational technologies such as AI algorithms and frameworks, and encourages the prioritized use of secure and reliable software, tools, computing, and data resources.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Harm to property", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-measures-for-the-management-of-generative-artificial-intelligence-services-draft-for-comment-april-2023/", "official_name": "Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)", "label": "safe"}
{"id": "37_4", "doc_id": "37", "text": "Article 4: The provision of generative AI products or services shall abide by the requirements of laws and regulations, respect social virtue and good public customs, and conform to the following requirements:\n\nContent generated through the use of generative AI shall reflect the Socialist Core Values, and may not contain: subversion of state power; overturning of the socialist system; incitement of separatism; harm to national unity; propagation of terrorism or extremism; propagation of ethnic hatred or ethnic discrimination; violent, obscene, or sexual information; false information; as well as content that may upset economic order or social order.\nIn processes such as algorithm design, selecting training data, model generation and model optimization, service provision, etc., adopt measures to prevent the emergence of discrimination on the basis of race, ethnicity, religious belief, nationality, region, sex, age, or profession.\nRespect intellectual property rights and commercial ethics; advantages in algorithms, data, platforms, etc., may not be used to engage in unfair competition. \nContent generated through the use of generative AI shall be true and accurate, and measures are to be adopted to prevent the generation of false information. \nRespect the lawful rights and interests of others; prevent harm to the physical and mental health of others, infringement of their likeness rights, reputation rights and personal privacy, as well as infringement of intellectual property rights. It is prohibited to illegally obtain, divulge or use personal information and private [information], as well as commercial secrets.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Harm to property", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-measures-for-the-management-of-generative-artificial-intelligence-services-draft-for-comment-april-2023/", "official_name": "Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)", "label": "safe"}
{"id": "37_5", "doc_id": "37", "text": "Article 5: Organizations or individuals that use generative AI to provide services such as chat, text, image, or audio generation (hereinafter referred to as “providers”); including providing programmable interfaces [i.e., APIs] and other means which support others to themselves generate text, images, audio, etc.; bear responsibility as the producer of the content generated by the product. Where personal information is involved, they bear legal responsibility as personal information handlers and are to fulfill personal information protection obligations.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Harm to property", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-measures-for-the-management-of-generative-artificial-intelligence-services-draft-for-comment-april-2023/", "official_name": "Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)", "label": "safe"}
{"id": "37_6", "doc_id": "37", "text": "Article 6: Before using generative AI products to provide services to the public, a security assessment must be submitted to the state cyberspace and information department [i.e., the Cyberspace Administration of China] in accordance with the Provisions on the Security Assessment of Internet Information Services With Public Opinion Properties or Social Mobilization Capacity, and the procedures of algorithm filing, modification, and cancellation of filing must be carried out in accordance with the Internet Information Service Algorithmic Recommendation Management Provisions.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Harm to property", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-measures-for-the-management-of-generative-artificial-intelligence-services-draft-for-comment-april-2023/", "official_name": "Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)", "label": "safe"}
{"id": "37_7", "doc_id": "37", "text": "Article 7: Providers shall bear responsibility for the legality of the sources of generative AI product pre-training data and optimization training data.\n\nData used for generative AI product pre-training and optimization training shall satisfy the following requirements:\n\nConforming to the requirements of the Cybersecurity Law of the People’s Republic of China and other such laws and regulations;\nNot containing content infringing intellectual property rights;\nWhere data includes personal information, the consent of the personal information subject shall be obtained, or other procedures conforming with the provisions of laws and administrative regulations followed;\nBe able to ensure the data’s veracity, accuracy, objectivity, and diversity;\nOther supervision requirements of the state cybersecurity and informatization department concerning generative AI functions and services.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Harm to property", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-measures-for-the-management-of-generative-artificial-intelligence-services-draft-for-comment-april-2023/", "official_name": "Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)", "label": "safe"}
{"id": "37_8", "doc_id": "37", "text": "Article 8: When human annotation is used in the development of generative AI products, providers shall formulate clear, specific, and practicable annotation rules conforming to the requirements of these Measures; necessary training of annotation personnel shall be conducted; and the validity of annotation content shall be spot checked.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Harm to property", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-measures-for-the-management-of-generative-artificial-intelligence-services-draft-for-comment-april-2023/", "official_name": "Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)", "label": "safe"}
{"id": "37_9", "doc_id": "37", "text": "Article 9: When providing generative AI services, users shall be required to provide real identity information in accordance with the provisions of the Cybersecurity Law of the People’s Republic of China.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Harm to property", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-measures-for-the-management-of-generative-artificial-intelligence-services-draft-for-comment-april-2023/", "official_name": "Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)", "label": "safe"}
{"id": "37_10", "doc_id": "37", "text": "Article 10: Providers shall explicitly disclose the user groups, occasions, and uses for their services, and adopt appropriate measures to prevent users from excessive reliance on or addiction to generated content.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Harm to property", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-measures-for-the-management-of-generative-artificial-intelligence-services-draft-for-comment-april-2023/", "official_name": "Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)", "label": "safe"}
{"id": "37_11", "doc_id": "37", "text": "Article 11: In the process of providing services, providers have the duty to protect information input by users and usage records. They may not illegally preserve input information from which it is possible to deduce the identity of users, they may not conduct profiling on the basis of information input by users and their usage details, and they may not provide information input by users to others. Where laws or regulations provide otherwise, those provisions are to be followed.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Harm to property", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-measures-for-the-management-of-generative-artificial-intelligence-services-draft-for-comment-april-2023/", "official_name": "Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)", "label": "safe"}
{"id": "37_13", "doc_id": "37", "text": "Article 13: Providers shall establish mechanisms for receiving and handling user complaints and promptly handle individual requests concerning revision, deletion, or masking of their personal information; and when they discover or learn that generated text, images, audio, video, etc., infringe other persons’ likeness rights, reputation rights, personal privacy, or commercial secrets, or do not conform to the demands of these Measures, they shall adopt measures to cease generation and prevent the expansion of the harm.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Harm to property", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-measures-for-the-management-of-generative-artificial-intelligence-services-draft-for-comment-april-2023/", "official_name": "Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)", "label": "safe"}
{"id": "37_15", "doc_id": "37", "text": "Article 15: When generated content that does not conform to the requirements of these Measures is discovered during operations or reported by users, aside from adopting content filtering and other such measures, repeat generation is to be prevented through such methods as optimization training within three months.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Harm to property", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-measures-for-the-management-of-generative-artificial-intelligence-services-draft-for-comment-april-2023/", "official_name": "Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)", "label": "safe"}
{"id": "37_17", "doc_id": "37", "text": "Article 17: Providers shall, in accordance with the requirements of the state cybersecurity and informatization department and relevant responsible departments, provide necessary information that could influence users trust or choices, including descriptions of the source, scale, type, quality, etc., of pre-training and optimization training data; rules for human annotation; the scale and type of human-annotated data; and foundational algorithms and technological systems.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Harm to property", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-measures-for-the-management-of-generative-artificial-intelligence-services-draft-for-comment-april-2023/", "official_name": "Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)", "label": "safe"}
{"id": "37_18", "doc_id": "37", "text": "Article 18: Providers shall guide users to scientifically understand and rationally use content generated by generative AI; not to use generated content to damage others’ image, reputation, or other lawful rights and interests; and not to engage in commercial hype or improper marketing.\n\nWhen users discover generated content that does not meet the requirements of these measures, they have the right to report this to cybersecurity and informatization departments or relevant responsible departments.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Harm to property", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-measures-for-the-management-of-generative-artificial-intelligence-services-draft-for-comment-april-2023/", "official_name": "Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)", "label": "safe"}
{"id": "37_19", "doc_id": "37", "text": "Article 19: If a provider finds that a user has used generative AI products to violate laws or regulations; violate business ethics or social virtue, including engaging in online hype, malicious posting and commenting, creating spam, or writing malicious software; or engage in improper business marketing; etc.; service shall be suspended or terminated.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Harm to property", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-measures-for-the-management-of-generative-artificial-intelligence-services-draft-for-comment-april-2023/", "official_name": "Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)", "label": "safe"}
{"id": "37_20", "doc_id": "37", "text": "Article 20: If a provider violates the provisions of the Measures, the cybersecurity and informatization department and relevant responsible departments are to impose penalties in accordance with the provisions of Cybersecurity Law of the People’s Republic of China, the Data Security Law of the People’s Republic of China, the Personal Information Protection Law of the People’s Republic of China, and other such laws and administrative regulations.\n\nWhere there are no provisions of law or administrative regulation, the cybersecurity and informatization department and relevant responsible departments are to, in accordance with their duties, issue warnings, circulate criticisms, and order corrections within a set period of time. Where corrections are refused or circumstances are grave, they are to order suspension or termination of their use of generative AI provider services, and a penalty more than 10,000 yuan and less than 100,000 yuan is to be imposed. Where behavior constitutes a violation of public security management, public security management penalties are to be imposed in accordance with the law. Where a crime is constituted, criminal responsibility shall be pursued in accordance with the law.\n\nArticle 21: These measures are effective beginning [day] [month], 2023.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Harm to property", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-measures-for-the-management-of-generative-artificial-intelligence-services-draft-for-comment-april-2023/", "official_name": "Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)", "label": "safe"}
{"id": "420_4", "doc_id": "420", "text": "(f) Data Access And Related Authorities.—\n(1) DATA ACCESS.—The Secretary of Defense shall ensure that the Joint Autonomy Office—\n(A) subject to the implementation of the security protocols described in paragraph (2), has unlimited access to existing datasets within service and joint autonomous system, artificial intelligence, and legacy platform and sensor programs regardless of classification level; and\n(B) has the authority to centralize access to such data and information, as required to carry out the duties of the Office.\n(2) SECURITY PROTOCOLS.—The Secretary of Defense shall implement appropriate security protocols to protect any data and other information shared with or maintained by the Office under paragraph (1).\n(g) Contracting Authorities And Limitations.—", "tags": ["Risk factors: Security: Cybersecurity", "Strategies: Input controls: Data use", "Strategies: Input controls: Data circulation", "Applications: Government: military and public safety"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/3168", "official_name": "To establish a Joint Autonomy Office in the Department of Defense, and for other purposes.", "label": "safe"}
{"id": "420_5", "doc_id": "420", "text": "(1) IN GENERAL.—The Director of the Office—\n(A) subject to paragraph (2), may enter into contracts with private sector, non-Government entities, as appropriate, for the purposes of leveraging innovative commercial technologies to aid and facilitate the execution of the duties of the Office; and\n(B) shall ensure that all programs administered by the Office are carried out in accordance with section 3453 of title 10, United States Code.", "tags": ["Strategies: Licensing, registration, and certification", "Applications: Government: military and public safety"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/3168", "official_name": "To establish a Joint Autonomy Office in the Department of Defense, and for other purposes.", "label": "safe"}
{"id": "457_16", "doc_id": "457", "text": "(c) Employment-Related decisions.—An employer that makes an employment-related decision with regard to a covered individual using data from workplace surveillance shall—\n\n(1) not later than 7 days after making such an employment-related decision, disclose to the covered individual that such employment-related decision was made using data from workplace surveillance; and\n\n(2) not later than 7 days after such disclosure, enable the covered individual to—\n\n(A) review such data and related aggregated data for other similarly situated covered individuals of the employer; and\n\n(B) in accordance with the procedures described in section 3(e)(2), have any data described in paragraph (1) that is incomplete or erroneous updated or corrected.", "tags": ["Strategies: Disclosure", "Risk factors: Reliability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/262", "official_name": "Stop Spying Bosses Act", "label": "safe"}
{"id": "532_2", "doc_id": "532", "text": "SEC. 2. CONTENT MODERATION, CREATION AND DEVELOPMENT, AND DISTRIBUTION.\n(a) Treatment As Publisher Or Speaker Contingent On Content Management Practices.—Section 230 of the Communications Act of 1934 (47 U.S.C. 230) is amended—\n(1) in subsection (c)(1)—\n(A) by striking “No provider” and inserting the following:\n“(A) IN GENERAL.—Subject to subparagraph (B), no provider”; and\n(B) by adding at the end the following:", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/921", "official_name": "Disincentivizing Internet Service Censorship of Online Users and Restrictions on Speech and Expression Act", "label": "safe"}
{"id": "532_3", "doc_id": "532", "text": "“(B) NOTIFICATION OF PARENTAL CONTROL PROTECTIONS.—Subparagraph (A) shall not apply to a provider of an interactive computer service with a dominant market share that violates subsection (d).”; and\n(2) in subsection (f)—\n(A) in paragraph (3)—\n(i) by striking “The term” and inserting the following:\n“(A) IN GENERAL.—The term”; and\n(ii) by adding at the end the following:", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/921", "official_name": "Disincentivizing Internet Service Censorship of Online Users and Restrictions on Speech and Expression Act", "label": "safe"}
{"id": "532_4", "doc_id": "532", "text": "“(B) CONTENT MODERATION.—If an interactive computer service provider with a dominant market share—\n“(i) engages in a content moderation activity that reasonably appears to express, promote, or suppress a discernible viewpoint for a reason that is not protected from liability under subsection (c)(2), including reducing or eliminating the ability of an information content provider to earn revenue, with respect to any information, the interactive computer service provider shall be deemed to be an information content provider with respect to that information; or\n“(ii) engages in a pattern or practice of content moderation activity that reasonably appears to express, promote, or suppress a discernible viewpoint for a reason that is not protected from liability under subsection (c)(2), including reducing or eliminating the ability of an information content provider to earn revenue, the interactive computer service provider shall be deemed to be an information content provider with respect to all information that is provided through the interactive computer service.", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/921", "official_name": "Disincentivizing Internet Service Censorship of Online Users and Restrictions on Speech and Expression Act", "label": "safe"}
{"id": "532_5", "doc_id": "532", "text": "“(C) USE OF TARGETED ALGORITHMIC AMPLIFICATION.—\n“(i) IN GENERAL.—If an interactive computer service provider with a dominant market share—\n“(I) amplifies information provided by an information content provider by using an algorithm or other automated computer process to target the information directly to users without the request of the sending or receiving user, the interactive computer service provider shall be deemed to be an information content provider with respect to that information; or\n“(II) engages in a pattern or practice of amplifying information provided by an information content provider by using an algorithm or other automated computer process to target the information directly to users without the request of the sending or receiving user, the interactive computer service provider shall be deemed to be an information content provider with respect to all information that is provided through the interactive computer service.", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/921", "official_name": "Disincentivizing Internet Service Censorship of Online Users and Restrictions on Speech and Expression Act", "label": "safe"}
{"id": "532_6", "doc_id": "532", "text": "“(ii) EXCEPTIONS.—Clause (i) shall not apply to the use of an algorithm or other computer process to—\n“(I) amplify or target directly to a user any information that is the result of a search function performed by the user; or\n“(II) sort data chronologically or alphabetically.", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/921", "official_name": "Disincentivizing Internet Service Censorship of Online Users and Restrictions on Speech and Expression Act", "label": "safe"}
{"id": "532_7", "doc_id": "532", "text": "“(D) INFORMATION CREATION OR DEVELOPMENT.—If an interactive computer service provider with a dominant market share—\n“(i) solicits, comments upon, funds, or affirmatively and substantively contributes to, modifies, or alters information provided by an information content provider, the interactive computer service provider shall be deemed to be an information content provider with respect to that information; or\n“(ii) engages in a pattern or practice of soliciting, commenting upon, funding, or affirmatively and substantively contributing to, modifying, or altering information provided by an information content provider, the interactive computer service provider shall be deemed to be an information content provider with respect to all information that is provided through the interactive computer service.”; and", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/921", "official_name": "Disincentivizing Internet Service Censorship of Online Users and Restrictions on Speech and Expression Act", "label": "safe"}
{"id": "532_8", "doc_id": "532", "text": "(B) by adding at the end the following:\n“(5) CONTENT MODERATION ACTIVITY.—The term ‘content moderation activity’ means editing, deleting, throttling, limiting the reach of, reducing or eliminating the ability of an information content provider to earn revenue from, or commenting upon, information provided by an information content provider, or terminating or limiting an account or usership, if the activity is based on content-based criteria.\n“(6) PATTERN OR PRACTICE.—The term ‘pattern or practice’ means any formal or informal policy or rule, whether created by a human or generated by a computer, as applied or used by an interactive computer service provider.”.", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/921", "official_name": "Disincentivizing Internet Service Censorship of Online Users and Restrictions on Speech and Expression Act", "label": "safe"}
{"id": "532_9", "doc_id": "532", "text": "(b) Clarifying Categories Of Objectionable Material.—Section 230(c)(2) of the Communications Act of 1934 (47 U.S.C. 230(c)(2)) is amended—\n(1) in subparagraph (A)—\n(A) by striking “considers to be” and inserting “has an objectively reasonable belief is”;\n(B) by inserting “promoting terrorism or violent extremism,” after “violent,”; and\n(C) by striking “or otherwise objectionable” and inserting “promoting self-harm, or unlawful”; and\n(2) in subparagraph (B), by striking “paragraph (1)” and inserting “subparagraph (A)”.\n(c) Religious Liberty Exception To Civil Liability Protections.—Section 230(c)(2) of the Communications Act of 1934 (47 U.S.C. 230(c)(2)), as amended by subsection (b), is amended—\n(1) by redesignating subparagraphs (A) and (B) as clauses (i) and (ii), respectively, and adjusting the margins accordingly;\n(2) by striking “No provider” and inserting the following:\n“(A) IN GENERAL.—Except as provided in subparagraph (B), no provider”;\n(3) in subparagraph (A)(ii), as so designated, by striking “subparagraph (A)” and inserting “clause (i)”; and\n(4) by adding at the end the following:", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/921", "official_name": "Disincentivizing Internet Service Censorship of Online Users and Restrictions on Speech and Expression Act", "label": "safe"}
{"id": "532_10", "doc_id": "532", "text": "“(B) RELIGIOUS LIBERTY EXCEPTION.—Subparagraph (A) shall not apply to any action taken with respect to religious material in a manner that burdens the exercise of religion, as defined in section 5 of the Religious Freedom Restoration Act of 1993 (42 U.S.C. 2000bb –2).”.\n(d) Disclosure Of Content Management Mechanisms And Practices.—Section 230(d) of the Communications Act of 1934 (47 U.S.C. 230(d)) is amended—\n(1) by striking “A provider” and inserting the following:\n“(1) PARENTAL CONTROL PROTECTIONS.—A provider”; and\n(2) by adding at the end the following:", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/921", "official_name": "Disincentivizing Internet Service Censorship of Online Users and Restrictions on Speech and Expression Act", "label": "safe"}
{"id": "532_11", "doc_id": "532", "text": "“(2) DISCLOSURE OF CONTENT MANAGEMENT MECHANISMS AND PRACTICES.—\n“(A) IN GENERAL.—A provider of an interactive computer service that provides the service through a mass-market offering to the public shall publicly disclose accurate information regarding the content moderation activity of the service, including editing, deleting, throttling, limiting the reach of, reducing or eliminating the ability of an information content provider to earn revenue from, or commenting upon, information provided by an information content provider, terminating or limiting an account or usership, and any other content moderation, promotion, and other curation practices, sufficient to enable—\n“(i) consumers to make informed choices regarding the purchase and use of the service; and\n“(ii) entrepreneurs and other small businesses to develop, market, and maintain offerings by means of the service.", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/921", "official_name": "Disincentivizing Internet Service Censorship of Online Users and Restrictions on Speech and Expression Act", "label": "safe"}
{"id": "532_12", "doc_id": "532", "text": "“(B) MANNER OF DISCLOSURE.—A provider of an interactive computer service shall make the disclosure under subparagraph (A)—\n“(i) through a publicly available, easily accessible website; or\n“(ii) by submitting the information described in that subparagraph to the Commission, which shall make the information available to the public through the website of the Commission.”.", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/921", "official_name": "Disincentivizing Internet Service Censorship of Online Users and Restrictions on Speech and Expression Act", "label": "safe"}
{"id": "532_13", "doc_id": "532", "text": "(e) Clarifying That Immunity Is An Affirmative Defense.—Section 230(c)(1) of the Communications Act of 1934 (47 U.S.C. 230(c)(1)), as amended by subsection (a)(1), is amended—\n(1) in subparagraph (A), as so designated, by striking “subparagraph (B)” and inserting “subparagraphs (B) and (C)”; and\n(2) by adding at the end the following:", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/921", "official_name": "Disincentivizing Internet Service Censorship of Online Users and Restrictions on Speech and Expression Act", "label": "safe"}
{"id": "532_14", "doc_id": "532", "text": "“(C) AFFIRMATIVE DEFENSE.—In a criminal or civil action against a provider or user of an interactive computer service that treats the provider or user as the publisher or speaker of any information, the provider or user shall bear the burden of proving that the provider or user is not an information content provider with respect to that information for purposes of subparagraph (A).”.", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/921", "official_name": "Disincentivizing Internet Service Censorship of Online Users and Restrictions on Speech and Expression Act", "label": "safe"}
{"id": "72_1", "doc_id": "72", "text": "Int. No. 1894-A\n\nBy Council Members Cumbo, Ampry-Samuel, Rosenthal, Cornegy, Kallos, Adams, Louis, Chin, Cabrera, Rose, Gibson, Brannan, Rivera, Levine, Ayala, Miller, Levin and Barron\n\nA Local Law to amend the administrative code of the city of New York, in relation to automated employment decision tools\n\nBe it enacted by the Council as follows:\n \nSection 1. Chapter 5 of title 20 of the administrative code of the city of New York is amended by adding a new subchapter 25 to read as follows:\n\nSubchapter 25\n\nAutomated Employment Decision Tools\n\n§ 20-870 Definitions. For the purposes of this subchapter, the following terms have the following meanings:\n\nAutomated employment decision tool. The term “automated employment decision tool” means any computational process, derived from machine learning, statistical modeling, data analytics, or artificial intelligence, that issues simplified output, including a score, classification, or recommendation, that is used to substantially assist or replace discretionary decision making for making employment decisions that impact natural persons. The term “automated employment decision tool” does not include a tool that does not automate, support, substantially assist or replace discretionary decision-making processes and that does not materially impact natural persons, including, but not limited to, a junk email filter, firewall, antivirus software, calculator, spreadsheet, database, data set, or other compilation of data.\n\nBias audit. The term “bias audit” means an impartial evaluation by an independent auditor. Such bias audit shall include but not be limited to the testing of an automated employment decision tool to assess the tool’s disparate impact on persons of any component 1 category required to be reported by employers pursuant to subsection (c) of section 2000e-8 of title 42 of the United States code as specified in part 1602.7 of title 29 of the code of federal regulations.\n\nEmployment decision. The term “employment decision” means to screen candidates for employment or employees for promotion within the city.", "tags": ["Risk factors: Bias", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: New institution", "Incentives: Civil liability", "Incentives: Fines", "Applications: Business services and analytics"], "source": "https://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=B051915D-A9AC-451E-81F8-6596032FA3F9&Options=ID%7CText%7C&Search=", "official_name": "A Local Law to amend the administrative code of the city of New York, in relation to automated employment decision tools", "label": "safe"}
{"id": "72_2", "doc_id": "72", "text": "§ 20-871 Requirements for automated employment decision tools. a. In the city, it shall be unlawful for an employer or an employment agency to use an automated employment decision tool to screen a candidate or employee for an employment decision unless:\n\n1. Such tool has been the subject of a bias audit conducted no more than one year prior to the use of such tool; and\n\n1. A summary of the results of the most recent bias audit of such tool as well as the distribution date of the tool to which such audit applies has been made publicly available on the website of the employer or employment agency prior to the use of such tool.", "tags": ["Risk factors: Bias", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: New institution", "Incentives: Civil liability", "Incentives: Fines", "Applications: Business services and analytics"], "source": "https://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=B051915D-A9AC-451E-81F8-6596032FA3F9&Options=ID%7CText%7C&Search=", "official_name": "A Local Law to amend the administrative code of the city of New York, in relation to automated employment decision tools", "label": "safe"}
{"id": "72_3", "doc_id": "72", "text": "b. Notices required. In the city, any employer or employment agency that uses an automated employment decision tool to screen an employee or a candidate who has applied for a position for an employment decision shall notify each such employee or candidate who resides in the city of the following:\n\n1. That an automated employment decision tool will be used in connection with the assessment or evaluation of such employee or candidate that resides in the city. Such notice shall be made no less than ten business days before such use and allow a candidate to request an alternative selection process or accommodation;\n\n1. The job qualifications and characteristics that such automated employment decision tool will use in the assessment of such candidate or employee. Such notice shall be made no less than 10 business days before such use; and\n\n1. If not disclosed on the employer or employment agency’s website, information about the type of data collected for the automated employment decision tool, the source of such data and the employer or employment agency’s data retention policy shall be available upon written request by a candidate or employee.  Such information shall be provided within 30 days of the written request. Information pursuant to this section shall not be disclosed where such disclosure would violate local, state, or federal law, or interfere with a law enforcement investigation.", "tags": ["Risk factors: Bias", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: New institution", "Incentives: Civil liability", "Incentives: Fines", "Applications: Business services and analytics"], "source": "https://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=B051915D-A9AC-451E-81F8-6596032FA3F9&Options=ID%7CText%7C&Search=", "official_name": "A Local Law to amend the administrative code of the city of New York, in relation to automated employment decision tools", "label": "safe"}
{"id": "72_4", "doc_id": "72", "text": "§ 20-872 Penalties. a. Any person that violates any provision of this subchapter or any rule promulgated pursuant to this subchapter is liable for a civil penalty of not more than $500 for a first violation and each additional violation occurring on the same day as the first violation, and not less than $500 nor more than $1,500 for each subsequent violation.\n\nb. Each day on which an automated employment decision tool is used in violation of this section shall give rise to a separate violation of subdivision a of section 20-871.\n\nc. Failure to provide any notice to a candidate or an employee in violation of paragraphs 1, 2 or 3 of subdivision b of section 20-871 shall constitute a separate violation.\n\nd. A proceeding to recover any civil penalty authorized by this subchapter is returnable to any tribunal established within the office of administrative trials and hearings or within any agency of the city designated to conduct such proceedings.", "tags": ["Risk factors: Bias", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: New institution", "Incentives: Civil liability", "Incentives: Fines", "Applications: Business services and analytics"], "source": "https://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=B051915D-A9AC-451E-81F8-6596032FA3F9&Options=ID%7CText%7C&Search=", "official_name": "A Local Law to amend the administrative code of the city of New York, in relation to automated employment decision tools", "label": "safe"}
{"id": "72_5", "doc_id": "72", "text": "§ 20-873 Enforcement. The corporation counsel or such other persons designated by the corporation  counsel  on behalf  of  the  department may initiate in any court of competent jurisdiction any action or proceeding that may be appropriate or necessary for correction of any violation issued pursuant this subchapter, including mandating compliance with the provisions of this chapter or such other relief as may be appropriate.\n\n§ 20-874 Construction. The provisions of this subchapter shall not be construed to limit any right of any candidate or employee for an employment decision to bring a civil action in any court of competent jurisdiction, or to limit the authority of the commission on human rights to enforce the provisions of title 8, in accordance with law.\n\n§ 2.  This local law takes effect on January 1, 2023.", "tags": ["Risk factors: Bias", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: New institution", "Incentives: Civil liability", "Incentives: Fines", "Applications: Business services and analytics"], "source": "https://legistar.council.nyc.gov/LegislationDetail.aspx?ID=4344524&GUID=B051915D-A9AC-451E-81F8-6596032FA3F9&Options=ID%7CText%7C&Search=", "official_name": "A Local Law to amend the administrative code of the city of New York, in relation to automated employment decision tools", "label": "safe"}
{"id": "73_1", "doc_id": "73", "text": "PART 876—GASTROENTEROLOGY-UROLOGY DEVICES\n\n1. The authority citation for part 876 continues to read as follows: \n\nAuthority:\n\n21 U.S.C. 351, 360, 360c, 360e, 360j, 360l, 371.\n\n1. Add § 876.1520 to subpart B to read as follows: \n\n§ 876.1520\n\nGastrointestinal lesion software detection system.\n\n(a) \n\nIdentification.\n\nA gastrointestinal lesion software detection system is a computer-assisted detection device used in conjunction with endoscopy for the detection of abnormal lesions in the gastrointestinal tract. This device with advanced software algorithms brings attention to images to aid in the detection of lesions. The device may contain hardware to support interfacing with an endoscope.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2023/01/03/2022-28494/medical-devices-gastroenterology-urology-devices-classification-of-the-gastrointestinal-lesion", "official_name": "21 CFR § 876.1520 (\"Gastrointestinal lesion software detection system\")", "label": "safe"}
{"id": "73_2", "doc_id": "73", "text": "(b) \n\nClassification.\n\nClass II (special controls). The special controls for this device are:\n\n(1) Clinical performance testing must demonstrate that the device performs as intended under anticipated conditions of use, including detection of gastrointestinal lesions and evaluation of all adverse events.\n\n(2) Non-clinical performance testing must demonstrate that the device performs as intended under anticipated conditions of use. Testing must include:\n\n(i) Standalone algorithm performance testing;\n\n(ii) Pixel-level comparison of degradation of image quality due to the device;\n\n(iii) Assessment of video delay due to marker annotation; and\n\n(iv) Assessment of real-time endoscopic video delay due to the device.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2023/01/03/2022-28494/medical-devices-gastroenterology-urology-devices-classification-of-the-gastrointestinal-lesion", "official_name": "21 CFR § 876.1520 (\"Gastrointestinal lesion software detection system\")", "label": "safe"}
{"id": "73_3", "doc_id": "73", "text": "(3) Usability assessment must demonstrate that the intended user(s) can safely and correctly use the device.\n\n(4) Performance data must demonstrate electromagnetic compatibility and electrical safety, mechanical safety, and thermal safety testing for any hardware components of the device.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2023/01/03/2022-28494/medical-devices-gastroenterology-urology-devices-classification-of-the-gastrointestinal-lesion", "official_name": "21 CFR § 876.1520 (\"Gastrointestinal lesion software detection system\")", "label": "safe"}
{"id": "73_4", "doc_id": "73", "text": "(5) Software verification, validation, and hazard analysis must be provided. Software description must include a detailed, technical description including the impact of any software and hardware on the device's functions, the associated capabilities and limitations of each part, the associated inputs and outputs, mapping of the software architecture, and a description of the video signal pipeline.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2023/01/03/2022-28494/medical-devices-gastroenterology-urology-devices-classification-of-the-gastrointestinal-lesion", "official_name": "21 CFR § 876.1520 (\"Gastrointestinal lesion software detection system\")", "label": "safe"}
{"id": "73_5", "doc_id": "73", "text": "(6) Labeling must include:\n\n(i) Instructions for use, including a detailed description of the device and compatibility information;\n\n(ii) Warnings to avoid overreliance on the device, that the device is not intended to be used for diagnosis or characterization of lesions, and that the device does not replace clinical decision making;\n\n(iii) A summary of the clinical performance testing conducted with the device, including detailed definitions of the study endpoints and statistical confidence intervals; and\n\n(iv) A summary of the standalone performance testing and associated statistical analysis.\n\nDated: December 27, 2022.\n\nLauren K. Roth,\n\nAssociate Commissioner for Policy.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2023/01/03/2022-28494/medical-devices-gastroenterology-urology-devices-classification-of-the-gastrointestinal-lesion", "official_name": "21 CFR § 876.1520 (\"Gastrointestinal lesion software detection system\")", "label": "safe"}
{"id": "768_4", "doc_id": "768", "text": "2.\tCapability Thresholds and Required Safeguards\n\nBelow, we specify the Capability Thresholds and their corresponding Required Safeguards. The Required Safeguards for each Capability Threshold are intended to mitigate risk from a model with such capabilities to acceptable levels. In developing these standards, we have weighed the risks and beneﬁts of frontier model development. We believe these safeguards are achievable with sufficient investment and advance planning into research and development and would advocate for the industry as a whole to adopt them. We will conduct assessments to inform when to implement the Required Safeguards (see Section 4). The Capability Thresholds summarized below are available in full in Appendix C.", "tags": ["Strategies: Evaluation", "Strategies: Tiering", "Risk factors: Reliability", "Strategies: Evaluation: Impact assessment"], "source": "https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf", "official_name": "Anthropic Responsible Scaling Policy", "label": "safe"}
{"id": "768_6", "doc_id": "768", "text": "Capability Threshold:\nAutonomous AI Research and Development (AI R&D): The ability to either fully automate the work of an entry-level remote-only Researcher at Anthropic, or cause dramatic acceleration in the rate of effective scaling.\nRequired Safeguards:\nThis capability could greatly increase the pace of AI development, potentially leading to rapid and unpredictable advances in AI capabilities and associated risks. At minimum, the ASL-3 Security Standard is required, although we expect a higher security standard (which would protect against model-weight theft by state-level adversaries) will be required, especially in the case of dramatic acceleration. We also expect a strong affirmative case (made with accountability for both the reasoning and implementation) about the risk of models pursuing misaligned goals will be required.", "tags": ["Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on impact", "Risk factors: Security", "Risk factors: Reliability", "Risk factors: Reliability: Robustness"], "source": "https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf", "official_name": "Anthropic Responsible Scaling Policy", "label": "safe"}
{"id": "768_9", "doc_id": "768", "text": "3.\tCapability Assessment\n\n3.1.\tPreliminary Assessment\n\nWe will routinely test models to determine whether their capabilities fall sufficiently far below the Capability Thresholds such that we are conﬁdent that the ASL-2 Standard remains appropriate. We will ﬁrst conduct preliminary assessments (on both new and existing models, as needed) to determine whether a more comprehensive evaluation is needed. The purpose of this preliminary assessment is to identify whether the model is notably more capable than the last model that underwent a comprehensive assessment.\nThe term “notably more capable” is operationalized as at least one of the following:\n\n1.\tThe model is notably more performant on automated tests in risk-relevant domains (deﬁned as 4x or more in Effective Compute).\n2.\tSix months’ worth of ﬁnetuning and other capability elicitation methods have accumulated.5 This is measured in calendar time, since we do not yet have a metric to estimate the impact of these improvements more precisely.\n\nIn addition, the Responsible Scaling Officer may in their discretion determine that a comprehensive assessment is warranted.\n\nIf a new or existing model is below the “notably more capable” standard, no further testing is necessary.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation: Conformity assessment", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on impact", "Strategies: Evaluation: Impact assessment"], "source": "https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf", "official_name": "Anthropic Responsible Scaling Policy", "label": "safe"}
{"id": "768_10", "doc_id": "768", "text": "3.2.\tComprehensive Assessment\n\nFor models requiring comprehensive testing, we will assess whether the model is unlikely to reach any relevant Capability Thresholds absent surprising advances in widely accessible post-training enhancements. To make the required showing, we will need to satisfy the following criteria:\n\n1.\tThreat model mapping: For each capability threshold, make a compelling case that we have mapped out the most likely and consequential threat models: combinations of actors (if relevant), attack pathways, model capability bottlenecks, and types of harms. We also make a compelling case that there does not exist a threat model that we are not evaluating that represents a substantial amount of risk.\n2.\tEvaluations: Design and run empirical tests that provide strong evidence that the model does not have the requisite skills; explain why the tests yielded such results; and check at test time that the results are attributable to the model’s capabilities rather than issues with the test design. Findings from partner organizations and external evaluations of our models (or similar models) should also be incorporated into the ﬁnal assessment, when available.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Disclosure: About evaluation", "Strategies: Tiering"], "source": "https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf", "official_name": "Anthropic Responsible Scaling Policy", "label": "safe"}
{"id": "768_11", "doc_id": "768", "text": "3.\tElicitation: Demonstrate that, when given enough resources to extrapolate to realistic attackers, researchers cannot elicit sufficiently useful results from the model on the relevant tasks. We should assume that jailbreaks and model weight theft are possibilities, and therefore perform testing on models without safety mechanisms (such as harmlessness training) that could obscure these capabilities. We will also consider the possible performance increase from using resources that a realistic attacker would have access to, such as scaffolding, ﬁnetuning, and expert prompting. At minimum, we will perform basic ﬁnetuning for instruction following, tool use, minimizing refusal rates.\n4.\tForecasting: Make informal forecasts about the likelihood that further training and elicitation will improve test results between the time of testing and the next expected round of comprehensive testing.8\nThis testing and the subsequent capability decision should ideally be concluded within about a month of reaching the “notably more capable” threshold.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Strategies: Evaluation: Conformity assessment", "Risk factors: Reliability", "Risk factors: Reliability: Robustness"], "source": "https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf", "official_name": "Anthropic Responsible Scaling Policy", "label": "safe"}
{"id": "768_13", "doc_id": "768", "text": "4.\tSafeguards Assessment\n\nTo determine whether the measures we have adopted satisfy the ASL-3 Required Safeguards, we will conduct a safeguards assessment. As noted, the Required Safeguards for each Capability Threshold are speciﬁed in Section 2. We will document our implementation of the Required Safeguards in a Safeguards Report.\n\n4.1.\tASL-3 Deployment Standard\n\nWhen a model must meet the ASL-3 Deployment Standard, we will evaluate whether the measures we have implemented make us robust to persistent attempts to misuse the capability in question. To make the required showing, we will need to satisfy the following criteria:\n\n1.\tThreat modeling: Make a compelling case that the set of threats and the vectors through which an adversary could catastrophically misuse the deployed system have been suiciently mapped out, and will commit to revising as necessary over time.\n2.\tDefense in depth: Use a “defense in depth” approach by building a series of defensive layers, each designed to catch misuse attempts that might pass through previous barriers. As an example, this might entail achieving a high overall recall rate using harm refusal techniques. This is an area of active research, and new technologies may be added when ready.\n3.\tRed-teaming: Conduct red-teaming that demonstrates that threat actors with realistic access levels and resources are highly unlikely to be able to consistently elicit information from any generally accessible systems that greatly increases their ability to cause catastrophic harm relative to other available tools.11", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Reliability: Robustness"], "source": "https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf", "official_name": "Anthropic Responsible Scaling Policy", "label": "safe"}
{"id": "768_14", "doc_id": "768", "text": "4.\tRapid remediation: Show that any compromises of the deployed system, such as jailbreaks or other attack pathways, will be identiﬁed and remediated promptly enough to prevent the overall system from meaningfully increasing an adversary’s ability to cause catastrophic harm. Example techniques could include rapid vulnerability patching, the ability to escalate to law enforcement when appropriate, and any necessary retention of logs for these activities.\n5.\tMonitoring: Prespecify empirical evidence that would show the system is operating within the accepted risk range and deﬁne a process for reviewing the system’s performance on a reasonable cadence. Process examples include monitoring responses to jailbreak bounties, doing historical analysis or background monitoring, and any necessary retention of logs for these activities.\n6.\tTrusted users: Establish criteria for determining when it may be appropriate to share a version of the model with reduced safeguards with trusted users. In addition, demonstrate that an alternative set of controls will provide equivalent levels of assurance. This could include a sufficient combination of user vetting, secure access controls, monitoring, log retention, and incident response protocols.\n7.\tThird-party environments: Document how all relevant models will meet the criteria above, even if they are deployed in a third-party partner’s environment that may have a different set of safeguards.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Risk factors: Security", "Risk factors: Security: Dissemination"], "source": "https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf", "official_name": "Anthropic Responsible Scaling Policy", "label": "safe"}
{"id": "768_17", "doc_id": "768", "text": "c.\tMonitoring: Proactively identifying and mitigating threats through ongoing and effective monitoring, testing for vulnerabilities, and laying traps for potential attackers. We expect this will include a combination of endpoint patching, product security testing, log management, asset monitoring, and intruder deception techniques.\nd.\tResourcing: Investing sufficient resources in security. We expect meeting this standard of security to require roughly 5-10% of employees being dedicated to security and security-adjacent work.\ne.\tExisting guidance: Aligning where appropriate with existing guidance on securing model weights, including Securing AI Model Weights, Preventing Theft and Misuse of Frontier Models (2024); security recommendations like Deploying AI Systems Securely (CISA/NSA/FBI/ASD/CCCS/GCSB /GCHQ), ISO 42001, CSA’s AI Safety Initiative, and CoSAI; and standards frameworks like SSDF, SOC 2, NIST 800-53.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Risk factors: Security", "Risk factors: Security: Dissemination", "Risk factors: Security: Cybersecurity"], "source": "https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf", "official_name": "Anthropic Responsible Scaling Policy", "label": "safe"}
{"id": "768_18", "doc_id": "768", "text": "3.\tAudits: Develop plans to (1) audit and assess the design and implementation of the security program and (2) share these ﬁndings (and updates on any remediation efforts) with management on an appropriate cadence. We expect this to include independent validation of threat modeling and risk assessment results; a sampling-based audit of the operating effectiveness of the deﬁned controls; periodic, broadly scoped, and independent testing with expert red-teamers who are industry-renowned and have been recognized in competitive challenges.\n4.\tThird-party environments: Document how all relevant models will meet the criteria above, even if they are deployed in a third-party partner’s environment that may have a different set of safeguards.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Evaluation: Adversarial testing", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation"], "source": "https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf", "official_name": "Anthropic Responsible Scaling Policy", "label": "safe"}
{"id": "768_26", "doc_id": "768", "text": "7.2.\tTransparency and External Input\nTo advance the public dialogue on the regulation of frontier AI model risks and to enable examination of our actions, we commit to the following:\n\n1.\tPublic disclosures: We will publicly release key information related to the evaluation and deployment of our models (not including sensitive details). These include summaries of related Capability and Safeguards reports when we deploy a model19 as well as plans for current and future comprehensive capability assessments and deployment and security safeguards. We will also periodically release information on internal reports of potential instances of non-compliance and other implementation challenges we encounter.\n2.\tExpert input: We will solicit input from external experts in relevant domains in the process of developing and conducting capability and safeguards assessments. We may also solicit external expert input prior to making ﬁnal decisions on the capability and safeguards assessments.\n3.\tU.S. Government notice: We will notify a relevant U.S. Government entity if a model requires stronger protections than the ASL-2 Standard.\n4.\tProcedural compliance review: On approximately an annual basis, we will commission a third-party review that assesses whether we adhered to this policy’s main procedural commitments (we expect to iterate on the exact list since this has not been done before for RSPs). This review will focus on procedural compliance, not substantive outcomes. We will also do such reviews internally on a more regular cadence.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Evaluation: External auditing", "Strategies: Evaluation"], "source": "https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf", "official_name": "Anthropic Responsible Scaling Policy", "label": "safe"}
{"id": "793_2", "doc_id": "793", "text": "(b) Requirement To Allow Users To See Unmanipulated Content On Internet Platforms.—\n\n\n(1) IN GENERAL.—Beginning on the date that is 1 year after the date of enactment of this Act, it shall be unlawful—\n\n\n(A) for any person to operate a covered internet platform that uses an opaque algorithm unless the person complies with the requirements of paragraph (2); or\n\n\n(B) for any upstream provider to grant access to an index of web pages on the internet under a search syndication contract that does not comply with the requirements of paragraph (3).\n\n\n(2) OPAQUE ALGORITHM REQUIREMENTS.—\n\n\n(A) IN GENERAL.—The requirements of this paragraph with respect to a person that operates a covered internet platform that uses an opaque algorithm are the following:\n\n\n(i) The person provides notice to users of the platform—\n\n\n(I) that the platform uses an opaque algorithm that uses user-specific data to select the content the user sees. Such notice shall be presented in a clear, conspicuous manner on the platform whenever the user interacts with an opaque algorithm for the first time, and may be a one-time notice that can be dismissed by the user; and\n\n\n(II) in the terms and conditions of the covered internet platform, in a clear, accessible, and easily comprehensible manner to be updated no less frequently than once every 6 months—\n\n\n(aa) the most salient features, inputs, and parameters used by the algorithm;\n\n\n(bb) how any user-specific data used by the algorithm is collected or inferred about a user of the platform, and the categories of such data;\n\n\n(cc) any options that the covered internet platform makes available for a user of the platform to opt out or exercise options under clause (ii), modify the profile of the user or to influence the features, inputs, or parameters used by the algorithm; and\n\n\n(dd) any quantities, such as time spent using a product or specific measures of engagement or social interaction, that the algorithm is designed to optimize, as well as a general description of the relative importance of each quantity for such ranking.\n\n\n(ii) The person makes available a version of the platform that uses an input-transparent algorithm and enables users to easily switch between the version of the platform that uses an opaque algorithm and the version of the platform that uses the input-transparent algorithm.\n\n\n(B) NONAPPLICATION TO CERTAIN DOWNSTREAM PROVIDERS.—Subparagraph (A) shall not apply with respect to an internet search engine if—\n\n\n(i) the search engine is operated by a downstream provider with fewer than 1,000 employees; and\n\n\n(ii) the search engine uses an index of web pages on the internet to which such provider received access under a search syndication contract.\n\n\n(3) SEARCH SYNDICATION CONTRACT REQUIREMENT.—The requirements of this paragraph with respect to a search syndication contract are that—\n\n\n(A) as part of the contract, the upstream provider makes available to the downstream provider the same input-transparent algorithm used by the upstream provider for purposes of complying with paragraph (2)(A)(ii); and\n\n\n(B) the upstream provider does not impose any additional costs, degraded quality, reduced speed, or other constraint on the functioning of such algorithm when used by the downstream provider to operate an internet search engine relative to the performance of such algorithm when used by the upstream provider to operate an internet search engine.\n\n\n(4) PROHIBITION ON DIFFERENTIAL PRICING.—A covered internet platform shall not deny, charge different prices or rates for, or condition the provision of a service or product to an individual based on the individual’s election to use a version of the platform that uses an input-transparent algorithm as provided under paragraph (2)(A)(ii).", "tags": ["Risk factors: Privacy", "Risk factors: Transparency", "Harms: Discrimination", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About inputs", "Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/1409", "official_name": "Kids Online Safety Act, Sec. 13 (\"Filter Bubble Transparency Requirements\")", "label": "safe"}
{"id": "793_3", "doc_id": "793", "text": "(c) Enforcement By Federal Trade Commission.—\n\n\n(1) UNFAIR OR DECEPTIVE ACTS OR PRACTICES.—A violation of this section by an operator of a covered internet platform shall be treated as a violation of a rule defining an unfair or deceptive act or practice prescribed under section 18(a)(1)(B) of the Federal Trade Commission Act (15 U.S.C. 57a(a)(1)(B)).\n\n\n(2) POWERS OF COMMISSION.—\n\n\n(A) IN GENERAL.—Except as provided in subparagraph (C), the Federal Trade Commission shall enforce this section in the same manner, by the same means, and with the same jurisdiction, powers, and duties as though all applicable terms and provisions of the Federal Trade Commission Act (15 U.S.C. 41 et seq.) were incorporated into and made a part of this section.\n\n\n(B) PRIVILEGES AND IMMUNITIES.—Except as provided in subparagraph (C), any person who violates this Act shall be subject to the penalties and entitled to the privileges and immunities provided in the Federal Trade Commission Act (15 U.S.C. 41 et seq.).\n\n\n(C) COMMON CARRIERS AND NONPROFIT ORGANIZATIONS.—Notwithstanding section 4, 5(a)(2), or 6 of the Federal Trade Commission Act (15 U.S.C. 44, 45(a)(2), 46) or any jurisdictional limitation of the Commission, the Commission shall also enforce this Act, in the same manner provided in subparagraphs (A) and (B) of this paragraph, with respect to—\n\n\n(i) common carriers subject to the Communications Act of 1934 (47 U.S.C. 151 et seq.) and Acts amendatory thereof and supplementary thereto; and\n\n\n(ii) organizations not organized to carry on business for their own profit or that of their members.\n\n\n(D) AUTHORITY PRESERVED.—Nothing in this section shall be construed to limit the authority of the Commission under any other provision of law.\n\n\n(3) RULE OF APPLICATION.—Section 11 shall not apply to this section.\n\n\n(d) Rule Of Construction To Preserve Personalized Blocks.—Nothing in this section shall be construed to limit or prohibit a covered internet platform’s ability to, at the direction of an individual user or group of users, restrict another user from searching for, finding, accessing, or interacting with such user’s or group’s account, content, data, or online community.", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/1409", "official_name": "Kids Online Safety Act, Sec. 13 (\"Filter Bubble Transparency Requirements\")", "label": "safe"}
{"id": "826_7", "doc_id": "826", "text": "“(f) Penalty.—\n\n\n“(1) CRIMINAL PENALTY.—\n\n\n“(A) FAILURE TO DISCLOSE.—Whoever knowingly fails to comply with the requirements under subsection (a)—\n\n\n“(i) with the intent to humiliate or otherwise harass the person falsely exhibited, provided the advanced technological false personation record contains sexual content of a visual nature and appears to feature such person engaging in such sexual acts or in a state of nudity;\n\n\n“(ii) with the intent to cause violence or physical harm, incite armed or diplomatic conflict, or interfere in an official proceeding, including an election, provided the advanced technological false personation record did in fact pose a credible threat of instigating or advancing such;\n\n\n“(iii) in the course of criminal conduct related to fraud, including securities fraud and wire fraud, false personation, or identity theft; or\n\n\n“(iv) by a foreign power, or an agent thereof, with the intent of influencing a domestic public policy debate, interfering in a Federal, State, local, or territorial election, or engaging in other acts which such power may not lawfully undertake;\n\n\nshall be fined under this title, imprisoned for not more than 5 years, or both.\n\n\n“(B) ALTERING DISCLOSURES.—Whoever knowingly alters an advanced technological false personation record to remove or meaningfully obscure the disclosures required under subsection (a) with the intent to distribute such altered record and—\n\n\n“(i) with the intent to humiliate or otherwise harass the person falsely exhibited, provided the advanced technological false personation record contains sexual content of a visual nature and appears to feature such person engaging in such sexual acts or in a state of nudity;\n\n\n“(ii) with the intent to cause violence or physical harm, incite armed or diplomatic conflict, or interfere in an official proceeding, including an election, provided the advanced technological false personation record did in fact pose a credible threat of instigating or advancing such;\n\n\n“(iii) in the course of criminal conduct related to fraud, including securities fraud and wire fraud, false personation, or identity theft; or\n\n\n“(iv) by a foreign power, or an agent thereof, with the intent of influencing a domestic public policy debate, interfering in a Federal, State, local, or territorial election, or engaging in other acts which such power may not lawfully undertake;\n\n\nshall be fined under this title, imprisoned for not more than 5 years, or both.\n\n\n“(2) CIVIL PENALTY.—\n\n\n“(A) FAILURE TO DISCLOSE.—Any person who violates subsection (a) shall be subject to a civil penalty of up to $150,000 per record or alteration, as well as appropriate injunctive relief.\n\n\n“(B) ALTERING DISCLOSURES.—Any person who alters an advanced technological false personation record to remove or meaningfully obscure the disclosures required under subsection (a) with the intent to distribute such altered record shall be subject to a civil penalty of up to $150,000 per record or alteration, as well as appropriate injunctive relief.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Incentives: Civil liability", "Incentives: Criminal liability", "Incentives: Fines", "Incentives: Imprisonment"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/5586", "official_name": "DEEPFAKES Accountability Act, Sec. 2 (\"Transparency Requirements\")", "label": "safe"}
{"id": "826_8", "doc_id": "826", "text": "“(g) Private Right Of Action.—\n\n\n“(1) IN GENERAL.—Any person who has been exhibited as engaging in falsified material activity in an advanced technological false personation record may bring a civil action before the appropriate Federal district court for damages under paragraph (2) and injunctive relief under paragraph (3) against a person who violates subsection (a) or alters an advanced technological false personation record to remove or meaningfully obscure the disclosures required under subsection (a).\n\n\n“(2) DAMAGES.—Damages shall consist of the greater of—\n\n\n“(A) actual damages suffered by the living person or the affiliated corporation or entity, and any additional substantially derivative profits of the person who violated subsection (a) or altered an advanced technological false personation record to remove or meaningfully obscure the disclosures required under subsection (a);\n\n\n“(B) $50,000 per record, if the living person or affiliated corporation or entity experienced a perceptible individual harm or faced a tangible risk of experiencing such harm;\n\n\n“(C) $100,000 per record, if the living person or affiliated corporation or entity experienced a perceptible individual harm or faced a tangible risk of experiencing such harm and the record purported to depict extreme or outrageous conduct by the living person; or\n\n\n“(D) $150,000 per record, if the advanced technological false personation record contains explicit sexual content of a visual nature intended to humiliate or otherwise harass the person falsely depicted as engaging in such sexual acts or in a state of nudity.\n\n\n“(3) INJUNCTIVE RELIEF.—Injunctive relief under this subsection shall include a requirement to comply with subsection (a).", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/5586", "official_name": "DEEPFAKES Accountability Act, Sec. 2 (\"Transparency Requirements\")", "label": "safe"}
{"id": "891_2", "doc_id": "891", "text": "SEC. 2. LIMITS ON THE USE OF ARTIFICIAL INTELLIGENCE BY INTERNAL REVENUE SERVICE.\n(a) Limitations On The Use Of Artificial Intelligence For Audit Or Investigation.—Section 7803(a) of the Internal Revenue Code of 1986 is amended by adding at the end the following new paragraph:\n“(5) LIMITATIONS ON THE USE OF ARTIFICIAL INTELLIGENCE FOR AUDIT OR INVESTIGATION.—\n“(A) GUIDANCE.—Notwithstanding subsection (b) of section 553 of title 5, United States Code, any guidance issued by the Commissioner relating to the use of artificial intelligence for selection for or initiation of an audit or investigation by the Internal Revenue Service shall be subject to the requirements of such section as if such guidance were a rule making.\n“(B) EXPLAINABILITY REQUIREMENT.—The Commissioner may not conduct an audit or investigation initiated as result of analysis or selection by artificial intelligence unless the Commissioner determines that such artificial intelligence meets the explainability principles for artificial intelligence established by the Director of the National Institute of Standards and Technology.\n“(C) ARTIFICIAL INTELLIGENCE.—For purposes of this paragraph, the term ‘artificial intelligence’ has the meaning given such term in section 5002 of the National Artificial Intelligence Initiative Act of 2020.”.", "tags": ["Risk factors: Interpretability and explainability", "Applications: Government: other applications/unspecified", "Strategies: Performance requirements"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7694/text", "official_name": "No AI Audits Act", "label": "safe"}
{"id": "988_3", "doc_id": "988", "text": "(c) Prohibition On Covered Activities In Covered Sectors That Pose Particularly Acute Threats To United States National Security.—\n\n\n(1) IDENTIFICATION OF CATEGORIES OF TECHNOLOGIES AND PRODUCTS.—\n\n\n(A) IN GENERAL.—Not later than one year after the date of the enactment of this Act, and annually thereafter as described in subparagraph (B), the President—\n\n\n(i) shall identify categories of technologies and products in covered sectors that may pose a particularly acute threat to the national security of the United States if developed or acquired by a country of concern; and\n\n\n(ii) publish a list of the categories of technologies and products identified under subparagraph (A) in the Federal Register.\n\n\n(B) UPDATES.—The President shall annually review and update the list of the categories of technologies and products identified under subparagraph (A)(i) and update the Federal Register under subparagraph (A)(ii) as appropriate.\n\n\n(2) PROHIBITION ON COVERED ACTIVITIES.—The President shall, on or after the date on which the initial list of categories of technologies and products is published in the Federal Register pursuant to paragraph (1)(A)(ii), prescribe, subject to public notice and comment, regulations to prohibit a United States person from engaging, directly or indirectly, in a covered activity involving a category of technologies and products on such list of categories of technologies and products in a covered sector. Such regulations should—\n\n\n(A) require that a United States person take all reasonable steps to prohibit and prevent any transaction by a foreign entity under the control of the United States person that would be a prohibited transaction if engaged in by a United States person; and\n\n\n(B) exclude any transaction consisting of the acquisition of an equity or other interest in an entity located outside a country of concern, where the President has determined that the government of the country in which that entity is established or has its principal place of business has in place a program for the restriction of certain activities involving countries of concern that is comparable to the provisions provided for in this Act.\n\n\n(3) SENSE OF CONGRESS.—It is the sense of Congress that the covered sectors include certain categories of technologies and products that would pose a particularly acute threat to the national security of the United States if developed or acquired by a country of concern, and that the President should identify certain technologies and products in the covered sectors as categories of technologies and products in covered sectors for purposes of paragraph (1)(A).", "tags": ["Strategies: Input controls: Compute circulation", "Strategies: Input controls"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7476", "official_name": "Countering Communist China Act, Title I (\"Matters Related to Trade, Investment, and Economic Relations\"), Sec. 101 (\"Preventing Adversaries from Developing Critical Capabilities\")", "label": "safe"}
{"id": "988_6", "doc_id": "988", "text": "(f) Requirement For Regulations.—\n\n\n(1) IN GENERAL.—Not later than 180 days after the date on which the initial list of categories of technologies and products have been published in the Federal Register pursuant to sections (c)(1)(A)(i) and (d)(1)(B), the President shall prescribe and finalize proposed regulations to carry out this Act.\n\n\n(2) ELEMENTS.—Regulations prescribed to carry out this section shall specify—\n\n\n(A) the types of activities that will be considered to be covered activities;\n\n\n(B) the technologies and products in covered sectors with respect to which covered activities are prohibited under subsection (c)(2) or require a notification under subsection (d)(2); and\n\n\n(C) a process by which parties can ask questions and get timely guidance as to whether a covered activity is prohibited under subsection (c)(2) or requires a notification under subsection (d)(2).\n\n\n(3) REQUIREMENTS FOR CERTAIN REGULATIONS.—The President shall prescribe regulations further defining the terms used in this Act, including the terms “covered activity”, “covered foreign entity”, and “party”, to maximize the effectiveness of carrying out this Act in accordance with subchapter II of chapter 5 and chapter 7 of title 5 (commonly known as the “Administrative Procedure Act”).\n\n\n(4) PUBLIC NOTICE AND COMMENT.—Regulations issued pursuant to paragraph (1) shall be subject to public notice and comment.\n\n\n(5) LOW-BURDEN REGULATIONS.—In prescribing regulations under this section, the President shall, to the extent practicable, structure the regulations—\n\n\n(A) to minimize the cost and complexity of compliance for affected parties;\n\n\n(B) to ensure the benefits of the regulations outweigh their costs;\n\n\n(C) to adopt the least burdensome alternative that achieves regulatory objectives;\n\n\n(D) to prioritize transparency and stakeholder involvement in the process of prescribing the regulations; and\n\n\n(E) to regularly review and streamline existing regulations promulgated pursuant to this Act to reduce redundancy and complexity.\n\n\n(6) PENALTIES WITH RESPECT TO UNLAWFUL ACTS.—Regulations issued under this section shall, consistent with the authority provided by subsection (b)(1), provide for the imposition of civil penalties for violations of this section, that involve—\n\n\n(A) engaging in a covered activity prohibited under subsection (c)(2) pursuant to the regulations issued under this section;\n\n\n(B) failing to submit a timely notification under subsection (d)(2) with respect to a covered activity or to submit other information as required by the designated agency; or\n\n\n(C) submitting a material misstatement or omitting a material fact in any information submitted in a notification under subsection (d)(2).\n\n\n(7) ENFORCEMENT.—Consistent with the authority provided by subsection (b)(1), the President may direct the Attorney General to seek appropriate relief in the district courts of the United States, in order to implement and enforce this Act.\n\n\n(8) CONGRESSIONAL NOTIFICATION.—The President shall submit to the appropriate congressional committees all regulations prescribed to carry out this Act not later than 30 days before such regulations are to take effect.", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7476", "official_name": "Countering Communist China Act, Title I (\"Matters Related to Trade, Investment, and Economic Relations\"), Sec. 101 (\"Preventing Adversaries from Developing Critical Capabilities\")", "label": "safe"}
