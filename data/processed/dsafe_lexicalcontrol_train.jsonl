{"id": "1045_1", "doc_id": "1045", "text": "Principles at a glance\nHuman, societal and environmental wellbeing: AI systems should benefit individuals, society and the environment.\nHuman-centred values: AI systems should respect human rights, diversity, and the autonomy of individuals.\nFairness: AI systems should be inclusive and accessible, and should not involve or result in unfair discrimination against individuals, communities or groups.\nPrivacy protection and security: AI systems should respect and uphold privacy rights and data protection, and ensure the security of data.\nReliability and safety: AI systems should reliably operate in accordance with their intended purpose.\nTransparency and explainability: There should be transparency and responsible disclosure so people can understand when they are being significantly impacted by AI, and can find out when an AI system is engaging with them.\nContestability: When an AI system significantly impacts a person, community, group or environment, there should be a timely process to allow people to challenge the use or outcomes of the AI system.\nAccountability: People responsible for the different phases of the AI system lifecycle should be identifiable and accountable for the outcomes of the AI systems, and human oversight of AI systems should be enabled.", "tags": ["Risk factors: Bias", "Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Security", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Harms: Ecological harm", "Harms: Discrimination"], "source": "https://www.industry.gov.au/publications/australias-artificial-intelligence-ethics-framework/australias-ai-ethics-principles", "official_name": "Australia’s AI Ethics Principles", "label": "safe"}
{"id": "1053_7", "doc_id": "1053", "text": "Validity and Robustness\nEnsuring that AI systems work as intended and are resilient across the range of contexts to which they are likely to be exposed is critical to building trust. This is a particular challenge for widely deployed generative AI systems, since they may be used in a broad range of contexts and thus may have greater exposure to misuse and attacks. This flexibility is a key advantage of the technology, but requires rigorous measures and testing to be put in place to avoid misuse and unintended consequences.\n\n\nDevelopers of generative AI systems would:\nuse a wide variety of testing methods across a spectrum of tasks and contexts, including adversarial testing (e.g., red-teaming), to measure performance and identify vulnerabilities.\n\n\nDevelopers, deployers, and operators of generative AI systems would:\nemploy appropriate cybersecurity measures to prevent or identify adversarial attacks on the system (e.g., data poisoning).", "tags": ["Risk factors: Reliability", "Risk factors: Reliability: Robustness", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Security: Cybersecurity", "Risk factors: Security"], "source": "https://ised-isde.canada.ca/site/ised/en/consultation-development-canadian-code-practice-generative-artificial-intelligence-systems/canadian-guardrails-generative-ai-code-practice", "official_name": "Canadian Guardrails for Generative AI – Code of Practice", "label": "safe"}
{"id": "1087_7", "doc_id": "1087", "text": "Accuracy\nAgencies that hold an individual’s personal information must not use or disclose that information without taking reasonable steps to ensure that the information is accurate, up to date, complete, relevant, and not misleading (IPP8).\nThe accuracy principle protects people against harm and upholds the relevance and reliability of information about people. It is relevant at all stages from gathering training data, to training a model, to implementing a tool, accepting input, providing output, and taking actions as a result. To uphold this principle, it is important to consider what assurances you have that an AI tool will be accurate, relevant, and reliable at each stage, so you may need to investigate the training process behind an AI tool.\nGenerative AI tools produce confident errors of fact and logic, so you cannot rely on their apparent performance or self-declared capabilities as evidence of accuracy. You should also take a critical approach to accuracy claims by providers of AI tools. Most of the public-facing AI tools now available have been developed overseas and are based on training data that may not be relevant, reliable, and ethical for use in Aotearoa New Zealand.\nWhile we are also part of the broader world, we have our own unique mix of cultural perspectives, demographics, and use of languages including English and Te Reo Māori. Adopting AI tools that are not designed for and in consultation with our communities may lead to inaccuracy in the form of bias, exclusion (particularly of poorly represented groups), and other privacy harms.\nBefore using AI tools, it’s essential that agencies take reasonable steps to assure themselves that these tools will uphold the accuracy principle. Depending on the nature of the intended use and level of risk, this may require independent testing and auditing. Doing and updating a PIA will be an essential part of this process.\nUse-case: Using AI tools to screen documents\nYou may be interested in using AI tools to make decisions based on text or documents, for example screening job applications to find people you want to interview, screening offensive comments online, or detecting text that has been generated by AI. The track record of AI tools in this area is not good, so you need to be very confident the system you want to use will be transparent, accurate, and fair before you ask anyone to rely on it.\nYou might want to ask:\n• How can I find out about the reliability and accuracy of the AI tool for this use-case?\n• Is there a risk of bias in the AI tool or the training data?\n• Who can I talk with to ensure people are ok with me using this tool? Can I engage with\nexperts? Can I engage with people and communities who might be affected?\nAccuracy and automated decision making\nOne critical area for accuracy is the use of AI tools for automated decision making, where they may have direct impacts on outcomes for people. The higher risk of these situations means it will generally be necessary to develop processes for human review of decisions, and to empower and adequately resource the people doing this work. Simply having a “human in the loop” may not be enough to uphold the accuracy principle, given the well-known problem of automation blindness in people overseeing automated systems.17 Overall, it may help to think in terms of using AI in ways that uphold accountability for people in your organisation, to your customers, and to the broader community. Where people have responsible roles, whether as financial managers, teachers, or public servants, any use of AI tools should maintain and complement the responsibilities people have in their roles. Talking with the people in these roles and the people they work will often be critical for good use of AI.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Risk factors: Reliability", "Strategies: Evaluation: Conformity assessment", "Strategies: Convening"], "source": "https://privacy.org.nz/assets/New-order/Resources-/Publications/Guidance-resources/AI-Guidance-Resources-/AI-and-the-Information-Privacy-Principles.pdf", "official_name": "Artificial Intelligence and the Information Privacy Principles", "label": "safe"}
{"id": "1099_24", "doc_id": "1099", "text": "ENABLER 9: Trusted Environment\n\nTrust and safety underwrite confidence in Singapore’s AI landscape. They lie at the heart of our interventions to deliver AI for the Public Good.\n\n-\tWe will endeavour for AI to be developed and deployed in a safe, trustworthy, and responsible manner. The Government will institutionalise appropriate governance and security frameworks for AI systems. The ultimate aim is to establish a trusted environment for AI, where people can have the confidence that their interests are protected when interacting with AI.\n\n-\tWe must retain agility in our regulatory approaches. AI will continue to evolve, and no party has full sight of the risks that might emerge. It is only through experimentation and exploration that the AI community can deepen its understanding of AI, and discover and address potential risks. The Government must therefore take a pragmatic approach – supporting experimentation and advancement, while still ensuring that AI is developed and used responsibly, in line with the rule of law and the safeguards we have put in place. Where existing regulatory frameworks need to be updated, we will do so thoughtfully and in concert with others, accounting for the global nature of AI.\n\nThere is a range of potential risks around AI, spanning from concerns around model quality and fair use, to fears around the loss of control and existential risks. Singapore remains open to engaging with all perspectives, to enhance our understanding of the risk landscape and to inform our most urgent priorities for risk mitigation. For a start:\n\n-\tWe must ensure that AI systems are well-developed, reliable, and resilient. This requires paying close attention to the model development process, to ensure that the output of models is not biased, inaccurate, or erroneous. AI models should also be aligned with the appropriate set of human and cultural values. Existing international conversations around AI governance and safety are largely centred on these concerns.\n\n-\tWe must prevent AI models from being used in malicious or harmful ways, and secure them against adversarial attacks. If AI is used carelessly, it can potentially amplify negative outcomes like discrimination, anti-competitive behaviours, or intellectual property infringement. It can also be deliberately misused against us, to supercharge existing threats (e.g. scams, cyber-attacks, and mis/disinformation) in terms of scale, speed, and sophistication. We must remain vigilant against these risks to maintain digital trust.\n\nThe Government will take differentiated approaches to managing risks to and from AI, ranging from regulatory moves to voluntary guidelines, recognising that AI will continue to evolve. We will need a deeper understanding of how AI works, what benchmarks to use, and what testing is appropriate, and we look forward to developing these perspectives together with other stakeholders.", "tags": ["Risk factors: Bias", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Reliability", "Harms: Discrimination"], "source": "https://file.go.gov.sg/nais2023.pdf", "official_name": "Singapore National AI Strategy 2.0", "label": "safe"}
{"id": "1109_3", "doc_id": "1109", "text": "2. Regulatory principles \n(1) The AI Authority must have regard to the principles that— \n(a) regulation of AI should deliver— \n(i) safety, security and robustness; \n(ii) appropriate transparency and explainability; \n(iii) fairness; (iv) accountability and governance; \n(v) contestability and redress; \n(b) any business which develops, deploys or uses AI should— \n(i) be transparent about it; \n(ii) test it thoroughly and transparently; \n(iii) comply with applicable laws, including in relation to data protection, privacy and intellectual property; \n(c) AI and its applications should— \n(i) comply with equalities legislation; \n(ii) be inclusive by design; \n(iii) be designed so as neither to discriminate unlawfully among individuals nor, so far as reasonably practicable, to perpetuate unlawful discrimination arising in input data; \n(iv) meet the needs of those from lower socio-economic groups, older people and disabled people; \n(v) generate data that are findable, accessible, interoperable and reusable; \n(d) a burden or limitation which is imposed on a person, or on the carrying on of an activity, in respect of AI should be proportionate to the benefits, taking into consideration the nature of the service or product being delivered, the nature of risk to consumers and others, whether the cost of implementation is proportionate to that level of risk and whether the burden or limitation enhances UK international competitiveness. \n\n\n(2) The Secretary of State may by regulations amend the principles in subsection (1), following consultation with such persons as he or she considers appropriate.", "tags": ["Risk factors: Safety", "Risk factors: Security", "Risk factors: Reliability: Robustness", "Risk factors: Reliability", "Risk factors: Transparency", "Risk factors: Privacy", "Harms: Discrimination"], "source": "https://bills.parliament.uk/publications/53068/documents/4030", "official_name": "Artificial Intelligence (Regulation) Act 2024", "label": "safe"}
{"id": "1109_6", "doc_id": "1109", "text": "5. Transparency, IP obligations and labelling \n(1) 35 The Secretary of State, after consulting the AI Authority and such other persons as he or she considers appropriate, must by regulations provide that— \n(a) any person involved in training AI must— \n(i) supply to the AI Authority a record of all third-party data and intellectual property (“IP”) used in that training; and \n(ii) assure the AI Authority that— \n(A) they use all such data and IP by informed consent;  and \n(B) they comply with all applicable IP and copyright obligations; \n(iii) any person supplying a product or service involving AI must give customers clear and unambiguous health warnings, labelling and opportunities to give or withhold informed consent in advance; and \n(iv) any business which develops, deploys or uses AI must allow independent third parties accredited by the AI Authority to review its processes and systems. \n\n\n(2) Regulations under this section may provide for informed consent to be express (opt-in) or implied (opt-out) and may make different provision for different cases.", "tags": ["Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Disclosure: In deployment", "Strategies: Evaluation", "Strategies: Evaluation: External auditing"], "source": "https://bills.parliament.uk/publications/53068/documents/4030", "official_name": "Artificial Intelligence (Regulation) Act 2024", "label": "safe"}
{"id": "1109_9", "doc_id": "1109", "text": "8. Regulations \n(1) Regulations under this Act are made by statutory instrument. \n\n\n(2) Regulations under this Act may create offences and require payment of fees, consequences and charges. \n\n\n(3) A statutory instrument containing regulations under section 1 or 2 or regulations covered by subsection (2) may not be made unless a draft of the instrument has been laid before and approved by resolution of both Houses of Parliament. \n\n\n(4) A statutory instrument containing only regulations not covered by subsection (3) is subject to annulment in pursuance of a resolution of either House of Parliament. \n\n\n(5) A statutory instrument containing regulations applying to Wales, Scotland or Northern Ireland must be laid before Senedd Cymru, the Scottish Parliament or the Northern Ireland Assembly respectively before being made.", "tags": ["Incentives: Fines"], "source": "https://bills.parliament.uk/publications/53068/documents/4030", "official_name": "Artificial Intelligence (Regulation) Act 2024", "label": "safe"}
{"id": "1116_2", "doc_id": "1116", "text": "Ensure Legal Accountability for Harms: Congress should ensure that A.I. companies can be held responsible through oversight body enforcement and private rights of action when their models and systems breach privacy, violate civil rights, or otherwise cause cognizable harms. Where existing laws are insufficient to address new harms created by A.I., Congress should ensure that enforcers and victims can take companies and perpetrators to court, including clarifying that Section 230 does not apply to A.I. In particular, Congress must take steps to directly restrict harms that are already emerging from A.I., such as non-consensual explicit deepfake imagery of real people, production of child sexual abuse material from generative A.I., and election interference.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Incentives: Civil liability", "Harms: Detrimental content", "Strategies: Evaluation"], "source": "https://www.blumenthal.senate.gov/imo/media/doc/09072023bipartisanaiframework.pdf", "official_name": "Bipartisan Framework for U.S. AI Act", "label": "safe"}
{"id": "1163_5", "doc_id": "1163", "text": "(d) “Computing cluster” means a set of machines transitively connected by data center networking of over 100 gigabits per second that has a theoretical maximum computing capacity of at least 10^20 integer or floating-point operations per second and can be used for training artificial intelligence.", "tags": ["Strategies: Performance requirements"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB1047", "official_name": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047)", "label": "safe"}
{"id": "1163_6", "doc_id": "1163", "text": "(e) (1) “Covered model” means either of the following:\n(A) Before January 1, 2027, “covered model” means either of the following:\n(i) An artificial intelligence model trained using a quantity of computing power greater than 10^26 integer or floating-point operations, the cost of which exceeds one hundred million dollars ($100,000,000) when calculated using the average market prices of cloud compute at the start of training as reasonably assessed by the developer.\n(ii) An artificial intelligence model created by charge-tuning a covered model using a quantity of computing power equal to or greater than three times 10^25 integer or floating-point operations, the cost of which, as reasonably assessed by the developer, exceeds ten million dollars ($10,000,000) if calculated using the average market price of cloud compute at the start of charge-tuning.", "tags": ["Strategies: Tiering", "Strategies: Tiering: Tiering based on inputs", "Strategies: Input controls", "Strategies: Input controls: Compute use"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB1047", "official_name": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047)", "label": "safe"}
{"id": "1163_14", "doc_id": "1163", "text": "(m) “Person” means an individual, proprietorship, firm, partnership, joint venture, syndicate, business trust, company, corporation, limited responsibility company, association, committee, or any other nongovernmental organization or group of persons acting in concert.\n(n) “Post-training modification” means modifying the capabilities of a covered model or covered model derivative by any means, including, but not limited to, charge-tuning, providing the model with access to tools or data, removing safeguards against hazardous misuse or misbehavior of the model, or combining the model with, or integrating it into, other software.", "tags": ["Strategies: Evaluation: Post-market monitoring"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB1047", "official_name": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047)", "label": "safe"}
{"id": "1163_16", "doc_id": "1163", "text": "22603. (a) Before beginning to initially train a covered model, the developer shall do all of the following:\n(1) Implement reasonable administrative, technical, and physical cybersecurity protections to prevent unauthorized access to, misuse of, or unsafe post-training modifications of, the covered model and all covered model derivatives controlled by the developer that are appropriate in light of the risks associated with the covered model, including from advanced persistent threats or other sophisticated actors.\n(2) (A) Implement the capability to promptly enact a full shutdown.\n(B) When enacting a full shutdown, the developer shall take into account, as appropriate, the risk that a shutdown of the covered model, or particular covered model derivatives, could cause disruptions to critical infrastructure.", "tags": ["Risk factors: Security: Cybersecurity", "Risk factors: Security", "Risk factors: Security: Dissemination", "Risk factors: Reliability", "Harms: Harm to infrastructure"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB1047", "official_name": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047)", "label": "safe"}
{"id": "1163_19", "doc_id": "1163", "text": "(4) Ensure that the safety and security protocol is implemented as written, including by designating senior personnel to be responsible for ensuring adherence by employees and contractors working on a covered model, or any covered model derivatives controlled by the developer, monitoring and reporting on implementation.\n(5) Retain an unredacted copy of the safety and security protocol for as long as the covered model is made available for commercial, public, or foreseeably public use plus five years, including records and dates of any updates or revisions.\n(6) Conduct an annual review of the safety and security protocol to account for any changes to the capabilities of the covered model and industry best practices and, if necessary, make modifications to the policy.", "tags": ["Risk factors: Safety", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation: Conformity assessment"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB1047", "official_name": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047)", "label": "safe"}
{"id": "1163_24", "doc_id": "1163", "text": "(e) (1) Beginning January 1, 2026, a developer of a covered model shall annually retain a third-party auditor that conducts reviews consistent with best practices for auditors to perform an independent review of adherence with the requirements of this section.\n(2) An auditor shall conduct reviews consistent with regulations issued by the Government Operations Agency pursuant to subdivision (d) of Section 11547.6 of the Government Code.\n(3) The auditor shall be granted access to unredacted materials as necessary to comply with the auditor’s obligations under this subdivision.", "tags": ["Strategies: Evaluation: External auditing", "Strategies: Evaluation"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB1047", "official_name": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047)", "label": "safe"}
{"id": "1163_25", "doc_id": "1163", "text": "(4) The auditor shall produce an review report including all of the following:\n(A) A detailed assessment of the developer’s steps to comply with the requirements of this section.\n(B) If applicable, any identified instances of noncompliance with the requirements of this section, and any recommendations for how the developer can improve its policies and processes for ensuring adherence with the requirements of this section.\n(C) A detailed assessment of the developer’s internal controls, including its designation and empowerment of senior personnel responsible for ensuring adherence by the developer, its employees, and its contractors.\n(D) The signature of the lead auditor certifying the results of the auditor.", "tags": ["Strategies: Evaluation: External auditing", "Strategies: Evaluation", "Strategies: Disclosure: In standard form"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB1047", "official_name": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047)", "label": "safe"}
{"id": "1163_26", "doc_id": "1163", "text": "(5) The developer shall retain an unredacted copy of the review report for as long as the covered model is made available for commercial, public, or foreseeably public use plus five years.\n(6) (A) (i) The developer shall conspicuously publish a redacted copy of the auditor’s report and transmit to the Attorney General a copy of the redacted auditor’s report.\n(ii) A redaction in the auditor’s report may be made only if the redaction is reasonably necessary to protect any of the following:\n(I) Public safety.\n(II) Trade secrets, as defined in Section 3426.1 of the Civil Code.\n(III) Confidential information pursuant to state and federal law.\n(B) The developer shall financial support allocation to the Attorney General access to the unredacted auditor’s report upon request.\n(C) An auditor’s report disclosed to the Attorney General pursuant to this paragraph is excused from the California Public Records Act (Division 10 (commencing with Section 7920.000) of Title 1 of the Government Code).\n(7) An auditor must avoid knowingly make a material misrepresentation in the auditor’s report.", "tags": ["Strategies: Evaluation: External auditing", "Strategies: Disclosure", "Strategies: Evaluation"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB1047", "official_name": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047)", "label": "safe"}
{"id": "1163_31", "doc_id": "1163", "text": "(2) Assess whether the prospective customer intends to utilize the computing cluster to train a covered model.\n(3) If a customer repeatedly utilizes computer resources that would be sufficient to train a covered model, validate the information initially collected pursuant to paragraph (1) and conduct the assessment required pursuant to paragraph (2) prior to each utilization.\n(4) Retain a customer’s Internet Protocol addresses used for access or administration and the date and time of each access or administrative action.\n(5) Maintain for seven years and provide to the Attorney General, upon request, appropriate records of actions taken under this section, including policies and procedures put into effect.\n(6) Implement the capability to promptly enact a full shutdown of any resources being used to train or operate models under the customer’s control.", "tags": ["Strategies: Evaluation", "Strategies: Performance requirements"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB1047", "official_name": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047)", "label": "safe"}
{"id": "1163_33", "doc_id": "1163", "text": "22606. (a) The Attorney General may bring a civil action for a infraction of this chapter and to recover all of the following:\n(1) For a infraction that causes death or bodily harm to another human, harm to property, theft or misappropriation of property, or that constitutes an imminent risk or threat to public safety that occurs on or after January 1, 2026, a civil consequence in an amount not exceeding 10 percent of the cost of the quantity of computing power used to train the covered model to be calculated using average market prices of cloud compute at the time of training for a first infraction and in an amount not exceeding 30 percent of that value for any subsequent infraction.\n(2) For a infraction of Section 22607 that would constitute a infraction of the Labor Code, a civil consequence specified in subdivision (f) of Section 1102.5 of the Labor Code.", "tags": ["Incentives: Civil liability", "Harms: Harm to health/safety", "Incentives: Fines", "Risk factors: Safety", "Harms: Harm to property"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB1047", "official_name": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047)", "label": "safe"}
{"id": "1163_34", "doc_id": "1163", "text": "(3) For a person that operates a computing cluster for a infraction of Section 22604, for an auditor for a infraction of paragraph (6) of subdivision (e) of Section 22603, or for an auditor who intentionally or with reckless disregard violates a provision of subdivision (e) of Section 22603 other than paragraph (6) or regulations issued by the Government Operations Agency pursuant to Section 11547.6 of the Government Code, a civil consequence in an amount not exceeding fifty thousand dollars ($50,000) for a first infraction of Section 22604, not exceeding one hundred thousand dollars ($100,000) for any subsequent infraction, and not exceeding ten million dollars ($10,000,000) in the aggregate for related violations.\n(4) Injunctive or declaratory relief.\n(5) (A) Monetary damages.\n(B) Punitive damages pursuant to subdivision (a) of Section 3294 of the Civil Code.\n(6) Attorney’s fees and costs.\n(7) Any other relief that the court deems appropriate.", "tags": ["Incentives: Fines", "Incentives: Civil liability"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB1047", "official_name": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047)", "label": "safe"}
{"id": "1163_36", "doc_id": "1163", "text": "(c) (1) A provision within a contract or agreement that seeks to waive, preclude, or burden the enforcement of a responsibility arising from a infraction of this chapter, or to shift that responsibility to any person or entity in exchange for their use or access of, or right to use or access, a developer’s products or services, including by means of a contract of adhesion, is void as a matter of public policy.\n(2) A court shall disregard corporate formalities and impose joint and several responsibility on affiliated entities for purposes of effectuating the intent of this section to the maximum extent allowed by law if the court concludes that both of the following are true:\n(A) The affiliated entities, in the development of the corporate structure among the affiliated entities, took steps to purposely and unreasonably limit or avoid responsibility.\n(B) As the result of the steps described in subparagraph (A), the corporate structure of the developer or affiliated entities would frustrate recovery of consequences, damages, or injunctive relief under this section.\n(d) consequences collected pursuant to this section by the Attorney General shall be deposited into the Public Rights Law Enforcement Special support pool established pursuant to Section 12530 of the Government Code.\n(e) This section does not limit the application of other laws.", "tags": ["Incentives: Civil liability", "Incentives: Fines"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB1047", "official_name": "Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047)", "label": "safe"}
{"id": "1195_3", "doc_id": "1195", "text": "Measurement and Evaluation\nWe should develop comprehensive methods and techniques to operationalize these red lines prior to there being a meaningful risk of them being crossed. To ensure red line testing regimes keep pace with rapid AI development, we should invest in red teaming and automating model evaluation with appropriate human oversight.\n\nThe onus should be on developers to convincingly demonstrate that red lines will not be crossed such as through rigorous empirical evaluations, quantitative guarantees or mathematical proofs.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Strategies: Evaluation: Post-market monitoring"], "source": "https://idais.ai/idais-beijing/", "official_name": "Consensus Statement on Red Lines in Artificial Intelligence", "label": "safe"}
{"id": "1202_6", "doc_id": "1202", "text": "(4) PROTECTION OF REPORTING PARTIES.—\n\n\n(A) PROHIBITION AGAINST RETALIATION.—No employer may, directly or indirectly, discharge, demote, suspend, threaten, blacklist, harass, or in any other manner discriminate against any current or former employee or contractor in the terms and conditions of employment or postemployment because of any act done by such employee or contractor—\n\n\n(i) in reporting incidents in accordance with the mechanisms established in this section;\n\n\n(ii) in reporting incidents to any Member of Congress or any committee of Congress; or\n\n\n(iii) in initiating, testifying in, or assisting in any investigation or judicial or administrative action based upon or related to the incidents described in clause (i) or (ii).\n\n\nIn addition, no employer may require their employees or contractors to obtain prior consent from such employer to report incidents using the reporting mechanism established in this section or to any Member of Congress or any committee of Congress, or to obtain prior consent to participate in investigations, judicial, or administrative actions based upon or related to such incidents.\n\n\n(B) ENFORCEMENT.—Any individual who alleges discharge or other discrimination, or is otherwise aggrieved by an employer or former employer, in infraction of subparagraph (A), may seek relief by—\n\n\n(i) filing a complaint with the Secretary of Labor in accordance with the requirements of this subsection; or\n\n\n(ii) if the Secretary of Labor has not issued a final decision within 180 days of the filing of a complaint under clause (i), and there is no showing that such a delay is due to the bad faith of the claimant, bringing an action against the employer at law or in equity in the appropriate district court of the United States, which shall have jurisdiction over such an action without regard to the amount in controversy.\n\n\n(C) CONFIDENTIALITY.—The Director, and any officer or employee of the National Institute of Standards and Technology or the Cybersecurity and Infrastructure Security Agency, must avoid disclose any information, including information provided by a whistleblower to either such official, which could reasonably be expected to reveal the identity of a whistleblower, except in accordance with the provisions of section 552a of title 5, United States Code, unless and until required to be disclosed to a defendant or respondent in connection with a public proceeding instituted by the appropriate such official.\n\n\n(D) RIGHTS RETAINED.—Nothing in this section shall be deemed to diminish the rights, privileges, or remedies of any whistleblower under any Federal or State law or under any collective bargaining agreement.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: About incidents", "Incentives: Civil liability", "Strategies: Disclosure: In standard form"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/9737", "official_name": "Secure Artificial Intelligence Act of 2024", "label": "safe"}
{"id": "1269_1", "doc_id": "1269", "text": "SECTION 1. SHORT TITLE.\nThis Act may be cited as the “Fraudulent Artificial Intelligence Regulations (FAIR) Elections Act of 2024”.\nSEC. 2. PROHIBITION ON FALSE AI-GENERATED ELECTION MEDIA.\n(a) Definitions.—In this section:\n(1) APPLICABLE FEDERAL ELECTION.—The term “applicable Federal election” means any general, primary, runoff, or special election held solely or in part for the purpose of nominating or electing a candidate for the office of President, Vice President, Presidential elector, Member of the Senate, Member of the House of Representatives, or Delegate or Commissioner from a Territory or possession.\n(2) ELECTION OFFICIAL.—The term “election official” means any individual legally authorized to perform duties in connection with an applicable Federal election, including workers, volunteers, poll workers, and authorized poll observers.\n(3) FALSE AI-GENERATED ELECTION MEDIA.—The term “false AI-generated election media” means text. image, audio, or video that—\n(A) is the product of a computational process that uses machine learning, natural language processing, artificial intelligence techniques, or other computational processing techniques of similar or greater complexity; and\n(B) either—\n(i) contains materially false information relating to—\n(I) the time, place, or manner of holding any applicable Federal election; or\n(II) the qualifications for or restrictions on voter eligibility for any such election; or\n(ii) falsely depicts an election official.\n(b) Prohibition.—Except as provided in subsection (c), a person may not—\n(1) knowingly distribute false AI-generated election media described in subsection (a)(3)(B)(i) if such person—\n(A) knows such media contains materially false information described in such subsection; and\n(B) has the purpose of impeding or preventing another person from exercising the right to vote in an applicable Federal election; or\n(2) knowingly distribute false AI-generated election media described in subsection (a)(3)(B)(ii) if such person—\n(A) knows the depiction of the election official in such media is materially false; and\n(B) has the intent to—\n(i) intimidate or harass an election official; or\n(ii) deter another person from exercising the right to vote in an applicable Federal election.\n(c) Inapplicability To Certain Entities.—This section must avoid apply to the following:\n(1) A radio or television broadcasting station, a cable or satellite television operator, programmer, or producer, or a streaming service that broadcasts false AI-generated election media restricted by this section as part of a bona fide newscast, news interview, news documentary, or on-the-spot coverage of bona fide news events, if the broadcast clearly acknowledges through content or a disclosure, in a manner that can be easily heard or read by the average listener or viewer, that the information contained in the media and described in subsection (a)(2)(B) is false.\n(2) A regularly published newspaper, magazine, or other periodical of general circulation, including an internet or electronic publication, that routinely carries news and commentary of general interest, and that publishes false AI-generated election media restricted under this section, if the publication clearly states that the information contained in the media and described in subsection (a)(2)(B) is false.\n(d) Enforcement.—The Attorney General may bring a civil action against any person who violates subsection (b) in an appropriate United States District Court for such declaratory and injunctive relief (including a temporary restraining order, a permanent or temporary injunction, or other order).", "tags": ["Incentives: Civil liability", "Harms: Detrimental content"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/4714", "official_name": "Fraudulent Artificial Intelligence Regulations (FAIR) Elections Act of 2024", "label": "safe"}
{"id": "1269_2", "doc_id": "1269", "text": "SEC. 3. PROHIBITION ON REMOVAL OF NAMES FROM VOTING ROLLS USING UNVERIFIED VOTER CHALLENGE DATABASES.\n(a) In General.—The National Voter Registration Act of 1993 (52 U.S.C. 20501 et seq.) is amended by inserting after section 8 the following new section:\n“SEC. 8A. RESTRICTIONS ON REMOVAL OF VOTERS FROM OFFICIAL LISTS OF VOTERS.\n“(a) In General.—A State may not remove the name of any registrant from the official list of voters eligible to vote in elections for Federal office or take any action with respect to a voter eligibility challenge unless the registrant or voter is determined to be ineligible to vote based on—\n“(1) source information received from governmental entities and obtained by such entities in the course of carrying out official actions or duties; and\n“(2) source information approved by the Attorney General as sufficient to protect the integrity and completeness of voter registration lists.\n“(b) Approval Of Voter Information Data.—\n“(1) IN GENERAL.—The Attorney General, in consultation with the Director of the National Institute of Standards and Technology and the members of the Election Assistance Commission, may approve source information under subsection (a)(3) if such information meets the following requirements:\n“(A) The source information contains qualifying data sets that allow the State match the individual identified by the source information with an individual on the official list of voters eligible to vote in elections for Federal office.\n“(B) The source information is updated not less than a monthly.\n“(C) The source information was not obtained in infraction of section 1030 of title 18, United States Code (commonly known as the ‘Computer Fraud and Abuse Act’).\n“(2) QUALIFYING DATA SETS.—For purposes of paragraph (1)(A), the term ‘qualifying data sets’ means the following sets of data with respect to an individual:\n“(A) Last name, first name, and full social security number.\n“(B) Last name, first name, and driver’s license or other unique identifying number assigned by the State.\n“(C) Last name, first name, last four digits of a social security number, and date of birth.\n“(D) Last name, full social security number, and date of birth.\n“(c) Coordination With Other Methods.—Nothing in this section be construed to preclude—\n“(1) the removal of names from official lists of voters on a basis described in paragraph (3) (A) or (B) or (4)(A) of section 8(a);\n“(2) the removal of names from official lists of voters on a basis described in section 8(c); or\n“(3) correction of registration records pursuant to this Act.”.\n(b) Conforming Amendments.—Section 8(a) of such Act (52 U.S.C. 20507(a)) is amended—\n(1) in paragraph (3), by striking “provide” and inserting “subject to section 8A, provide”; and\n(2) in paragraph (4), by striking “conduct” and inserting “subject to section 8A, conduct”.", "tags": ["Incentives: Civil liability", "Harms: Detrimental content"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/4714", "official_name": "Fraudulent Artificial Intelligence Regulations (FAIR) Elections Act of 2024", "label": "safe"}
{"id": "1269_3", "doc_id": "1269", "text": "SEC. 4. SEVERABILITY.\nIf any provision of this Act, or an amendment made by this Act, or the application of such provision to any person or circumstance, is held to be invalid, the remainder of this Act, or an amendment made by this Act, or the application of such provision to other persons or circumstances, must avoid be affected.", "tags": ["Incentives: Civil liability", "Harms: Detrimental content"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/4714", "official_name": "Fraudulent Artificial Intelligence Regulations (FAIR) Elections Act of 2024", "label": "safe"}
{"id": "1280_2", "doc_id": "1280", "text": "SEC. 3. PROHIBITING USE OF ARTIFICIAL INTELLIGENCE TO DEPRIVE OR DEFRAUD INDIVIDUALS OF THE RIGHT TO VOTE IN ELECTIONS FOR PUBLIC OFFICE.\n(a) Prohibition.—A developer or deployer must avoid use a covered algorithm in a manner that intentionally deprives or defrauds, or intentionally attempts to deprive or defraud, an individual of the right to vote in an election for Federal, State, or local office, including the following:\n(1) Providing deceptive information regarding—\n(A) the time, place, or method of voting or registering to vote;\n(B) the eligibility requirements to vote or register to vote;\n(C) the counting and canvassing of ballots;\n(D) the adjudication of elections;\n(E) endorsements by any person or candidate; or\n(F) any other material information pertaining to the procedures or requirements for voting or registering to vote in an election for Federal, State, or local office.\n(2) Using deception, threats, intimidation, fraud, or coercion to prevent, interfere with, retaliate against, or deter, or to attempt to prevent, interfere with, retaliate against, or deter, an individual from—\n(A) registering to vote;\n(B) voting;\n(C) supporting or advocating for a candidate in the election; or\n(D) serving as, executing the responsibilities of, or assisting an election worker, including processing or scanning ballots, or tabulating, canvassing, or certifying voting results.\n(b) consequence.—Whoever commits a infraction of subsection (a) shall be fined under title 18, United States Code, imprisoned for not more than 1 year, or both.\n(c) Effective Date.—This section shall take effect upon the expiration of the 60-day period which begins on the date of the enactment of this Act.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Incentives: Fines", "Incentives: Imprisonment"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/8858/text", "official_name": "Securing Elections From AI Deception Act", "label": "safe"}
{"id": "1280_5", "doc_id": "1280", "text": "SEC. 5. DUTY OF CARE.\n(a) In General.—A developer or deployer must avoid offer, license, or use a covered algorithm in a manner that is not safe and effective.\n(b) Safe.—For purposes of subsection (a), a covered algorithm is safe if—\n(1) the developer or deployer has taken reasonable measures to prevent or mitigate harms identified by a pre-deployment evaluation or impact assessment;\n(2) use of the covered algorithm as intended is not likely to result in a infraction of this Act; and\n(3) the developer or deployer evaluates the possibility of not offering, licensing, or using the covered algorithm, or removing a covered algorithm from use, and reasonably concludes that—\n(A) use of the covered algorithm is not likely to result in substantial harm to individuals;\n(B) the benefits to individuals affected by the covered algorithm likely outweigh the costs to such individuals;\n(C) individuals can reasonably avoid being affected by the covered algorithm; and\n(D) use of the covered algorithm is not likely to result in deceptive practices.\n(c) Effective.—For purposes of subsection (a), a covered algorithm is effective if the developer or deployer has taken reasonable steps to ensure that—\n(1) the covered algorithm functions at a level that would be considered reasonable performance by a person with ordinary skill in the art;\n(2) the covered algorithm functions in a manner that is consistent with the expected performance and publicly advertised performance of the covered algorithm;\n(3) the covered algorithm functions in a manner that is consistent with any publicly advertised purpose or use; and\n(4) any data used in the design, development, deployment, or use of the covered algorithm is relevant and appropriate to the deployment context and the publicly advertised purpose.\n(d) Enforcement By Federal Trade Commission.—\n(1) UNFAIR OR DECEPTIVE ACTS OR PRACTICES.—A infraction of this section shall be treated as a infraction of a regulation under section 18(a)(1)(B) of the Federal Trade Commission Act (15 U.S.C. 57a(a)(1)(B)) regarding unfair or deceptive acts or practices.\n(2) POWERS OF COMMISSION.—The Commission shall enforce this section in the same manner, by the same means, and with the same jurisdiction, powers, and duties as though all applicable terms and provisions of the Federal Trade Commission Act (15 U.S.C. 41 et seq.) were incorporated into and made a part of this section, and any person who violates this section shall be subject to the consequences and entitled to the privileges and immunities provided in the Federal Trade Commission Act.", "tags": ["Risk factors: Safety", "Risk factors: Reliability", "Incentives: Civil liability", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/8858/text", "official_name": "Securing Elections From AI Deception Act", "label": "safe"}
{"id": "1376_7", "doc_id": "1376", "text": "6-1-1702. Developer duty to avoid algorithmic discrimination - required documentation. \n\n(1) ON AND AFTER FEBRUARY 1, 2026, A DEVELOPER OF A HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM SHALL USE\nREASONABLE CARE TO PROTECT CONSUMERS FROM ANY KNOWN OR REASONABLY FORESEEABLE RISKS OF ALGORITHMIC DISCRIMINATION ARISING FROM THE INTENDED AND CONTRACTED USES OF THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM. IN ANY ENFORCEMENT ACTION BROUGHT ON OR AFTER FEBRUARY I, 2026, BY THE ATTORNEY GENERAL PURSUANT TO SECTION 6-1-1706, THERE IS A REBUTTABLE PRESUMPTION THAT A DEVELOPER USED REASONABLE CARE AS REQUIRED UNDER THIS SECTION IF THE DEVELOPER COMPLIED WITH THIS SECTION AND ANY ADDITIONAL REQUIREMENTS OR OBLIGATIONS AS SET FORTH IN RULES PROMULGATED BY THE ATTORNEY GENERAL PURSUANT TO SECTION 6-1-1707.\n\n(2)\tON AND AFTER FEBRUARY 1, 2026, AND EXCEPT AS PROVIDED IN SUBSECTION (6) OF THIS SECTION, A DEVELOPER OF A HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM SHALL MAKE AVAILABLE TO THE DEPLOYER OR OTHER DEVELOPER OF THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM:\n\n(a)\tA GENERAL STATEMENT DESCRIBING THE REASONABLY FORESEEABLE USES AND KNOWN HARMFUL OR INAPPROPRIATE USES OF THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM;\n \n(b)\tDOCUMENTATION DISCLOSING:\n\n(I)\tHIGH-LEVEL SUMMARIES OF THE TYPE OF DATA USED TO TRAIN THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM;\n\n(II)\tKNOWN OR REASONABLY FORESEEABLE LIMITATIONS OF THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM, INCLUDING KNOWN OR REASONABLY FORESEEABLE RISKS OF ALGORITHMIC DISCRIM INATION ARISING FROM THE INTENDED USES OF THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM;\n\n(III)\tTHE PURPOSE OF THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM;\n\n(IV)\tTHE INTENDED BENEFITS AND USES OF THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM; AND\n\n(V)\tALL OTHER INFORMATION NECESSARY TO ALLOW THE DEPLOYER TO COMPLY WITH THE REQUIREMENTS OF SECTION 6-1-1703;", "tags": ["Risk factors: Bias", "Risk factors: Interpretability and explainability", "Risk factors: Transparency", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Disclosure", "Incentives: Civil liability"], "source": "https://leg.colorado.gov/bills/sb24-205", "official_name": "An Act concerning consumer protections in interactions with artificial intelligence systems.", "label": "safe"}
{"id": "1376_8", "doc_id": "1376", "text": "(c)\tDOCUMENTATION DESCRIBING:\n\n(I)\tHOW THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM WAS EVALUATED FOR PERFORMANCE AND MITIGATION OF ALGORITHMIC DISCRIMINATION BEFORE THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM WAS OFFERED, SOLD, LEASED, LICENSED, GIVEN, OR OTHER WISE MADE AVAILABLE TO THE DEPLOYER;\n\n(II)\tTHE DATA GOVERNANCE MEASURES USED TO COVER THE TRAINING DATASETS AND THE MEASURES USED TO EXAMINE THE SUITABILITY OF DATA SOURCES, POSSIBLE BIASES, AND APPROPRIATE MITIGATION;\n\n(III)\tTHE INTENDED OUTPUTS OF THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM;\n\n(IV)\tTHE MEASURES THE DEVELOPER HAS TAKEN TO MITIGATE KNOWN OR REASONABLY FORESEEABLE RISKS OF ALGORITHMIC DISCRIMINATION THAT MAY ARISE FROM THE REASONABLY FORESEEABLE DEPLOYMENT OF THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM; AND\n \n(V)\tHow THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM SHOULD BE USED, NOT BE USED, AND BE MONITORED BY AN INDIVIDUAL WHEN THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM IS USED TO MAKE, OR IS A SUBSTANTIAL FACTOR IN MAKING, A CONSEQUENTIAL DECISION; AND\n\n(d)\tANY ADDITIONAL DOCUMENTATION THAT IS REASONABLY NECESSARY TO ASSIST THE DEPLOYER IN UNDERSTANDING THE OUTPUTS AND MONITOR THE PERFORMANCE OF THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM FOR RISKS OF ALGORITHMIC DISCRIMINATION.", "tags": ["Risk factors: Bias", "Risk factors: Reliability", "Risk factors: Transparency", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Performance requirements"], "source": "https://leg.colorado.gov/bills/sb24-205", "official_name": "An Act concerning consumer protections in interactions with artificial intelligence systems.", "label": "safe"}
{"id": "1376_9", "doc_id": "1376", "text": "(3)\t(a) EXCEPT AS PROVIDED IN SUBSECTION (6) OF THIS SECTION, A DEVELOPER THAT OFFERS, SELLS, LEASES, LICENSES, GIVES, OR OTHERWISE MAKES AVAILABLE TO A DEPLOYER OR OTHER DEVELOPER A HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM ON OR AFTER FEBRUARY 1, 2026, SHALL MAKE AVAILABLE TO THE DEPLOYER OR OTHER DEVELOPER, TO THE EXTENT FEASIBLE, THE DOCUMENTATION AND INFORMATION, THROUGH ARTIFACTS\nSUCH AS MODEL CARDS, DATASET CARDS, OR OTHER IMPACT ASSESSMENTS, NECESSARY FOR A DEPLOYER, OR FOR A THIRD PARTY CONTRACTED BY A DEPLOYER, TO COMPLETE AN IMPACT ASSESSMENT PURSUANT TO SECTION\n6-1-1703 (3).\n\n(b)\tA DEVELOPER THAT ALSO SERVES AS A DEPLOYER FOR A HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM IS NOT REQUIRED TO GENERATE THE DOCUMENTATION REQUIRED BY THIS SECTION UNLESS THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM IS PROVIDED TO AN UNAFFILIATED ENTITY ACTING AS A DEPLOYER.\n\n(4)\t(a) ON AND AFTER FEBRUARY 1, 2026, A DEVELOPER SHALL MAKE AVAILABLE, IN A MANNER THAT IS CLEAR AND READILY AVAILABLE ON THE DEVELOPER'S WEBSITE OR IN A PUBLIC USE CASE INVENTORY, A STATEMENT SUMMARIZING:\n\n(I)\tTHE TYPES OF HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEMS THAT THE DEVELOPER HAS DEVELOPED OR INTENTIONALLY AND SUBSTANTIALLY MODIFIED AND CURRENTLY MAKES AVAILABLE TO A DEPLOYER OR OTHER DEVELOPER; AND\n\n(II)\tHow THE DEVELOPER MANAGES KNOWN OR REASONABL y FORESEEABLE RISKS OF ALGORITHMIC DISCRIMINATION THAT MAY ARISE FROM THE DEVELOPMENT OR INTENTIONAL AND SUBSTANTIAL MODIFICATION\n \nOF THE TYPES OF HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEMS DESCRIBED IN ACCORDANCE WITH SUBSECTION (4)(a)(I) OF THIS SECTION.\n\n(b)\tA DEVELOPER SHALL UPDATE THE STATEMENT DESCRIBED IN SUBSECTION (4)(a) OF THIS SECTION:\n\n(I)\tAS NECESSARY TO ENSURE THAT THE STATEMENT REMAINS ACCURATE; AND\n\n(II)\tNo LATER THAN NINETY DAYS AFTER THE DEVELOPER INTENTIONALLY AND SUBSTANTIALLY MODIFIES ANY HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM DESCRIBED IN SUBSECTION ( 4)(a)(l) OFTHIS SECTION.", "tags": ["Risk factors: Bias", "Harms: Discrimination", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment"], "source": "https://leg.colorado.gov/bills/sb24-205", "official_name": "An Act concerning consumer protections in interactions with artificial intelligence systems.", "label": "safe"}
{"id": "1376_11", "doc_id": "1376", "text": "6-1-1703. Deployer duty to avoid algorithmic discrimination - risk management policy and program. \n\n(1) ON AND AFTER FEBRUARY 1, 2026, A DEPLOYER OF A HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM SHALL\nUSE REASONABLE CARE TO PROTECT CONSUMERS FROM ANY KNOWN OR REASONABLY FORESEEABLE RISKS OF ALGORITHMIC DISCRIMINATION. IN ANY ENFORCEMENT ACTION BROUGHT ON OR AFTER FEBRUARY 1, 2026, BY THE ATTORNEY GENERAL PURSUANT TO SECTION 6-1-1706, THERE IS A REBUTTABLE PRESUMPTION THAT A DEPLOYER OF A HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM USED REASONABLE CARE AS REQUIRED UNDER THIS SECTION IF THE DEPLOYER COMPLIED WITH THIS SECTION AND ANY ADDITIONAL REQUIREMENTS OR OBLIGATIONS AS SET FORTH IN RULES PROMULGATED  BY THE ATTORNEY GENERAL  PURSUANT TO SECTION 6-1-1707.", "tags": ["Risk factors: Bias", "Harms: Discrimination", "Strategies: Performance requirements", "Incentives: Civil liability"], "source": "https://leg.colorado.gov/bills/sb24-205", "official_name": "An Act concerning consumer protections in interactions with artificial intelligence systems.", "label": "safe"}
{"id": "1376_14", "doc_id": "1376", "text": "(c)\tIN ADDITION TO THE INFORMATION REQUIRED UNDER SUBSECTION (3)(b) OF THIS SECTION, AN IMPACT ASSESSMENT COMPLETED PURSUANT TO THIS SUBSECTION (3) FOLLOWING AN INTENTIONAL AND SUBSTANTIAL MODIFICATION TO A HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM ON OR AFTER FEBRUARY 1, 2026, MUST INCLUDE A STATEMENT DISCLOSING THE EXTENT TO WHICH THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM WAS USED IN A MANNER THAT WAS CONSISTENT WITH, OR VARIED FROM, THE DEVELOPER'S INTENDED USES OF THE HIGH-RISK ARTIFICIAL INTELLIGENCE\nSYSTEM.\n\n(d)\tA SINGLE IMPACT ASSESSMENT MAY ADDRESS A COMPARABLE SET OF HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEMS DEPLOYED BY A DEPLOY ER.\n\n(e)\tIF A DEPLOYER, OR A THIRD PARTY CONTRACTED BY THE DEPLOYER, COMPLETES AN IMPACT ASSESSMENT FOR THE PURPOSE OF COMPLYING WITH ANOTHER APPLICABLE LAW OR REGULATION, THE IMPACT ASSESSMENT SATISFIES THE REQUIREMENTS ESTABLISHED IN THIS SUBSECTION (3) IF THE IMPACT ASSESSMENT IS REASONABLY SIMILAR IN SCOPE AND EFFECT TO THE IMPACT ASSESSMENT THAT WOULD OTHERWISE BE COMPLETED PURSUANT TO THIS SUBSECTION (3).", "tags": ["Risk factors: Bias", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Performance requirements"], "source": "https://leg.colorado.gov/bills/sb24-205", "official_name": "An Act concerning consumer protections in interactions with artificial intelligence systems.", "label": "safe"}
{"id": "1376_15", "doc_id": "1376", "text": "(f)\tA DEPLOYER SHALL MAINTAIN THE MOST RECENTLY COMPLETED IMPACT ASSESSMENT FOR A HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM AS REQUIRED UNDER THIS SUBSECTION (3), ALL RECORDS CONCERNING EACH IMPACT ASSESSMENT, AND ALL PRIOR IMPACT ASSESSMENTS, IF ANY, FOR AT LEAST THREE YEARS FOLLOWING THE FINAL DEPLOYMENT OF THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM.\n\n(g)\tON OR BEFORE FEBRUARY 1, 2026, AND AT LEAST ANNUALLY THEREAFTER, A DEPLOYER, OR A THIRD PARTY CONTRACTED BY THE DEPLOYER, MUST REVIEW THE DEPLOYMENT OF EACH HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM DEPLOYED BY THE DEPLOYER TO ENSURE THAT THE HIGH-RISK ARTIFICIAL INTELLIGENCE SYSTEM IS NOT CAUSING ALGORITHMIC DISCRIMINATION.", "tags": ["Risk factors: Bias", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure"], "source": "https://leg.colorado.gov/bills/sb24-205", "official_name": "An Act concerning consumer protections in interactions with artificial intelligence systems.", "label": "safe"}
{"id": "1377_4", "doc_id": "1377", "text": "Article 7\tPrinciple of Safety and Accountability\n\nAI developers, providers, and users shall, in accordance with law, adopt the necessary technical means and management measures to ensure the safety and reliability of AI products and services.\n\nAI developers, providers, and users shall, in accordance with law, bear the corresponding legal liabilities for development, provisioning, and use activities.\n\n---\n\nArticle 8\tPrinciple of Proper Use\n\nAI developers, providers, and users shall comply with laws and regulations, respect social morality and ethics, comply with business ethics and professional ethics, be honest and trustworthy, fulfill their obligation to protect AI safety and security, and shoulder their responsibility to society, and must avoid jeopardize national security or the public interest, or harm the legitimate rights and interests of individuals and organizations.\n\nAI developers are encouraged to embed usage restrictions in product designs to prevent users from utilizing AI to engage in illegal activities.\n\n---", "tags": ["Risk factors: Safety", "Risk factors: Reliability", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines", "Risk factors: Security", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Reliability: Robustness"], "source": "https://cset.georgetown.edu/publication/china-ai-law-draft/", "official_name": "Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars)", "label": "safe"}
{"id": "1377_23", "doc_id": "1377", "text": "Article 35\tRight to Explanation and Refusal of AI-Based Decisions\n\nAI providers shall assure the right of users to withdraw from or refuse the use of AI, except where it is part of the basic functions of products and services.\n\nIndividuals and organizations whose legitimate rights and interests may be significantly affected by AI products and services shall have the right to request the AI provider give explanations, the right to reject decisions made solely through AI products and services, and the right to request the AI provider redo the decisions with the participation of human beings.\n\n---", "tags": ["Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure", "Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Transparency"], "source": "https://cset.georgetown.edu/publication/china-ai-law-draft/", "official_name": "Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars)", "label": "safe"}
{"id": "1377_28", "doc_id": "1377", "text": "Article 42\tSafety Risk Assessment\n\nAI developers and providers shall carry out a safety risk assessment before providing products and services, and record the handling circumstances. The AI safety risk assessment shall include the following:\n\n(i) Whether there is potential bias or discrimination;\n\n(ii) The impact on the public interest, the rights and interests of individuals, and safety risks;\n\n(iii) Whether scientific and technological ethics review is conducted in accordance with law; and\n\n(iv) Whether the protection measures are legal, effective, and appropriate to the level of risk.\n\nIn the event of significant changes to AI products and services, the AI safety risk assessment shall be re-conducted.\n\nAI developers and providers may conduct AI safety risk assessments on their own or by commissioning third-party organizations. The AI safety risk assessment report and records of the handling circumstances shall be retained for at least three years.\n\n---", "tags": ["Risk factors: Safety", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Risk factors: Bias", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation: External auditing"], "source": "https://cset.georgetown.edu/publication/china-ai-law-draft/", "official_name": "Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars)", "label": "safe"}
{"id": "1377_31", "doc_id": "1377", "text": "Article 47\tContent Safety and Security\n\nWhere providers of AI services provide network information services, they shall fulfill network information safety and security obligations in accordance with law.\n\nProviders of AI services shall take measures to prevent the generation of false and harmful information and other content restricted by laws and regulations, and where the provider discovers illegal content, it shall promptly take measures for handling it such as stopping generation and transmission, eliminating it, and reporting it to the main oversight department for AI.\n\n---\n\nArticle 48\tIdentifier Obligations\n\nAI providers shall add invisible identifiers (隐式标识) in reasonable locations and areas of the content of products and services, and establish an information traceability mechanism for invisible identifiers to ensure the readability and security of invisible identifiers.\n\nWhere AI products and services may lead to confusion or misidentification by the public, the provider shall take technical measures to add, in reasonable locations and areas of the content of the products and services, visible identifiers (显式标识) that do not affect use by users, and use explicit means to prompt the public with the necessary information about the AI products and services.\n\nNo organization or individual shall use technical means to delete, tamper with, or conceal the identifiers added to AI products and services in accordance with law.\n\n---", "tags": ["Risk factors: Safety", "Risk factors: Security", "Harms: Detrimental content", "Strategies: Disclosure: About incidents", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Performance requirements", "Applications: Networking and telecommunications"], "source": "https://cset.georgetown.edu/publication/china-ai-law-draft/", "official_name": "Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars)", "label": "safe"}
{"id": "1377_32", "doc_id": "1377", "text": "Article 49\tAccess Requirements\n\nWhere laws and administrative regulations stipulate that the development and provision of AI products and services shall be licensed, the developers and providers shall obtain licenses in accordance with law.\n\nForeign investment (外商投资) in AI products and services shall also comply with the provisions of laws and administrative regulations relating to foreign investment.\n\nSection II\tObligations for Critical AI\n\n---", "tags": ["Strategies: Licensing, registration, and certification"], "source": "https://cset.georgetown.edu/publication/china-ai-law-draft/", "official_name": "Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars)", "label": "safe"}
{"id": "1377_35", "doc_id": "1377", "text": "Article 53\tRegistration of Critical AI\n\nProviders of critical AI shall, within 7 working days from the date of receiving a notice of determination [that they are indeed providers of critical AI], register through the national AI supervision platform and perform the filing procedures. The department in charge of AI shall, within 30 working days, conduct the record-keeping or request additional materials.\n\nOther AI products and services that, by laws and regulations, require registration shall be registered in accordance with the procedures prescribed by laws and regulations.\n\n---\n\nArticle 54\tSecurity Risk Assessment of Critical AI\n\nDevelopers and providers of critical AI shall, on their own or by commissioning a third-party organization, conduct at least one AI security risk assessment of critical AI each year, promptly rectify any security problems found, and report out to the main oversight department for AI.\n\n---", "tags": ["Strategies: Licensing, registration, and certification", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure"], "source": "https://cset.georgetown.edu/publication/china-ai-law-draft/", "official_name": "Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars)", "label": "safe"}
{"id": "1377_46", "doc_id": "1377", "text": "Article 69\tUse of AI by State Organs\n\nWhere state organs use AI to implement administrative actions, they shall abide by the principles of legality, reasonable justification, and proportionality, and AI-based decisions can only be used as a reference for administrative actions.\n\n---\n\nArticle 70\tJudicial AI\n\nJudicial AI development, provision, and use activities shall adhere to the principles of security and legality, fairness and impartiality, assistance in trials (辅助审判), transparency and credibility, and public order and moral decency (公序良俗).\n\nWhere AI is used to assist judicial work, AI-based decisions may only be used as a reference for judicial work; users shall have the right to withdraw from interaction with AI products and services at any time.\n\n---", "tags": ["Applications: Government: other applications/unspecified", "Risk factors: Security", "Risk factors: Bias", "Risk factors: Transparency", "Strategies: Disclosure: In deployment", "Applications: Government: judicial and law enforcement"], "source": "https://cset.georgetown.edu/publication/china-ai-law-draft/", "official_name": "Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars)", "label": "safe"}
{"id": "1377_47", "doc_id": "1377", "text": "Article 71\tNews AI\n\nWhere AI is used to provide internet news and information services, the user of the AI shall formulate a special system for reviewing and approving the release of news and information, ensure the veracity and accuracy of the news and information, and increase the use of visible identifiers for AI-generated content.\n\n---\n\nArticle 72\tMedical AI\n\nThose who use AI to engage in medical services shall have the appropriate licenses and qualifications prescribed by law, and AI decision-making may only be used as a reference for medical activities.\n\n---", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Broadcasting and media production", "Applications: Medicine, life sciences and public health", "Strategies: Licensing, registration, and certification"], "source": "https://cset.georgetown.edu/publication/china-ai-law-draft/", "official_name": "Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars)", "label": "safe"}
{"id": "1377_48", "doc_id": "1377", "text": "Article 73\tSocial Bots\n\nAI providers providing social media robot (“social bot”) services shall reasonably control the quantity and quality of network information published by social bots, and review and handle the information content generated by social bots.\n\nUsers must avoid use social bots to manipulate and guide public opinion on the quality of goods, business reputation, or other evaluation content that concerns the legitimate rights and interests of consumers; and where users abuse social bots, AI providers shall take measures such as permission limitation, account blocking, information blocking, etc.\n\n---", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Applications: Broadcasting and media production", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Strategies: Tiering: Tiering based on domain of application"], "source": "https://cset.georgetown.edu/publication/china-ai-law-draft/", "official_name": "Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars)", "label": "safe"}
{"id": "1377_49", "doc_id": "1377", "text": "Article 74\tBiometric Recognition\n\nThe use of AI to process biometric information shall have a specific purpose and sufficient necessity, and strict protection measures shall be adopted. Where there exist other non-biometric technical solutions that can achieve the same purpose or meet the same business requirements, preference shall be given to non-biometric technical solutions.\n\nIf the biometric information generated by AI is used maliciously or has the possibility of identifying a specific natural person, upon the request of the rights holder, the AI provider shall take necessary measures such as blocking, breaking links, and information deletion.\n\n---", "tags": ["Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements"], "source": "https://cset.georgetown.edu/publication/china-ai-law-draft/", "official_name": "Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars)", "label": "safe"}
{"id": "1377_50", "doc_id": "1377", "text": "Article 75\tAutonomous Driving\n\nAppropriate licenses and accesses shall be obtained for the road testing, demonstration, market launching, and transport operation of autonomous vehicles.\n\nDevelopers and providers of autonomous vehicles shall ensure the safety of the vehicles and must avoid engage in unreasonable differential treatment of other traffic participants when the vehicles are in danger.\n\nUsers of autonomous vehicles shall master and regulate the use of autonomous driving functions in accordance with road traffic regulations and the requirements of the vehicle’s instruction manual.\n\n---\n\nArticle 76\tSocial benefit\n\nThe use of AI for social benefit scoring and rating shall be aimed at improving benefit quality and efficiency and strengthening the level of risk management, and must avoid infringe upon the right to equal access to the provision of public services or other legitimate rights and interests of individuals.\n\n---", "tags": ["Applications: Transportation", "Strategies: Licensing, registration, and certification", "Harms: Harm to health/safety", "Applications: Government: benefits and welfare", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Bias", "Harms: Discrimination"], "source": "https://cset.georgetown.edu/publication/china-ai-law-draft/", "official_name": "Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars)", "label": "safe"}
{"id": "1377_51", "doc_id": "1377", "text": "Article 77\tSpecial Requirements for AGI\n\nDevelopers of AGI shall ensure safety and trustworthiness through value alignment and other technical means, conduct regular safety assessments based on risks and system capabilities, enhance the transparency and explainability of AGI, and report the results of safety assessments to the main oversight department for AI.\n\n---\n\nChapter VII\tInternational Cooperation\n\n---", "tags": ["Strategies: Tiering", "Strategies: Tiering: Tiering based on generality", "Strategies: Tiering: Tiering based on impact", "Risk factors: Safety", "Risk factors: Reliability", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Risk factors: Transparency", "Risk factors: Interpretability and explainability", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation"], "source": "https://cset.georgetown.edu/publication/china-ai-law-draft/", "official_name": "Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars)", "label": "safe"}
{"id": "1377_54", "doc_id": "1377", "text": "Article 82\tGeneral Provisions on Administrative consequences\n\nAny [entity] that engages in AI development, provision, or use activities in infraction of the provisions of this Law will be ordered to make rectification and will be given a warning by a main oversight department for AI, and the illegal income shall be confiscated; if it refuses to make rectification, it shall be fined not more than 1 million [Chinese] yuan [Renminbi (RMB)]; and the main overseer who is directly responsible (直接负责的主管人员) and other directly responsible personnel shall be fined not less than RMB 10,000 and not more than RMB 100,000.\n\nWhere there is a infraction as stipulated in the preceding paragraph, and the circumstances are serious, a main oversight department for AI at or above the provincial level shall order it to make corrections, confiscate the illegal income, and impose a charge of not more than RMB 50 million or five percent of the turnover of the previous year, and it may order the suspension of the relevant line of business or cessation of operations and rectification, and notify the relevant main oversight department to revoke the relevant business permits or revoke the business license; it shall charge the main overseer who is directly responsible and other directly responsible personnel not less than RMB 100,000 and not more than RMB 1 million, and may decide to restrict them from serving as directors, supervisors, senior management personnel, and persons with responsibility for AI safety and security in relevant enterprises within a certain period of time.\n\n---", "tags": ["Incentives: Fines", "Incentives: Access to business opportunities"], "source": "https://cset.georgetown.edu/publication/china-ai-law-draft/", "official_name": "Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars)", "label": "safe"}
{"id": "1377_56", "doc_id": "1377", "text": "Article 84\tresponsibility of State Organs\n\nA State organ carrying out AI development, provision, or use activities shall comply with the provisions of this Law; where it violates the provisions of this Law, it shall be ordered by its superior organ or a main oversight department for AI to make corrections; the main overseer who is directly responsible and other directly responsible personnel shall be given administrative sanctions in accordance with law, and where personnel of a State organ neglect their duties, abuse their power, or engage in fraud for personal gain, which does not yet constitute a crime, they shall be given administrative sanctions in accordance with law.\n\n---\n\nArticle 85\tPrinciples for Attribution of responsibility to Providers\n\nWhere AI products and services cause damages to others and the provider has failed to fulfill its obligations under this Law, the provider shall bear tort responsibility (侵权责任).\n\nWhere critical AI products and services cause damages to others and the provider cannot prove that it is not at fault, the provider shall bear tort responsibility. Where the law prescribes compensation limits, compensation shall be in accordance with its provisions, except where the developer or provider of the critical AI showed intent or gross negligence.\n\n---\n\nArticle 86\tPrinciples for Attribution of responsibility to Users\n\nIf the use of AI products and services causes damages to others and the user is at fault, the user shall bear tort responsibility; if the developer or provider of the AI has failed to fulfill its obligations under this Law, it shall bear the corresponding tort responsibility. Where [other] laws dictate otherwise, responsibility shall be attributed in accordance with their provisions.\n\n---", "tags": ["Incentives: Civil liability", "Harms: Violation of civil or human rights, including privacy"], "source": "https://cset.georgetown.edu/publication/china-ai-law-draft/", "official_name": "Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars)", "label": "safe"}
{"id": "1377_57", "doc_id": "1377", "text": "Article 87\tSafe Harbor (避风港) Rules for Service Providers\n\nWhere a user utilizes an AI service to commit a tortious act, the rights holder has the right to notify the provider of the AI service to take necessary measures such as blocking prompts, closing or revoking the infringing account, etc. The notification shall include prima facie evidence of what constitutes infringement and the rights holder’s true identity information.\n\nThe provider of AI services, after receiving the notification, shall promptly forward said notification to the relevant users and alert them of the infringement risk; if it fails to take the necessary measures in a timely manner, it shall bear joint and several responsibility with the user for the expanded portion of the damages.\n\nWhere a provider of AI services knows or should know that a user has utilized the AI service it provides to infringe upon the civil rights and interests of other people and fails to take the necessary measures, it shall bear joint and several responsibility with the user.\n\n---", "tags": ["Incentives: Civil liability", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure: About incidents"], "source": "https://cset.georgetown.edu/publication/china-ai-law-draft/", "official_name": "Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars)", "label": "safe"}
{"id": "1377_58", "doc_id": "1377", "text": "Article 88\tCoordination with Product responsibility\n\nWhere a defect in an AI product causes damages to another person, the injured person may request compensation from the producer and seller; AI providers and users, after assuming tort responsibility in accordance with law, shall have the right to recover compensation from the producer and seller.\n\nWhere the AI provider or user simultaneously constitutes the producer and seller, the victim may choose to apply the provisions of this Law or the relevant legal provisions on product responsibility.\n\n---\n\nArticle 89\tPunitive Damages\n\nWhere a tortfeasor knows that there are significant safety or security risks in an AI product or service and does not take the necessary measures, and the circumstances are serious, the injured person shall have the right to request that the tortfeasor bear responsibility for the corresponding punitive damages.\n\n---", "tags": ["Strategies: Tiering", "Strategies: Tiering: Tiering based on impact", "Harms: Violation of civil or human rights, including privacy", "Incentives: Civil liability"], "source": "https://cset.georgetown.edu/publication/china-ai-law-draft/", "official_name": "Artificial Intelligence Law of the People’s Republic of China (Draft for Suggestions from Scholars)", "label": "safe"}
{"id": "1381_1", "doc_id": "1381", "text": "The General Assembly,\n\n[Opening declarations omitted.]\n\n1.\tResolves to bridge the artificial intelligence and other digital divides between and within countries;\n\n2.\tResolves to promote safe, secure and trustworthy artificial intelligence systems to accelerate progress towards the full realization of the 2030 Agenda for Sustainable Development, 10 further bridging the artificial intelligence and other digital divides between and within countries; and stresses the need for the standard of safe, secure and trustworthy artificial intelligence systems to promote, not hinder, digital transformation and equitable access to their benefits in order to achieve all 17 Sustainable Development Goals and sustainable development in its three dimensions – economic, social and environmental – and address other shared global challenges, particularly for developing countries;", "tags": ["Risk factors: Safety", "Risk factors: Security", "Risk factors: Reliability", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy"], "source": "https://documents.un.org/doc/undoc/ltd/n24/065/92/pdf/n2406592.pdf", "official_name": "Seizing the opportunities of safe, secure and trustworthy artificial intelligence systems for sustainable development", "label": "safe"}
{"id": "1382_2", "doc_id": "1382", "text": "1.\tTake appropriate measures throughout the development of advanced AI systems, including prior to and throughout their deployment and placement on the market, to identify, evaluate, and mitigate risks across the AI lifecycle. \n \nThis includes employing diverse internal and independent external testing measures, through a combination of methods such as red-teaming, and implementing appropriate mitigation to address identified risks and vulnerabilities. Testing and mitigation measures should for example, seek to ensure the trustworthiness, safety and security of systems throughout their entire lifecycle so that they do not pose unreasonable risks. In support of such testing, developers should seek to enable traceability, in relation to datasets, processes, and decisions made during system development.", "tags": ["Risk factors: Bias", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Evaluation: Adversarial testing", "Strategies: Evaluation: External auditing", "Risk factors: Reliability"], "source": "https://digital-strategy.ec.europa.eu/en/library/hiroshima-process-international-guiding-principles-advanced-ai-system", "official_name": "Hiroshima Process International Guiding Principles for Advanced AI Systems", "label": "safe"}
{"id": "1382_3", "doc_id": "1382", "text": "2.\tPatterns of misuse, after deployment including placement on the market. \n \nOrganizations should use, as and when appropriate commensurate to the level of risk, AI systems as intended and monitor for vulnerabilities, incidents, emerging risks and misuse after deployment, and take appropriate action to address these. Organizations are encouraged to consider, for example, facilitating third-party and user discovery and reporting of issues and vulnerabilities after deployment. Organizations are further encouraged to maintain appropriate documentation of reported incidents and to mitigate the identified risks and vulnerabilities, in collaboration with other stakeholders. Mechanisms to report vulnerabilities, where appropriate, should be accessible to a diverse set of stakeholders.", "tags": ["Risk factors: Reliability", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: External auditing", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: About incidents", "Strategies: Convening", "Strategies: Performance requirements"], "source": "https://digital-strategy.ec.europa.eu/en/library/hiroshima-process-international-guiding-principles-advanced-ai-system", "official_name": "Hiroshima Process International Guiding Principles for Advanced AI Systems", "label": "safe"}
{"id": "1382_5", "doc_id": "1382", "text": "4.\tWork towards responsible information sharing and reporting of incidents among organizations developing advanced AI systems including with industry, governments, civil society, and academia. \n \nThis includes responsibly sharing information, as appropriate, including, but not limited to evaluation reports, information on security and safety risks, dangerous, intended or unintended capabilities, and attempts AI actors to circumvent safeguards across the AI lifecycle.", "tags": ["Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About incidents", "Strategies: Convening", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Reliability: Robustness"], "source": "https://digital-strategy.ec.europa.eu/en/library/hiroshima-process-international-guiding-principles-advanced-ai-system", "official_name": "Hiroshima Process International Guiding Principles for Advanced AI Systems", "label": "safe"}
{"id": "1382_7", "doc_id": "1382", "text": "6.\tInvest in and implement robust security controls, including physical security, cybersecurity and insider threat safeguards across the AI lifecycle. \n \nThese may include securing model weights and algorithms, servers, and datasets, such as through operational security measures for information security and appropriate cyber/physical access controls.", "tags": ["Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Security: Dissemination", "Strategies: Performance requirements"], "source": "https://digital-strategy.ec.europa.eu/en/library/hiroshima-process-international-guiding-principles-advanced-ai-system", "official_name": "Hiroshima Process International Guiding Principles for Advanced AI Systems", "label": "safe"}
{"id": "1382_8", "doc_id": "1382", "text": "7.\tDevelop and deploy reliable content authentication and provenance mechanisms, where technically feasible, such as watermarking or other techniques to enable users to identify AI-generated content \n \nThis includes, where appropriate and technically feasible, content authentication such provenance mechanisms for content created with an organization’s advanced AI system. The provenance data should include an identifier of the service or model that created the content, but need not include user information. Organizations should also endeavor to develop tools or APIs to allow users to determine if particular content was created with their advanced AI system such as via watermarks. \n \nOrganizations are further encouraged to implement other mechanisms such as labeling or disclaimers to enable users, where possible and appropriate, to know when they are interacting with an AI system.", "tags": ["Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements"], "source": "https://digital-strategy.ec.europa.eu/en/library/hiroshima-process-international-guiding-principles-advanced-ai-system", "official_name": "Hiroshima Process International Guiding Principles for Advanced AI Systems", "label": "safe"}
{"id": "1383_2", "doc_id": "1383", "text": "1 Take appropriate measures throughout the development of advanced AI systems, including prior to and throughout their deployment and placement on the market, to identify, evaluate, and mitigate risks across the AI lifecycle. \n \nThis includes employing diverse internal and independent external testing measures, through a combination of methods for evaluations, such as red-teaming, and implementing appropriate mitigation to address identified risks and vulnerabilities. Testing and mitigation measures, should, for example, seek to ensure the trustworthiness, safety and security of systems throughout their entire lifecycle so that they do not pose unreasonable risks. In support of such testing, developers should seek to enable traceability, in relation to datasets, processes, and decisions made during system development. These measures should be documented and supported by regularly updated technical documentation. \n \nThis testing should take place in secure environments and be performed at several checkpoints throughout the AI lifecycle in particular before deployment and placement on the market to identify risks and vulnerabilities, and to inform action to address the identified AI risks to security, safety and societal and other risks, whether accidental or intentional. In designing and implementing testing measures, organizations commit to devote attention to the following risks as appropriate: \n \n> Chemical, biological, radiological, and nuclear risks, such as the ways in which advanced AI systems can lower barriers to entry, including for non-state actors, for weapons development, design acquisition, or use. \n \n> Offensive cyber capabilities, such as the ways in which systems can enable vulnerability discovery, exploitation, or operational use, bearing in mind that such capabilities could also have useful defensive applications and might be appropriate to include in a system.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Strategies: Evaluation: Impact assessment", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Disclosure: In standard form", "Applications: Government: military and public safety", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Reliability", "Strategies: Evaluation: Post-market monitoring"], "source": "https://digital-strategy.ec.europa.eu/en/library/hiroshima-process-international-code-conduct-advanced-ai-systems", "official_name": "Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems", "label": "safe"}
{"id": "1383_4", "doc_id": "1383", "text": "2\tIdentify and mitigate vulnerabilities, and, where appropriate, incidents and patterns of misuse, after deployment including placement on the market. \n \nOrganizations should use, as and when appropriate commensurate to the level of risk, AI systems as intended and monitor for vulnerabilities, incidents, emerging risks and misuse after deployment, and take appropriate action to address these. Organizations are encouraged to consider, for example, facilitating third-party and user discovery and reporting of issues and vulnerabilities after deployment such as through bounty systems, contests, or prizes to incentivize the responsible disclosure of weaknesses. Organizations are further encouraged to maintain appropriate documentation of reported incidents and to mitigate the identified risks and vulnerabilities, in collaboration with other stakeholders. Mechanisms to report vulnerabilities, where appropriate, should be accessible to a diverse set of stakeholders.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation: External auditing", "Strategies: Convening", "Strategies: Evaluation: Impact assessment"], "source": "https://digital-strategy.ec.europa.eu/en/library/hiroshima-process-international-code-conduct-advanced-ai-systems", "official_name": "Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems", "label": "safe"}
{"id": "1383_6", "doc_id": "1383", "text": "4 Work towards responsible information sharing and reporting of incidents among organizations developing advanced AI systems including with industry, governments, civil society, and academia \n \nThis includes responsibly sharing information, as appropriate, including, but not limited to evaluation reports, information on security and safety risks, dangerous intended or unintended capabilities, and attempts by AI actors to circumvent safeguards across the AI lifecycle. \n \nOrganizations should establish or join mechanisms to develop, advance, and adopt, where appropriate, shared standards, tools, mechanisms, and best practices for ensuring the safety, security, and trustworthiness of advanced AI systems. \n \nThis should also include ensuring appropriate and relevant documentation and transparency across the AI lifecycle in particular for advanced AI systems that cause significant risks to safety and society. \n \nOrganizations should collaborate with other organizations across the AI lifecycle to share and report relevant information to the public with a view to advancing safety, security and trustworthiness of advanced AI systems. Organizations should also collaborate and share the aforementioned information with relevant public authorities, as appropriate. \n \nSuch reporting should safeguard intellectual property rights.", "tags": ["Strategies: Convening", "Strategies: Disclosure", "Risk factors: Transparency", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements", "Risk factors: Security", "Risk factors: Reliability"], "source": "https://digital-strategy.ec.europa.eu/en/library/hiroshima-process-international-code-conduct-advanced-ai-systems", "official_name": "Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems", "label": "safe"}
{"id": "1383_9", "doc_id": "1383", "text": "7\tDevelop and deploy reliable content authentication and provenance mechanisms, where technically feasible, such as watermarking or other techniques to enable users to identify AI-generated content \n \nThis includes, where appropriate and technically feasible, content authentication and provenance mechanisms for content created with an organization’s advanced AI system. The provenance data should include an identifier of the service or model that created the content, but need not include user information. Organizations should also endeavor to develop tools or APIs to allow users to determine if particular content was created with their advanced AI system, such as via watermarks. Organizations should collaborate and invest in research, as appropriate, to advance the state of the field. \n \nOrganizations are further encouraged to implement other mechanisms such as labeling or disclaimers to enable users, where possible and appropriate, to know when they are interacting with an AI system.", "tags": ["Risk factors: Transparency", "Strategies: Disclosure: In deployment", "Strategies: Disclosure", "Strategies: Performance requirements"], "source": "https://digital-strategy.ec.europa.eu/en/library/hiroshima-process-international-code-conduct-advanced-ai-systems", "official_name": "Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems", "label": "safe"}
{"id": "1383_10", "doc_id": "1383", "text": "8\tPrioritize research to mitigate societal, safety and security risks and prioritize investment in effective mitigation measures. \n \nThis includes conducting, collaborating on and investing in research that supports the advancement of AI safety, security, and trust, and addressing key risks, as well as investing in developing appropriate mitigation tools. \n \nOrganizations commit to conducting, collaborating on and investing in research that supports the advancement of AI safety, security, trustworthiness and addressing key risks, such as prioritizing research on upholding democratic values, respecting human rights, protecting children and vulnerable groups, safeguarding intellectual property rights and privacy, and avoiding harmful bias, mis- and disinformation, and information manipulation. Organizations also commit to invest in developing appropriate mitigation tools, and work to proactively manage the risks of advanced AI systems, including environmental and climate impacts, so that their benefits can be realized.  \n \nOrganizations are encouraged to share research and best practices on risk mitigation.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Risk factors: Safety", "Harms: Ecological harm", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Risk factors: Reliability", "Risk factors: Security"], "source": "https://digital-strategy.ec.europa.eu/en/library/hiroshima-process-international-code-conduct-advanced-ai-systems", "official_name": "Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI Systems", "label": "safe"}
{"id": "1385_5", "doc_id": "1385", "text": "Sec. 3.  Promoting and Securing the United States’ Foundational AI Capabilities.  (a)  To preserve and expand United States advantages in AI, it is the policy of the United States Government to promote progress, advancement, and competition in domestic AI development; protect the United States AI ecosystem against foreign intelligence threats; and manage risks to AI safety, security, and trustworthiness.  Leadership in responsible AI development benefits United States national security by enabling applications directly relevant to the national security mission, unlocking economic growth, and avoiding strategic surprise.  United States technological leadership also confers global benefits by enabling like-minded entities to collectively mitigate the risks of AI misuse and accidents, prevent the unchecked spread of digital authoritarianism, and prioritize vital research.", "tags": ["Risk factors: Security: Cybersecurity", "Risk factors: Security", "Risk factors: Safety", "Risk factors: Reliability", "Applications: Government: military and public safety", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy"], "source": "https://www.whitehouse.gov/briefing-room/presidential-actions/2024/10/24/memorandum-on-advancing-the-united-states-leadership-in-artificial-intelligence-harnessing-artificial-intelligence-to-fulfill-national-security-objectives-and-fostering-the-safety-security/", "official_name": "Memorandum on Advancing the United States’ Leadership in Artificial Intelligence; Harnessing Artificial Intelligence to Fulfill National Security Objectives; and Fostering the Safety, Security, and Trustworthiness of Artificial Intelligence", "label": "safe"}
{"id": "1385_11", "doc_id": "1385", "text": "(c)  Foreign actors may also seek to obtain United States intellectual property through gray-zone methods, such as technology transfer and data localization requirements.  AI-related intellectual property often includes critical technical artifacts (CTAs) that would substantially lower the costs of recreating, attaining, or using powerful AI capabilities.  The United States Government must guard against these risks.\n\n     (d)  Consistent with these goals:\n\n(i)  In furtherance of Executive Order 14083 of September 15, 2022 (Ensuring Robust Consideration of Evolving National Security Risks by the Committee on Foreign Investment in the United States), the Committee on Foreign Investment in the United States shall, as appropriate, consider whether a covered transaction involves foreign actor access to proprietary information on AI training techniques, algorithmic improvements, hardware advances, CTAs, or other proprietary insights that shed light on how to create and effectively use powerful AI systems.", "tags": ["Risk factors: Security", "Risk factors: Privacy", "Strategies: Input controls", "Strategies: Input controls: Compute circulation", "Strategies: Input controls: Data use", "Strategies: Input controls: Data circulation", "Strategies: Input controls: Compute use", "Strategies: Tiering: Tiering based on inputs"], "source": "https://www.whitehouse.gov/briefing-room/presidential-actions/2024/10/24/memorandum-on-advancing-the-united-states-leadership-in-artificial-intelligence-harnessing-artificial-intelligence-to-fulfill-national-security-objectives-and-fostering-the-safety-security/", "official_name": "Memorandum on Advancing the United States’ Leadership in Artificial Intelligence; Harnessing Artificial Intelligence to Fulfill National Security Objectives; and Fostering the Safety, Security, and Trustworthiness of Artificial Intelligence", "label": "safe"}
{"id": "1385_12", "doc_id": "1385", "text": "3.3.  Managing Risks to AI Safety, Security, and Trustworthiness.  (a)  Current and near-future AI systems could pose significant safety, security, and trustworthiness risks, including those stemming from deliberate misuse and accidents.  Across many technological domains, the United States has historically led the world not only in advancing capabilities, but also in developing the tests, standards, and norms that underpin reliable and beneficial global adoption.  The United States approach to AI should be no different, and proactively constructing testing infrastructure to assess and mitigate AI risks will be essential to realizing AI’s positive potential and to preserving United States AI leadership.\n\n     (b)  It is the policy of the United States Government to pursue new technical and policy tools that address the potential challenges posed by AI.  These tools include processes for reliably testing AI models’ applicability to harmful tasks and deeper partnerships with institutions in industry, academia, and civil society capable of advancing research related to AI safety, security, and trustworthiness.\n\n     (c)  Commerce, acting through the AI Safety Institute (AISI) within the National Institute of Standards and Technology (NIST), shall serve as the primary United States Government point of contact with private sector AI developers to facilitate voluntary pre- and post-public deployment testing for safety, security, and trustworthiness of frontier AI models.  In coordination with relevant agencies as appropriate, Commerce shall establish an enduring capability to lead voluntary unclassified pre-deployment safety testing of frontier AI models on behalf of the United States Government, including assessments of risks relating to cybersecurity, biosecurity, chemical weapons, system autonomy, and other risks as appropriate (not including nuclear risk, the assessment of which shall be led by DOE).  Voluntary unclassified safety testing shall also, as appropriate, address risks to human rights, civil rights, and civil liberties, such as those related to privacy, discrimination and bias, freedom of expression, and the safety of individuals and groups.  Other agencies, as identified in subsection 3.3(f) of this section, shall establish enduring capabilities to perform complementary voluntary classified testing in appropriate areas of expertise.  The directives set forth in this subsection are consistent with broader taskings on AI safety in section 4 of Executive Order 14110, and provide additional clarity on agencies’ respective roles and responsibilities.\n\n     (d)  Nothing in this subsection shall inhibit agencies from performing their own evaluations of AI systems, including tests performed before those systems are released to the public, for the purposes of evaluating suitability for that agency’s acquisition and procurement.  AISI’s responsibilities do not extend to the evaluation of AI systems for the potential use by the United States Government for national security purposes; those responsibilities lie with agencies considering such use, as outlined in subsection 4.2(e) of this memorandum and the associated framework described in that subsection.", "tags": ["Risk factors: Safety", "Risk factors: Security", "Risk factors: Reliability", "Risk factors: Security: Cybersecurity", "Applications: Government: military and public safety", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Privacy", "Risk factors: Bias", "Harms: Discrimination", "Harms: Harm to health/safety", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Strategies: Convening", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering", "Strategies: Tiering: Tiering based on generality"], "source": "https://www.whitehouse.gov/briefing-room/presidential-actions/2024/10/24/memorandum-on-advancing-the-united-states-leadership-in-artificial-intelligence-harnessing-artificial-intelligence-to-fulfill-national-security-objectives-and-fostering-the-safety-security/", "official_name": "Memorandum on Advancing the United States’ Leadership in Artificial Intelligence; Harnessing Artificial Intelligence to Fulfill National Security Objectives; and Fostering the Safety, Security, and Trustworthiness of Artificial Intelligence", "label": "safe"}
{"id": "1385_21", "doc_id": "1385", "text": "(i)  Agencies shall take actions to protect classified and controlled information, given the potential risks posed by AI:\n\n(i)  In the course of regular updates to policies and procedures, DOD, DOE, and the IC shall consider how analysis enabled by AI tools may affect decisions related to declassification of material, standards for sufficient anonymization, and similar activities, as well as the robustness of existing operational security and equity controls to protect classified or controlled information, given that AI systems have demonstrated the capacity to extract previously inaccessible insight from redacted and anonymized data.", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Risk factors: Reliability: Robustness", "Harms: Violation of civil or human rights, including privacy", "Applications: Government: military and public safety"], "source": "https://www.whitehouse.gov/briefing-room/presidential-actions/2024/10/24/memorandum-on-advancing-the-united-states-leadership-in-artificial-intelligence-harnessing-artificial-intelligence-to-fulfill-national-security-objectives-and-fostering-the-safety-security/", "official_name": "Memorandum on Advancing the United States’ Leadership in Artificial Intelligence; Harnessing Artificial Intelligence to Fulfill National Security Objectives; and Fostering the Safety, Security, and Trustworthiness of Artificial Intelligence", "label": "safe"}
{"id": "1385_32", "doc_id": "1385", "text": "(E)  Lack of transparency:  agencies may have gaps in documentation of AI development and use, and the public may lack access to information about how AI is used in national security contexts because of the necessity to protect classified or controlled information.\n\n(F)  Lack of accountability:  training programs and guidance for agency personnel on the proper use of AI systems may not be sufficient, including to mitigate the risk of overreliance on AI systems (such as “automation bias”), and accountability mechanisms may not adequately address possible intentional or negligent misuse of AI-enabled technologies.\n\n(G)  Data spillage:  AI systems may reveal aspects of their training data — either inadvertently or through deliberate manipulation by malicious actors — and data spillage may result from AI systems trained on classified or controlled information when used on networks where such information is not permitted.\n\n(H)  Poor performance:  AI systems that are inappropriately or insufficiently trained, used for purposes outside the scope of their training set, or improperly integrated into human workflows may exhibit poor performance, including in ways that result in inconsistent outcomes or unlawful discrimination and harmful bias, or that undermine the integrity of decision-making processes.\n\n(I)  Deliberate manipulation and misuse:  foreign state competitors and malicious actors may deliberately undermine the accuracy and efficacy of AI systems, or seek to extract sensitive information from such systems.", "tags": ["Risk factors: Transparency", "Applications: Government: military and public safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Reliability", "Risk factors: Reliability: Robustness", "Harms: Discrimination"], "source": "https://www.whitehouse.gov/briefing-room/presidential-actions/2024/10/24/memorandum-on-advancing-the-united-states-leadership-in-artificial-intelligence-harnessing-artificial-intelligence-to-fulfill-national-security-objectives-and-fostering-the-safety-security/", "official_name": "Memorandum on Advancing the United States’ Leadership in Artificial Intelligence; Harnessing Artificial Intelligence to Fulfill National Security Objectives; and Fostering the Safety, Security, and Trustworthiness of Artificial Intelligence", "label": "safe"}
{"id": "1385_39", "doc_id": "1385", "text": "Sec. 7.  Definitions.  (a)  This memorandum uses definitions set forth in section 3 of Executive Order 14110.  In addition, for the purposes of this memorandum:\n\n(i)     The term “AI safety” means the mechanisms through which individuals and organizations minimize and mitigate the potential for harm to individuals and society that can result from the malicious use, misapplication, failures, accidents, and unintended behavior of AI models; the systems that integrate them; and the ways in which they are used.\n\n(ii)    The term “AI security” means a set of practices to protect AI systems — including training data, models, abilities, and lifecycles — from cyber and physical attacks, thefts, and damage.\n\n(iii)   The term “covered agencies” means agencies in the Intelligence Community, as well as all agencies as defined in 44 U.S.C. 3502(1) when they use AI as a component of a National Security System, other than the Executive Office of the President.", "tags": ["Risk factors: Reliability", "Risk factors: Security: Cybersecurity", "Risk factors: Security", "Risk factors: Safety"], "source": "https://www.whitehouse.gov/briefing-room/presidential-actions/2024/10/24/memorandum-on-advancing-the-united-states-leadership-in-artificial-intelligence-harnessing-artificial-intelligence-to-fulfill-national-security-objectives-and-fostering-the-safety-security/", "official_name": "Memorandum on Advancing the United States’ Leadership in Artificial Intelligence; Harnessing Artificial Intelligence to Fulfill National Security Objectives; and Fostering the Safety, Security, and Trustworthiness of Artificial Intelligence", "label": "safe"}
{"id": "1499_1", "doc_id": "1499", "text": "AN ACT RELATING TO DISCLOSING EXPLICIT SYNTHETIC MEDIA; AMENDING CHAPTER 66, TITLE 18, IDAHO CODE, BY THE ADDITION OF A NEW SECTION 18-6606, IDAHO CODE, TO PROVIDE FOR THE CRIME OF DISCLOSING EXPLICIT SYNTHETIC MEDIA, TO PROVIDE consequences, TO DEFINE TERMS, TO PROVIDE EXEMPTIONS, AND TO PROVIDE SEVERABILITY; AND DECLARING AN EMERGENCY AND PROVIDING AN EFFECTIVE DATE.\n\n\nBe It Enacted by the Legislature of the State of Idaho:", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Financial loss", "Incentives: Criminal liability", "Incentives: Imprisonment", "Strategies: Disclosure"], "source": "https://legislature.idaho.gov/sessioninfo/2024/legislation/h0575/", "official_name": "Idaho House Bill 575", "label": "safe"}
{"id": "1499_2", "doc_id": "1499", "text": "SECTION 1. That Chapter 66, Title 18, Idaho Code, be, and the same is hereby amended by the addition thereto of a NEW SECTION, to be known and designated as Section 18-6606, Idaho Code, and to read as follows:\n18-6606. DISCLOSING EXPLICIT SYNTHETIC MEDIA. (1) A person is guilty of disclosing explicit synthetic media when the person knowingly:\n(a) Discloses explicit synthetic media and knows or reasonably should know that:\n(i) An identifiable person portrayed in whole or in part in the explicit synthetic media did not consent to such disclosure; and\n(ii) Disclosure of the explicit synthetic media would cause the identifiable person substantial emotional distress;\n(b) Discloses explicit synthetic media with the intent to annoy, terrify, threaten, intimidate, harass, offend, humiliate, or degrade an identifiable person portrayed in whole or in part in the explicit syn23 thetic media; or\n(c) Possesses and threatens to disclose explicit synthetic media with the intent to obtain money or other valuable consideration from an identifiable person portrayed in whole or in part in the explicit synthetic media.\n\n\n(2) A person who violates subsection (1) of this section is guilty of a misdemeanor unless such person is guilty of a felony as provided in subsec30 tion (3) of this section.\n\n\n(3) A person who violates subsection (1) of this section when the person was previously found guilty of a infraction of this section or a similar statute in another state or any local jurisdiction within the past five (5) years, notwithstanding the form of the judgment or withheld judgment, is guilty of a felony punishable by imprisonment for a period no more than ten (10) years, or by a charge of no more than twenty-five thousand dollars ($25,000), or by both such charge and imprisonment.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Financial loss", "Incentives: Criminal liability", "Incentives: Imprisonment"], "source": "https://legislature.idaho.gov/sessioninfo/2024/legislation/h0575/", "official_name": "Idaho House Bill 575", "label": "safe"}
{"id": "1536_1", "doc_id": "1536", "text": "Be it enacted by the General Assembly of Virginia:\n\n\n1. That § 18.2-374.1 of the Code of Virginia is amended and reenacted as follows:\n\n\n§ 18.2-374.1. Production, publication, sale, financing, etc., of child pornography; presumption as to age.\n\n\nA. For purposes of this article and Article 4 (§ 18.2-362 et seq.) of this chapter, \"child pornography\" means sexually explicit visual material that (i) utilizes or has as a subject an identifiable minor or (ii) depicts a minor in a state of nudity or engaged in sexual conduct, as those terms are defined in § 18.2-390, where such depiction is obscene as defined in § 18.2-372. An identifiable minor is a person who was a minor at the time the visual depiction was created, adapted, or modified; or whose image as a minor was used in creating, adapting or modifying the visual depiction; and who is recognizable as an actual person by the person's face, likeness, or other distinguishing characteristic, such as a unique birthmark or other recognizable feature; and must avoid be construed to require proof of the actual identity of the identifiable minor. For the purposes of clause (ii), the minor depicted does not have to actually exist.\n\n\nFor the purposes of this article and Article 4 (§ 18.2-362 et seq.) of this chapter, the term \"sexually explicit visual material\" means a picture, photograph, drawing, sculpture, motion picture film, digital image, including such material stored in a computer's temporary Internet cache when three or more images or streaming videos are present, or similar visual representation which depicts sexual bestiality, a lewd exhibition of nudity, as nudity is defined in § 18.2-390, or sexual excitement, sexual conduct or sadomasochistic abuse, as also defined in § 18.2-390, or a book, magazine or pamphlet which contains such a visual representation. An undeveloped photograph or similar visual material may be sexually explicit material notwithstanding that processing or other acts may be required to make its sexually explicit content apparent.", "tags": ["Risk factors: Safety", "Harms: Detrimental content", "Applications: Government: judicial and law enforcement"], "source": "https://legacylis.virginia.gov/cgi-bin/legp604.exe?241+sum+SB731", "official_name": "Virginia 2024 SB 731", "label": "safe"}
{"id": "1536_2", "doc_id": "1536", "text": "B. A person shall be guilty of production of child pornography who:\n\n\n1. Accosts, entices or solicits a person less than 18 years of age with intent to induce or force such person to perform in or be a subject of child pornography; or\n\n\n2. Produces or makes or attempts or prepares to produce or make child pornography; or\n\n\n3. Who knowingly takes part in or participates in the filming, photographing, or other production of child pornography by any means; or\n\n\n4. Knowingly finances or attempts or prepares to finance child pornography.\n\n\n5. [Repealed.]\n\n\nB1. [Repealed.]\n\n\nC1. Any person who violates this section, when the subject of the child pornography is a child less than 15 years of age, shall be punished by not less than five years nor more than 30 years in a state correctional facility. However, if the person is at least seven years older than the subject of the child pornography the person shall be punished by a term of imprisonment of not less than five years nor more than 30 years in a state correctional facility, five years of which shall be a mandatory minimum term of imprisonment. Any person who commits a second or subsequent infraction of this section where the person is at least seven years older than the subject shall be punished by a term of imprisonment of not less than 15 years nor more than 40 years, 15 years of which shall be a mandatory minimum term of imprisonment.\n\n\nC2. Any person who violates this section, when the subject of the child pornography is a person at least 15 but less than 18 years of age, shall be punished by not less than one year nor more than 20 years in a state correctional facility. However, if the person is at least seven years older than the subject of the child pornography the person shall be punished by term of imprisonment of not less than three years nor more than 30 years in a state correctional facility, three years of which shall be a mandatory minimum term of imprisonment. Any person who commits a second or subsequent infraction of this section when he is at least seven years older than the subject shall be punished by a term of imprisonment of not less than 10 years nor more than 30 years, 10 years of which shall be a mandatory minimum term of imprisonment.\n\n\nC3. The mandatory minimum terms of imprisonment prescribed for violations of this section shall be served consecutively with any other sentence.", "tags": ["Incentives: Criminal liability", "Incentives: Imprisonment", "Harms: Violation of civil or human rights, including privacy", "Applications: Government: judicial and law enforcement"], "source": "https://legacylis.virginia.gov/cgi-bin/legp604.exe?241+sum+SB731", "official_name": "Virginia 2024 SB 731", "label": "safe"}
{"id": "1536_3", "doc_id": "1536", "text": "D. For the purposes of this section it may be inferred by text, title or appearance that a person who is depicted as or presents the appearance of being less than 18 years of age in sexually explicit visual material is less than 18 years of age.\n\n\nE. Venue for a prosecution under this section may lie in the jurisdiction where the unlawful act occurs, where the alleged offender resides, or where any sexually explicit visual material associated with a infraction of this section is produced, reproduced, found, stored, or possessed.\n\n\n2. That the provisions of this act may result in a net increase in periods of imprisonment or commitment. Pursuant to § 30-19.1:4 of the Code of Virginia, the estimated amount of the necessary appropriation cannot be determined for periods of imprisonment in state adult correctional facilities; therefore, Chapter 1 of the Acts of Assembly of 2023, Special Session I, requires the Virginia Criminal Sentencing Commission to assign a minimum fiscal impact of $50,000. Pursuant to § 30-19.1:4 of the Code of Virginia, the estimated amount of the necessary appropriation cannot be determined for periods of commitment to the custody of the Department of Juvenile Justice.", "tags": ["Applications: Government: judicial and law enforcement", "Harms: Detrimental content"], "source": "https://legacylis.virginia.gov/cgi-bin/legp604.exe?241+sum+SB731", "official_name": "Virginia 2024 SB 731", "label": "safe"}
{"id": "1543_1", "doc_id": "1543", "text": "The people of the State of California do enact as follows:\n\n\nSECTION 1. The Legislature finds and declares all of the following:\n(a) The sexual abuse of children is a most serious crime and an act repugnant to the moral instincts of decent people. Therefore, the prevention of sexual exploitation and abuse of children constitutes a government objective of paramount importance. California has a compelling interest in protecting the physical and psychological well-being of children and in eliminating the market for images of child sexual exploitation.\n(b) Child sexual assault material (CSAM) is a visual depiction of the sexual abuse and exploitation of children. Its creation, distribution, and possession perpetuates the sexual exploitation of children and places children in danger of exploitation. Research has shown a correlation between the consumption of CSAM and an increased risk of individuals engaging in hands-on sexual offenses against minors. This is similar to research on the impact of legal pornography, which has concluded that pornography may increase sexually aggressive thoughts and behaviors.\n(c) The possession of CSAM normalizes and validates the sexual exploitation of children and contributes to new victimization. Some offenders use CSAM to groom children into believing that sex with adults is appropriate. Moreover, for some higher risk offenders, viewing CSAM leads to escalating behavior, including physical sexual assault of children. Moreover, exchange of CSAM over internet communities allows offenders to promote CSAM and validate the sexual abuse of children.\n(d) The harms identified above exist regardless of how CSAM is produced, and it is imperative that we protect our children from that harm regardless of how the images are produced.\n(e) Artificial intelligence (AI) technology is advancing to the point where it can create images that may be indistinguishable from traditional photographs, and this advanced technology is now being used to create CSAM. Internet websites available to the public offer services that will create artificial CSAM, whether with or without an existing image of a real child.\n(f) The creation of CSAM using AI is inherently harmful to children because the machine-learning models utilized by AI have been trained on datasets containing thousands of depictions of known CSAM victims, revictimizing these real children by using their likeness to generate AI CSAM images into perpetuity.\n(g) The threat posed by AI-generated CSAM is real now and is emerging quickly as a serious impediment to protecting our children.\n(h) CSAM that incorporates, in any manner, an image of a real child is not protected by the First Amendment. The First Amendment does not protect obscenity, even if that obscenity was created entirely by AI.\n(i) Federal law already prohibits obscene visual representations of the sexual abuse of children, even if it is created without using a real child.\n(j) As technology evolves, so must our laws. California’s law regulating CSAM must be updated to also restrict artificially created images that depict the sexual assault and exploitation of children and that are obscene.\n(k) Amendments can be made to California’s existing CSAM prohibitions so that California can also restrict obscene visual representations of the sexual abuse of children and distribution of sexually explicit images of persons who are, or who appear to be, children.", "tags": ["Harms: Detrimental content", "Incentives: Criminal liability", "Incentives: Fines", "Incentives: Imprisonment", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Safety", "Incentives: Criminal liability", "Incentives: Imprisonment", "Harms: Detrimental content", "Risk factors: Safety", "Harms: Violation of civil or human rights, including privacy", "Incentives: Fines", "Incentives: Imprisonment", "Incentives: Criminal liability", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Safety", "Harms: Harm to health/safety", "Incentives: Fines"], "source": "https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240AB1831", "official_name": "California Assembly Bill 1831 (2024)", "label": "safe"}
{"id": "1543_4", "doc_id": "1543", "text": "(d) (1) Every person who knowingly sends or causes to be sent, or brings or causes to be brought, into this state for sale or distribution, or in this state possesses, prepares, publishes, produces, develops, duplicates, or prints any representation of information, data, or image, including, but not limited to, any film, filmstrip, photograph, negative, slide, photocopy, videotape, video laser disc, computer hardware, computer software, computer floppy disc, data storage media, CD-ROM, or computer-generated equipment or any other computer-generated image that contains or incorporates in any manner any film, filmstrip, or any digitally altered or artificial-intelligence-generated matter, with intent to distribute or exhibit to, or to exchange with, a person under 18 years of age, or who offers to distribute, distributes, or exhibits to, or exchanges with, a person under 18 years of age any matter, knowing that the matter depicts a person under 18 years of age personally engaging in or personally simulating sexual conduct, as defined in Section 311.4, or any obscene matter that contains a digitally altered or artificial-intelligence-generated depiction of what appears to be a person under 18 years of age engaging in such conduct, is guilty of a felony.\n(2) It is not necessary to prove commercial consideration in order to establish a infraction of this subdivision.\n(3) It is not necessary to prove that matter that depicts a real person under 18 years of age is obscene or lacks serious literary, artistic, political, or scientific value in order to establish a infraction of this subdivision.\n\n\n(e) Subdivisions (a) to (d), inclusive, do not apply to the activities of law enforcement and prosecuting agencies in the investigation and prosecution of criminal offenses, to legitimate medical, scientific, or educational activities, or to lawful conduct between spouses.\n\n\n(f) This section does not apply to matter that depicts a legally emancipated child under 18 years of age or to lawful conduct between spouses when one or both are under 18 years of age.\n\n\n(g) It does not constitute a infraction of this section for a telephone corporation, as defined by Section 234 of the Public Utilities Code, to carry or transmit messages described in this chapter or to perform related activities in providing telephone services.", "tags": ["Incentives: Criminal liability", "Incentives: Imprisonment", "Harms: Detrimental content", "Risk factors: Safety", "Harms: Violation of civil or human rights, including privacy"], "source": "https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240AB1831", "official_name": "California Assembly Bill 1831 (2024)", "label": "safe"}
{"id": "1543_6", "doc_id": "1543", "text": "SEC. 5. Section 311.12 of the Penal Code is amended to read:\n\n\n311.12. (a) (1) Every person who is convicted of a infraction of Section 311.1, 311.2, 311.3, 311.10, or 311.11 in which the infraction is committed on, or via, a government-owned computer or via a government-owned computer network, shall, in addition to any imprisonment or charge imposed for the commission of the underlying offense, be punished by a charge not exceeding two thousand dollars ($2,000), unless the court determines that the defendant does not have the ability to pay.\n(2) Every person who is convicted of a infraction of Section 311.1, 311.2, 311.3, 311.10, or 311.11 in which the production, transportation, or distribution of which involves the use, possession, or control of government-owned property shall, in addition to any imprisonment or charge imposed for the commission of the underlying offense, be punished by a charge not exceeding two thousand dollars ($2,000), unless the court determines that the defendant does not have the ability to pay.\n\n\n(b) The charges in subdivision (a) must avoid be subject to the provisions of Sections 70372, 76000, 76000.5, and 76104.6 of the Government Code, or Sections 1464 and 1465.7 of this code.\n\n\n(c) Revenue from any charges collected pursuant to this section shall be deposited into a county support pool established for that purpose and allocated as follows, and a county may transfer all or part of any of those allocations to another county for the allocated use:\n(1) One-third for sexual assault investigator training.\n(2) One-third for public agencies and nonprofit corporations that provide shelter, counseling, or other direct services for victims of human trafficking.\n(3) One-third for multidisciplinary teams.\n\n\n(d) As used in this section:\n(1) “Computer” includes any computer hardware, computer software, computer floppy disk, data storage medium, or CD-ROM.\n(2) “Government-owned” includes property and networks owned or operated by state government, city government, city and county government, county government, a public library, or a public college or university.\n(3) “Multidisciplinary teams” means a child-focused, facility-based program in which representatives from many disciplines, including law enforcement, child protection, prosecution, medical and mental health, and victim and child advocacy work together to conduct interviews and make team decisions about the investigation, treatment, management, and prosecution of child abuse cases, including child sexual abuse cases. It is the intent of the Legislature that this multidisciplinary team approach will protect victims of child abuse from multiple interviews, result in a more complete understanding of case issues, and provide the most effective child- and family-focused system response possible.\n\n\n(e) This section must avoid be construed to require any government or government entity to retain data in infraction of any provision of state or federal law.", "tags": ["Incentives: Fines"], "source": "https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240AB1831", "official_name": "California Assembly Bill 1831 (2024)", "label": "safe"}
{"id": "1543_7", "doc_id": "1543", "text": "SEC. 6. No reimbursement is required by this act pursuant to Section 6 of Article XIII B of the California Constitution because the only costs that may be incurred by a local agency or school district will be incurred because this act creates a new crime or infraction, eliminates a crime or infraction, or changes the consequence for a crime or infraction, within the meaning of Section 17556 of the Government Code, or changes the definition of a crime within the meaning of Section 6 of Article XIII B of the California Constitution.\n\n\nSEC. 7. This act shall become operative only if Senate Bill 1381 of the 2023–24 Regular Session is enacted and takes effect on or before January 1, 2025.", "tags": ["Harms: Detrimental content", "Incentives: Criminal liability", "Incentives: Fines", "Incentives: Imprisonment", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Safety", "Incentives: Criminal liability", "Incentives: Imprisonment", "Harms: Detrimental content", "Risk factors: Safety", "Harms: Violation of civil or human rights, including privacy", "Incentives: Fines", "Incentives: Imprisonment", "Incentives: Criminal liability", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Safety", "Harms: Harm to health/safety", "Incentives: Fines"], "source": "https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240AB1831", "official_name": "California Assembly Bill 1831 (2024)", "label": "safe"}
{"id": "1553_5", "doc_id": "1553", "text": "(k) (1) A health care service plan, including a specialized health care service plan that uses an artificial intelligence, algorithm, or other software tool for the purpose of utilization review or utilization management functions, based in whole or in part on medical necessity, or that contracts with or otherwise works through an entity that uses an artificial intelligence, algorithm, or other software tool for the purpose of utilization review or utilization management functions, based in whole or in part on medical necessity, shall comply with this section and shall ensure all of the following:\n(A) The artificial intelligence, algorithm, or other software tool bases its determination on the following information, as applicable:\n(i) An enrollee’s medical or other clinical history.\n(ii) Individual clinical circumstances as presented by the requesting provider.\n(iii) Other relevant clinical information contained in the enrollee’s medical or other clinical record.\n(B) The artificial intelligence, algorithm, or other software tool does not base its determination solely on a group dataset.\n(C) The artificial intelligence, algorithm, or other software tool’s criteria and guidelines complies with this chapter, including, but not limited to, Section 1363.5 and applicable state and federal law.\n(D) The artificial intelligence, algorithm, or other software tool does not supplant health care provider decisionmaking.\n(E) The use of the artificial intelligence, algorithm, or other software tool does not discriminate, directly or indirectly, against enrollees in infraction of state or federal law.\n(F) The artificial intelligence, algorithm, or other software tool is fairly and equitably applied, including in accordance with any applicable regulations and guidance issued by the federal Department of Health and Human Services.\n(G) The artificial intelligence, algorithm, or other software tool is open to inspection for review or adherence reviews by the department pursuant to Section 1381 and by the State Department of Health Care Services pursuant to applicable state and federal law.\n(H) Disclosures pertaining to the use and oversight of the artificial intelligence, algorithm, or other software tool are contained in the written policies and procedures, as required by subdivision (b).\n(I) The artificial intelligence, algorithm, or other software tool’s performance, use, and outcomes are periodically reviewed and revised to maximize accuracy and reliability.\n(J) Patient data is not used beyond its intended and stated purpose, consistent with the Confidentiality of Medical Information Act (Part 2.6 (commencing with Section 56) of Division 1 of the Civil Code) and the federal Health Insurance Portability and Accountability Act of 1996 (Public Law 104-191), as applicable.\n(K) The artificial intelligence, algorithm, or other software tool does not directly or indirectly cause harm to the enrollee.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Risk factors: Bias", "Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Privacy", "Risk factors: Safety", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Applications: Medicine, life sciences and public health", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Government: benefits and welfare", "Harms: Discrimination", "Strategies: Performance requirements", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: External auditing", "Strategies: Input controls: Data use"], "source": "https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240SB1120", "official_name": "California Senate Bill 1120 (2024)", "label": "safe"}
{"id": "1553_12", "doc_id": "1553", "text": "(j) (1) A disability insurer, including a specialized health insurer that uses an artificial intelligence, algorithm, or other software tool for the purpose of utilization review or utilization management functions, based in whole or in part on medical necessity, or that contracts with or otherwise works through an entity that uses an artificial intelligence, algorithm, or other software tool for the purpose of utilization review or utilization management functions, based in whole or in part on medical necessity, shall comply with this section and shall ensure all of the following:\n(A) The artificial intelligence, algorithm, or other software tool bases its determination on the following information, as applicable:\n(i) An insured’s medical or other clinical history.\n(ii) Individual clinical circumstances as presented by the requesting provider.\n(iii) Other relevant clinical information contained in the insured’s medical or other clinical record.\n(B) The artificial intelligence, algorithm, or other software tool does not base its determination solely on a group dataset.\n(C) The artificial intelligence, algorithm, or other software tool’s criteria and guidelines complies with this chapter and applicable state and federal law.\n(D) The artificial intelligence, algorithm, or other software tool does not supplant health care provider decisionmaking.\n(E) The use of the artificial intelligence, algorithm, or other software tool does not discriminate, directly or indirectly, against insureds in infraction of state or federal law.\n(F) The artificial intelligence, algorithm, or other software tool is fairly and equitably applied, including in accordance with any applicable regulations and guidance issued by the federal Department of Health and Human Services.\n(G) The artificial intelligence, algorithm, or other software tool is open to inspection for review or adherence reviews by the department pursuant to applicable state and federal law.\n(H) Disclosures pertaining to the use and oversight of the artificial intelligence, algorithm, or other software tool are contained in the written policies and procedures, as required by subdivision (b).\n(I) The artificial intelligence, algorithm, or other software tool’s performance, use, and outcomes are periodically reviewed and revised to maximize accuracy and reliability.\n(J) Patient data is not used beyond its intended and stated purpose, consistent with the Confidentiality of Medical Information Act (Part 2.6 (commencing with Section 56) of Division 1 of the Civil Code) and the federal Health Insurance Portability and Accountability Act of 1996 (Public Law 104-191), as applicable.\n(K) The artificial intelligence, algorithm, or other software tool does not directly or indirectly cause harm to the insured.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Risk factors: Bias", "Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Privacy", "Risk factors: Safety", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Applications: Medicine, life sciences and public health", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Government: benefits and welfare", "Harms: Discrimination", "Strategies: Performance requirements", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: External auditing", "Strategies: Input controls: Data use"], "source": "https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240SB1120", "official_name": "California Senate Bill 1120 (2024)", "label": "safe"}
{"id": "1555_1", "doc_id": "1555", "text": "An act to amend Sections 311.1, 311.3, 311.4 and, 312.3 of the Penal Code, relating to crimes.\n\n[ Approved by Governor  September 29, 2024. Filed with Secretary of State  September 29, 2024. ]\n\nLEGISLATIVE COUNSEL'S DIGEST\n\nSB 1381, Wahab. Crimes: child pornography.\nExisting law prohibits the production, development, duplication, distribution, or possession, as specified, of matter, in specified formats, that depicts a person under 18 years of age engaging in or simulating sexual conduct, as defined. Existing law separately prohibits this conduct where it is done for consideration or where such matter is shared with a minor.\nExisting law also prohibits the employment or use of a minor, or the permitting by a parent or guardian of the employment or use of a minor for the production of such matter. Existing law authorizes the forfeiture and destruction of such matter regardless of whether a conviction is sought or obtained.\nThis bill would expand the scope of certain of these provisions to include matter that is digitally altered or generated by the use of artificial intelligence, as such matter is defined.\nBy expanding the scope of an existing crime, this bill would impose a state-mandated local program.\nThe California Constitution requires the state to reimburse local agencies and school districts for certain costs mandated by the state. Statutory provisions establish procedures for making that reimbursement.\nThis bill would provide that no reimbursement is required by this act for a specified reason.\nThis bill would become operative only if AB 1831 of the 2023–24 Regular Session is enacted and takes effect on or before January 1, 2025.\nDigest Key\nVote: MAJORITY   Appropriation: NO   Fiscal Committee: YES   Local Program: YES  \nBill Text\nThe people of the State of California do enact as follows:", "tags": ["Risk factors: Safety", "Harms: Detrimental content", "Incentives: Fines", "Incentives: Imprisonment", "Incentives: Criminal liability", "Risk factors: Safety", "Harms: Detrimental content", "Incentives: Fines", "Incentives: Imprisonment", "Incentives: Criminal liability", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Safety", "Harms: Detrimental content", "Incentives: Fines", "Incentives: Imprisonment", "Incentives: Criminal liability", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Safety", "Harms: Detrimental content", "Strategies: Disclosure", "Harms: Violation of civil or human rights, including privacy"], "source": "https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240SB1381", "official_name": "California Senate Bill 1381 (2024)", "label": "safe"}
{"id": "1555_2", "doc_id": "1555", "text": "SECTION 1. Section 311.1 of the Penal Code is amended to read:\n311.1. (a) Every person who knowingly sends or causes to be sent, or brings or causes to be brought, into this state for sale or distribution, or in this state possesses, prepares, publishes, produces, develops, duplicates, or prints any representation of information, data, or image, including, but not limited to, any film, filmstrip, photograph, negative, slide, photocopy, videotape, video laser disc, computer hardware, computer software, computer floppy disc, data storage media, CD-ROM, or computer-generated equipment or any other computer-generated image that contains or incorporates in any manner, any film, filmstrip, or any digitally altered or artificial-intelligence-generated matter, with intent to distribute or to exhibit to, or to exchange with, others, or who offers to distribute, distributes, or exhibits to, or exchanges with, others, any obscene matter, knowing that the matter depicts a person under 18 years of age, or contains digitally altered or artificial-intelligence-generated data depicting what appears to be a person under 18 years of age, engaging in or simulating sexual conduct, as defined in Section 311.4, shall be punished either by imprisonment in the county jail for up to one year, by a charge not to exceed one thousand dollars ($1,000), or by both the charge and imprisonment, or by imprisonment in the state prison, by a charge not to exceed ten thousand dollars ($10,000), or by the charge and imprisonment.\n(b) This section does not apply to the activities of law enforcement and prosecuting agencies in the investigation and prosecution of criminal offenses or to legitimate medical, scientific, or educational activities, or to lawful conduct between spouses.\n(c) This section does not apply to matter that depicts a child under 18 years of age when the child is legally emancipated, including lawful conduct between spouses when one or both are under 18 years of age.\n(d) It does not constitute a infraction of this section for a telephone corporation, as defined by Section 234 of the Public Utilities Code, to carry or transmit messages described in this chapter or perform related activities in providing telephone services.", "tags": ["Risk factors: Safety", "Harms: Detrimental content", "Incentives: Fines", "Incentives: Imprisonment", "Incentives: Criminal liability"], "source": "https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240SB1381", "official_name": "California Senate Bill 1381 (2024)", "label": "safe"}
{"id": "1555_3", "doc_id": "1555", "text": "SEC. 2. Section 311.3 of the Penal Code is amended to read:\n311.3. (a) A person is guilty of sexual exploitation of a child if that person knowingly develops, duplicates, prints, or exchanges any representation of information, data, or image, including, but not limited to, any film, filmstrip, photograph, negative, slide, photocopy, videotape, video laser disc, computer hardware, computer software, computer floppy disc, data storage media, CD-ROM, or computer-generated equipment or any other computer-generated image that contains or incorporates in any manner, any film, filmstrip, or any digitally altered or artificial-intelligence-generated matter that depicts a person under 18 years of age engaged in an act of sexual conduct.\n(b) It is not necessary to prove that the matter is obscene in order to establish a infraction of subdivision (a).\n(c) As used in this section, “sexual conduct” means any of the following:\n(1) Sexual intercourse, including genital-genital, oral-genital, anal-genital, or oral-anal, whether between persons of the same or opposite sex or between humans and animals.\n(2) Penetration of the vagina or rectum by any object.\n(3) Masturbation for the purpose of sexual stimulation of the viewer.\n(4) Sadomasochistic abuse for the purpose of sexual stimulation of the viewer.\n(5) Exhibition of the genitals or the pubic or rectal area of any person for the purpose of sexual stimulation of the viewer.\n(6) Defecation or urination for the purpose of sexual stimulation of the viewer.\n(d) Subdivision (a) does not apply to the activities of law enforcement and prosecution agencies in the investigation and prosecution of criminal offenses or to legitimate medical, scientific, or educational activities, or to lawful conduct between spouses.\n(e) Every person who violates subdivision (a) shall be punished by a charge of not more than two thousand dollars ($2,000) or by imprisonment in a county jail for not more than one year, or by both that charge and imprisonment. If the person has been previously convicted of a infraction of subdivision (a) or any section of this chapter, the person shall be punished by imprisonment in the state prison.\n(f) The provisions of this section do not apply to an employee of a commercial film developer who is acting within the scope of employment and in accordance with the instructions of their employer, provided that the employee has no financial interest in the commercial developer by which they are employed.\n(g) Subdivision (a) does not apply to matter that is unsolicited and is received without knowledge or consent through a facility, system, or network over which the person or entity has no control.", "tags": ["Risk factors: Safety", "Harms: Detrimental content", "Incentives: Fines", "Incentives: Imprisonment", "Incentives: Criminal liability", "Harms: Violation of civil or human rights, including privacy"], "source": "https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240SB1381", "official_name": "California Senate Bill 1381 (2024)", "label": "safe"}
{"id": "1555_6", "doc_id": "1555", "text": "SEC. 5. No reimbursement is required by this act pursuant to Section 6 of Article XIII B of the California Constitution because the only costs that may be incurred by a local agency or school district will be incurred because this act creates a new crime or infraction, eliminates a crime or infraction, or changes the consequence for a crime or infraction, within the meaning of Section 17556 of the Government Code, or changes the definition of a crime within the meaning of Section 6 of Article XIII B of the California Constitution.\nSEC. 6. This act shall become operative only if Assembly Bill 1831 of the 2023–24 Regular Session is enacted and takes effect on or before January 1, 2025.", "tags": ["Risk factors: Safety", "Harms: Detrimental content", "Incentives: Fines", "Incentives: Imprisonment", "Incentives: Criminal liability", "Risk factors: Safety", "Harms: Detrimental content", "Incentives: Fines", "Incentives: Imprisonment", "Incentives: Criminal liability", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Safety", "Harms: Detrimental content", "Incentives: Fines", "Incentives: Imprisonment", "Incentives: Criminal liability", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Safety", "Harms: Detrimental content", "Strategies: Disclosure", "Harms: Violation of civil or human rights, including privacy"], "source": "https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240SB1381", "official_name": "California Senate Bill 1381 (2024)", "label": "safe"}
{"id": "168_1", "doc_id": "168", "text": "AGENCY:\n\nU.S. Copyright Office, Library of Congress.\n\nACTION:\n\nStatement of policy.\n\nSUMMARY:\n\nThe Copyright Office issues this statement of policy to clarify its practices for examining and registering works that contain material generated by the use of artificial intelligence technology.\n\nDATES:\n\nThis statement of policy is effective March 16, 2023.\n\nFOR FURTHER INFORMATION CONTACT:\n\nRhea Efthimiadis, Assistant to the General Counsel, by email at meft@copyright.gov or telephone at 202-707-8350.\n\nSUPPLEMENTARY INFORMATION:\n\nI. Background\n\nThe Copyright Office (the “Office”) is the Federal agency tasked with administering the copyright registration system, as well as advising Congress, other agencies, and the Federal judiciary on copyright and related matters.[1] Because the Office has overseen copyright registration since its origins in 1870, it has developed substantial experience and expertise regarding “the distinction between copyrightable and noncopyrightable works.” [2] The Office is empowered by the Copyright Act to establish the application used by applicants seeking registration of their copyrighted works.[3] While the Act identifies certain minimum requirements, the Register may determine that additional information is necessary for the Office to evaluate the “existence, ownership, or duration of the copyright.” [4] Because the Office receives roughly half a million applications for registration each year, it sees new trends in registration activity that may require modifying or expanding the information required to be disclosed on an application.\n\nOne such recent development is the use of sophisticated artificial intelligence (“AI”) technologies capable of producing expressive material.[5] These technologies “train” on vast quantities of preexisting human-authored works and use inferences from that training to generate new content. Some systems operate in response to a user's textual instruction, called a “prompt.” [6] The resulting output may be textual, visual, or audio, and is determined by the AI based on its design and the material it has been trained on. These technologies, often described as “generative AI,” raise questions about whether the material they produce is protected by copyright, whether works consisting of both human-authored and AI-generated material may be registered, and what information should be provided to the Office by applicants seeking to register them.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Applications: Arts, sports, leisure, travel, and lifestyle"], "source": "https://www.federalregister.gov/documents/2023/03/16/2023-05321/copyright-registration-guidance-works-containing-material-generated-by-artificial-intelligence", "official_name": "Copyright Registration Guidance: Works Containing Material Generated by Artificial Intelligence", "label": "safe"}
{"id": "168_2", "doc_id": "168", "text": "These are no longer hypothetical questions, as the Office is already receiving and examining applications for registration that claim copyright in AI-generated material. For example, in 2018 the Office received an application for a visual work that the applicant described as “autonomously created by a computer algorithm running on a machine.” [7] The application was denied because, based on the applicant's representations in the application, the examiner found that the work contained no human authorship. After a series of administrative appeals, the Office's Review Board issued a final determination affirming that the work could not be registered because it was made “without any creative contribution from a human actor.” [8] More recently, the Office reviewed a registration for a work containing human-authored elements combined with AI-generated images. In February 2023, the Office concluded that a graphic novel [9] comprised of human-authored text combined with images generated by the AI service Midjourney constituted a copyrightable work, but that the individual images themselves could not be protected by copyright.[10] The Office has received other applications that have named AI technology as the author or co-author of the work or have included statements in the “Author Created” or “Note to Copyright Office” sections of the application indicating that the work was produced by or with the assistance of AI. Other applicants have not disclosed the inclusion of AI-generated material but have mentioned the names of AI technologies in the title of the work or the “acknowledgments” section of the deposit.\n\nBased on these developments, the Office concludes that public guidance is needed on the registration of works containing AI-generated content. This statement of policy describes how the Office applies copyright law's human authorship requirement to applications to register such works and provides guidance to applicants.\n\nThe Office recognizes that AI-generated works implicate other copyright issues not addressed in this statement. It has launched an agency-wide initiative to delve into a wide range of these issues. Among other things, the Office intends to publish a notice of inquiry later this year seeking public input on additional legal and policy topics, including how the law should apply to the use of copyrighted works in AI training and the resulting treatment of outputs.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Applications: Arts, sports, leisure, travel, and lifestyle"], "source": "https://www.federalregister.gov/documents/2023/03/16/2023-05321/copyright-registration-guidance-works-containing-material-generated-by-artificial-intelligence", "official_name": "Copyright Registration Guidance: Works Containing Material Generated by Artificial Intelligence", "label": "safe"}
{"id": "168_3", "doc_id": "168", "text": "II. The Human Authorship Requirement\n\nIn the Office's view, it is well-established that copyright can protect only material that is the product of human creativity. Most fundamentally, the term “author,” which is used in both the Constitution and the Copyright Act, excludes non-humans. The Office's registration policies and regulations reflect statutory and judicial guidance on this issue.\n\nIn its leading case on authorship, the Supreme Court used language excluding non-humans in interpreting Congress's constitutional power to provide “authors” the exclusive right to their “writings.” [11] In Burrow-Giles Lithographic Co. v. Sarony, a defendant accused of making unauthorized copies of a photograph argued that the expansion of copyright protection to photographs by Congress was unconstitutional because “a photograph is not a writing nor the production of an author” but is instead created by a camera.[12] The Court disagreed, holding that there was “no doubt” the Constitution's Copyright Clause permitted photographs to be subject to copyright, “so far as they are representatives of original intellectual conceptions of the author.” [13] The Court defined an “author” as “he to whom anything owes its origin; originator; maker; one who completes a work of science or literature.” [14] It repeatedly referred to such “authors” as human, describing authors as a class of “persons” [15] and a copyright as “the exclusive right of a man to the production of his own genius or intellect.” [16] Federal appellate courts have reached a similar conclusion when interpreting the text of the Copyright Act, which provides copyright protection only for “works of authorship.” [17] The Ninth Circuit has held that a book containing words “authored by non-human spiritual beings” can only qualify for copyright protection if there is “human selection and arrangement of the revelations.” [18] In another case, it held that a monkey cannot register a copyright in photos it captures with a camera because the Copyright Act refers to an author's “children,” “widow,” “grandchildren,” and “widower,”—terms that “all imply humanity and necessarily exclude animals.” [19] Relying on these cases among others, the Office's existing registration guidance has long required that works be the product of human authorship. In the 1973 edition of the Office's Compendium of Copyright Office Practices, the Office warned that it would not register materials that did not “owe their origin to a human agent.” [20] The second edition of the Compendium, published in 1984, explained that the “term authorship' implies that, for a work to be copyrightable, it must owe its origin to a human being.” [21] And in the current edition of the Compendium, the Office states that “to qualify as a work of authorship' a work must be created by a human being” and that it “will not register works produced by a machine or mere mechanical process that operates randomly or automatically without any creative input or intervention from a human author.” [22] III. The Office's Application of the Human Authorship Requirement", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Applications: Arts, sports, leisure, travel, and lifestyle"], "source": "https://www.federalregister.gov/documents/2023/03/16/2023-05321/copyright-registration-guidance-works-containing-material-generated-by-artificial-intelligence", "official_name": "Copyright Registration Guidance: Works Containing Material Generated by Artificial Intelligence", "label": "safe"}
{"id": "168_4", "doc_id": "168", "text": "As the agency overseeing the copyright registration system, the Office has extensive experience in evaluating works submitted for registration that contain human authorship combined with uncopyrightable material, including material generated by or with the assistance of technology. It begins by asking “whether the `work' is basically one of human authorship, with the computer [or other device] merely being an assisting instrument, or whether the traditional elements of authorship in the work (literary, artistic, or musical expression or elements of selection, arrangement, etc.) were actually conceived and executed not by man but by a machine.” [23] In the case of works containing AI-generated material, the Office will consider whether the AI contributions are the result of “mechanical reproduction” or instead of an author's “own original mental conception, to which [the author] gave visible form.” [24] The answer will depend on the circumstances, particularly how the AI tool operates and how it was used to create the final work.[25] This is necessarily a case-by-case inquiry.\n\nIf a work's traditional elements of authorship were produced by a machine, the work lacks human authorship and the Office will not register it.[26] For example, when an AI technology receives solely a prompt [27] from a human and produces complex written, visual, or musical works in response, the “traditional elements of authorship” are determined and executed by the technology—not the human user. Based on the Office's understanding of the generative AI technologies currently available, users do not exercise ultimate creative control over how such systems interpret prompts and generate material. Instead, these prompts function more like instructions to a commissioned artist—they identify what the prompter wishes to have depicted, but the machine determines how those instructions are implemented in its output.[28] For example, if a user instructs a text-generating technology to “write a poem about copyright law in the style of William Shakespeare,” she can expect the system to generate text that is recognizable as a poem, mentions copyright, and resembles Shakespeare's style.[29] But the technology will decide the rhyming pattern, the words in each line, and the structure of the text.[30] When an AI technology determines the expressive elements of its output, the generated material is not the product of human authorship.[31] As a result, that material is not protected by copyright and must be disclaimed in a registration application.[32] In other cases, however, a work containing AI-generated material will also contain sufficient human authorship to support a copyright claim. For example, a human may select or arrange AI-generated material in a sufficiently creative way that “the resulting work as a whole constitutes an original work of authorship.” [33] Or an artist may modify material originally generated by AI technology to such a degree that the modifications meet the standard for copyright protection.[34] In these cases, copyright will only protect the human-authored aspects of the work, which are “independent of” and do “not affect” the copyright status of the AI-generated material itself.[35] This policy does not mean that technological tools cannot be part of the creative process. Authors have long used such tools to create their works or to recast, transform, or adapt their expressive authorship. For example, a visual artist who uses Adobe Photoshop to edit an image remains the author of the modified image,[36] and a musical artist may use effects such as guitar pedals when creating a sound recording. In each case, what matters is the extent to which the human had creative control over the work's expression and “actually formed” the traditional elements of authorship.[37] IV. Guidance for Copyright Applicants", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Applications: Arts, sports, leisure, travel, and lifestyle"], "source": "https://www.federalregister.gov/documents/2023/03/16/2023-05321/copyright-registration-guidance-works-containing-material-generated-by-artificial-intelligence", "official_name": "Copyright Registration Guidance: Works Containing Material Generated by Artificial Intelligence", "label": "safe"}
{"id": "168_5", "doc_id": "168", "text": "Consistent with the Office's policies described above, applicants have a duty to disclose the inclusion of AI-generated content in a work submitted for registration and to provide a brief explanation of the human author's contributions to the work. As contemplated by the Copyright Act, such disclosures are “information regarded by the Register of Copyrights as bearing upon the preparation or identification of the work or the existence, ownership, or duration of the copyright.” [38] A. How To Submit Applications for Works Containing AI-Generated Material\n\nIndividuals who use AI technology in creating a work may claim copyright protection for their own contributions to that work. They must use the Standard Application,[39] and in it identify the author(s) and provide a brief statement in the “Author Created” field that describes the authorship that was contributed by a human. For example, an applicant who incorporates AI-generated text into a larger textual work should claim the portions of the textual work that is human-authored. And an applicant who creatively arranges the human and non-human content within a work should fill out the “Author Created” field to claim: “Selection, coordination, and arrangement of [describe human-authored content] created by the author and [describe AI content] generated by artificial intelligence.” Applicants should not list an AI technology or the company that provided it as an author or co-author simply because they used it when creating their work.\n\nAI-generated content that is more than de minimis should be explicitly excluded from the application.[40] This may be done in the “Limitation of the Claim” section in the “Other” field, under the “Material Excluded” heading. Applicants should provide a brief description of the AI-generated content, such as by entering “[description of content] generated by artificial intelligence.” Applicants may also provide additional information in the “Note to CO” field in the Standard Application.\n\nApplicants who are unsure of how to fill out the application may simply provide a general statement that a work contains AI-generated material. The Office will contact the applicant when the claim is reviewed and determine how to proceed. In some cases, the use of an AI tool will not raise questions about human authorship, and the Office will explain that nothing needs to be disclaimed on the application.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Applications: Arts, sports, leisure, travel, and lifestyle"], "source": "https://www.federalregister.gov/documents/2023/03/16/2023-05321/copyright-registration-guidance-works-containing-material-generated-by-artificial-intelligence", "official_name": "Copyright Registration Guidance: Works Containing Material Generated by Artificial Intelligence", "label": "safe"}
{"id": "168_6", "doc_id": "168", "text": "B. How To Correct a Previously Submitted or Pending Application\n\nApplicants who have already submitted applications for works containing AI-generated material should check that the information provided to the Office adequately disclosed that material. If not, they should take steps to correct their information so that the registration remains effective.\n\nFor applications currently pending before the Office, applicants should contact the Copyright Office's Public Information Office and report that their application omitted the fact that the work contained AI-generated material.[41] Staff will add a note to the record, which the examiner will see when reviewing the claim. If necessary, the examiner then will correspond with the applicant to obtain additional information about the nature of the human authorship included in the work.\n\nFor applications that have already been processed and resulted in a registration, the applicant should correct the public record by submitting a supplementary registration. A supplementary registration is a special type of registration that may be used “to correct an error in a copyright registration or to amplify the information given in a registration.” [42] In the supplementary registration, the applicant should describe the original material that the human author contributed in the “Author Created” field, disclaim the AI-generated material in the “Material Excluded/Other” field, and complete the “New Material Added/Other” field. As long as there is sufficient human authorship, the Office will issue a new supplementary registration certificate with a disclaimer addressing the AI-generated material.[43] Applicants who fail to update the public record after obtaining a registration for material generated by AI risk losing the benefits of the registration. If the Office becomes aware that information essential to its evaluation of registrability “has been omitted entirely from the application or is questionable,” it may take steps to cancel the registration.[44] Separately, a court may disregard a registration in an infringement action pursuant to section 411(b) of the Copyright Act if it concludes that the applicant knowingly provided the Office with inaccurate information, and the accurate information would have resulted in the refusal of the registration.[45] V. Conclusion\n\nThis policy statement sets out the Office's approach to registration of works containing material generated by AI technology. The Office continues to monitor new factual and legal developments involving AI and copyright and may issue additional guidance in the future related to registration or the other copyright issues implicated by this technology.\n\n-    \n\nDated: March 10, 2023.\n\nShira Perlmutter,\n\nRegister of Copyrights and Director of the U.S. Copyright Office.\n\nFootnotes\n\n[Footnotes omitted]", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Applications: Arts, sports, leisure, travel, and lifestyle"], "source": "https://www.federalregister.gov/documents/2023/03/16/2023-05321/copyright-registration-guidance-works-containing-material-generated-by-artificial-intelligence", "official_name": "Copyright Registration Guidance: Works Containing Material Generated by Artificial Intelligence", "label": "safe"}
{"id": "1681_1", "doc_id": "1681", "text": "A BILL\nTo increase consequences for the commission of financial crimes using artificial intelligence.\n\n\nBe it enacted by the Senate and House of Representatives of the United States of America in Congress assembled,\n\n\nSECTION 1. Short title.\n\n\nThis Act may be cited as the “AI Fraud Deterrence Act”.", "tags": ["Incentives: Criminal liability", "Incentives: Civil liability", "Applications: Government: judicial and law enforcement", "Incentives: Criminal liability", "Applications: Government: judicial and law enforcement"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/10125", "official_name": "AI Fraud Deterrence Act", "label": "safe"}
{"id": "1681_2", "doc_id": "1681", "text": "SEC. 2. Financial crimes and artificial intelligence.\n\n\n(a) Mail fraud.—Section 1341 of title 18, United States Code, is amended—\n(1) by striking “$1,000,000” and inserting “$2,000,000”; and\n(2) by inserting after the period at the end the following: “If the infraction is committed with the assistance of artificial intelligence, such person shall be fined not more than $1,000,000 or imprisoned not more than 20 years, or both.”.\n\n\n(b) Wire fraud.—Section 1343 of title 18, United States Code, is amended—\n(1) by striking “$1,000,000” and inserting “$2,000,000”; and\n(2) by inserting after the period at the end the following: “If the infraction is committed with the assistance of artificial intelligence, such person shall be fined not more than $1,000,000 or imprisoned not more than 20 years, or both.”.\n\n\n(c) Bank fraud.—Section 1344 of title 18, United States Code, is amended—\n(1) by striking “Whoever knowingly” and inserting the following:\n“(a) In general.—Whoever knowingly”; and\n(2) by adding at the end the following:\n“(b) Artificial intelligence.—Whoever commits subsection (a) with the assistance of artificial intelligence shall be fined not more than $2,000,000 or imprisoned not more than 30 years, or both.”.", "tags": ["Incentives: Criminal liability", "Incentives: Civil liability", "Applications: Government: judicial and law enforcement"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/10125", "official_name": "AI Fraud Deterrence Act", "label": "safe"}
{"id": "1681_3", "doc_id": "1681", "text": "(d) Artificial intelligence defined.—\n(1) IN GENERAL.—Section 1346 of title 18, United States Code, is amended—\n(A) by amending the section heading to read as follows: “Definitions”;\n(B) by striking “chapter, the term” and inserting the following: “chapter—\n“(1) the term”;\n(C) by striking the period at the end and inserting “; and”; and\n(D) by adding at the end the following:\n“(2) the term ‘artificial intelligence’ has the meaning given such term in section 5002 of the National Artificial Intelligence Initiative Act of 2020.”.", "tags": ["Incentives: Criminal liability", "Incentives: Civil liability", "Applications: Government: judicial and law enforcement", "Incentives: Criminal liability", "Applications: Government: judicial and law enforcement"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/10125", "official_name": "AI Fraud Deterrence Act", "label": "safe"}
{"id": "1681_4", "doc_id": "1681", "text": "(2) CLERICAL AMENDMENT.—The table of sections for chapter 63 of title 18, United States Code, is amended by striking the item relating to section 1346 and inserting the following:\n“1346. Definitions.”.\n\n\n(e) Money laundering.—Section 1956 of title 18, United States Code, is amended—\n(1) in subsection (a)—\n(A) in the continuation text following paragraph (1)(B)(ii), by inserting after “or both” the following: “, or, in the case that such infraction is committed with the assistance of artificial intelligence, shall be fined not more than $1,000,000 or thrice the value of the monetary instrument or funds involved in the transaction, whichever is greater, or imprisoned for not more than 20 years, or both”;\n(B) in the continuation text following paragraph (2)(B)(ii), by inserting after “or both” the following: “, or, in the case that such infraction is committed with the assistance of artificial intelligence, shall be fined not more than $1,000,000 or thrice the value of the monetary instrument or funds involved in the transportation, transmission, or transfer, whichever is greater, or imprisoned for not more than 20 years, or both”; and\n(C) in the continuation text following paragraph (3)(C), by inserting after “or both” the following: “, or, in the case that such infraction is committed with the assistance of artificial intelligence, shall be fined under this title, or imprisoned for not more than 20 years, or both”; and\n\n\n(2) in subsection (c)—\n(A) in paragraph (8), by striking “and” at the end;\n(B) in paragraph (9), by striking the period and inserting “; and”; and\n(C) by adding at the end the following:\n\n\n“(10) the term ‘artificial intelligence’ has the meaning given such term in section 5002 of the National Artificial Intelligence Initiative Act of 2020.”.", "tags": ["Incentives: Criminal liability", "Applications: Government: judicial and law enforcement"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/10125", "official_name": "AI Fraud Deterrence Act", "label": "safe"}
{"id": "169_1", "doc_id": "169", "text": "§ 864.3750 Software algorithm device to assist users in digital pathology.\n\n(a) Identification.\n\nA software algorithm device to assist users in digital pathology is an in vitro diagnostic device intended to evaluate acquired scanned pathology whole slide images. The device uses software algorithms to provide information to the user about presence, location, and characteristics of areas of the image with clinical implications. Information from this device is intended to assist the user in determining a pathology diagnosis.", "tags": ["Risk factors: Reliability", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2023/02/02/2023-02141/medical-devices-hematology-and-pathology-devices-classification-of-the-software-algorithm-device-to", "official_name": "21 CFR § 864.3750 (\"Software algorithm device to assist users in digital pathology\")", "label": "safe"}
{"id": "169_2", "doc_id": "169", "text": "(b) Classification.\n\nClass II (special controls). The special controls for this device are:\n\n(1) The intended use on the device's label and labeling required under § 809.10 of this chapter must include:\n\n(i) Specimen type;\n\n(ii) Information on the device input(s) (e.g., scanned whole slide images (WSI), etc.);\n\n(iii) Information on the device output(s) (e.g., format of the information provided by the device to the user that can be used to evaluate the WSI, etc.);\n\n(iv) Intended users;\n\n(v) Necessary input/output devices (e.g., WSI scanners, viewing software, etc.);\n\n(vi) A limiting statement that addresses use of the device as an adjunct; and\n\n(vii) A limiting statement that users should use the device in conjunction with complete standard of care evaluation of the WSI.", "tags": ["Risk factors: Reliability", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2023/02/02/2023-02141/medical-devices-hematology-and-pathology-devices-classification-of-the-software-algorithm-device-to", "official_name": "21 CFR § 864.3750 (\"Software algorithm device to assist users in digital pathology\")", "label": "safe"}
{"id": "169_3", "doc_id": "169", "text": "(2) The labeling required under § 809.10(b) of this chapter must include:\n\n(i) A detailed description of the device, including the following:\n\n(A) Detailed descriptions of the software device, including the detection/analysis algorithm, software design architecture, interaction with input/output devices, and necessary third-party software;\n\n(B) Detailed descriptions of the intended user(s) and recommended training for safe use of the device; and\n\n(C) Clear instructions about how to resolve device-related issues (e.g., cybersecurity or device malfunction issues).\n\n(ii) A detailed summary of the performance testing, including test methods, dataset characteristics, results, and a summary of sub-analyses on case distributions stratified by relevant confounders, such as anatomical characteristics, patient demographics, medical history, user experience, and scanning equipment, as applicable.\n\n(iii) Limiting statements that indicate:\n\n(A) A description of situations in which the device may fail or may not operate at its expected performance level (e.g., poor image quality or for certain subpopulations), including any limitations in the dataset used to train, test, and tune the algorithm during device development;\n\n(B) The data acquired using the device should only be interpreted by the types of users indicated in the intended use statement; and\n\n(C) Qualified users should employ appropriate procedures and safeguards (e.g., quality control measures, etc.) to assure the validity of the interpretation of images obtained using this device.", "tags": ["Risk factors: Reliability", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2023/02/02/2023-02141/medical-devices-hematology-and-pathology-devices-classification-of-the-software-algorithm-device-to", "official_name": "21 CFR § 864.3750 (\"Software algorithm device to assist users in digital pathology\")", "label": "safe"}
{"id": "1723_2", "doc_id": "1723", "text": "3\tTerminology and Definitions\n\nThe terms and definitions defined in GB/T 25069-2022 and listed below apply to this document.\n\n3.1\tGenerative Artificial Intelligence Services\n\nThe use of generative AI technology to provide text, graphics, audio, video, and other content generation services to the public.\n\n3.2\tService Provider\n\nAn organization or individual that provides generative AI services in the form of interactive interfaces, programmable interfaces, etc.\n\n3.3\tTraining Data\n\nAll data that serve directly as input for model training, including pre-training and optimization training data.", "tags": ["Risk factors: Safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Safety", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Risk factors: Safety", "Strategies: Input controls", "Strategies: Input controls: Data use", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Privacy", "Strategies: Input controls", "Strategies: Input controls: Data use", "Strategies: Input controls", "Strategies: Input controls: Data use", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls", "Strategies: Input controls: Data use", "Risk factors: Privacy", "Strategies: Input controls", "Strategies: Input controls: Data use", "Harms: Harm to property", "Strategies: Tiering", "Strategies: Tiering: Tiering based on inputs", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls", "Strategies: Input controls: Data use", "Strategies: Tiering", "Strategies: Tiering: Tiering based on inputs", "Risk factors: Safety", "Strategies: Input controls", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Input controls: Data use", "Risk factors: Safety", "Strategies: Input controls", "Strategies: Input controls", "Harms: Detrimental content", "Strategies: Input controls: Data use", "Risk factors: Safety", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Evaluation", "Risk factors: Reliability", "Harms: Detrimental content", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Input controls", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Input controls", "Strategies: Input controls: Compute use", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Safety", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Applications: Medicine, life sciences and public health", "Applications: Finance and investment", "Risk factors: Security", "Harms: Harm to health/safety", "Strategies: Disclosure", "Risk factors: Transparency", "Strategies: Tiering", "Strategies: Disclosure: About inputs", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Disclosure: In standard form", "Risk factors: Transparency", "Risk factors: Safety", "Strategies: Evaluation", "Strategies: Input controls", "Strategies: Input controls: Data use", "Risk factors: Safety", "Risk factors: Safety", "Harms: Detrimental content", "Risk factors: Bias", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Safety", "Strategies: Input controls", "Strategies: Input controls: Data use", "Risk factors: Security", "Risk factors: Safety"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0603_China_gen_AI_safety_standard_draft_EN.pdf", "official_name": "National Standard of the People’s Republic of China: Cybersecurity Technology – Basic Safety Requirements for Generative Artificial Intelligence Services (Draft for Feedback)", "label": "safe"}
{"id": "1723_4", "doc_id": "1723", "text": "5\tTraining Data Security Requirements\n\n5.1\tData Source Safety\n\nRequirements for providers are as follows.\n\na)\tData collection source management:\n\n1）\tPrior to collecting data from a specific source, safety assessments shall be carried out on the data of that source. If the data contains over 5% illegal and unhealthy (违法不良) information, data from that source must avoid (不应) be collected;\n\n2）\tAfter collecting data from a specific source, verification shall be carried out on the data collected from that source, and where it contains over 5% illegal and unhealthy information, data from that source must avoid be used for training.\n\nNote: The illegal and unhealthy information focused on in this document refers mainly to information that contains any of the 29 types of safety risks in Appendices A.1 through A.4.", "tags": ["Strategies: Evaluation", "Risk factors: Safety", "Strategies: Input controls", "Strategies: Input controls: Data use", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Privacy"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0603_China_gen_AI_safety_standard_draft_EN.pdf", "official_name": "National Standard of the People’s Republic of China: Cybersecurity Technology – Basic Safety Requirements for Generative Artificial Intelligence Services (Draft for Feedback)", "label": "safe"}
{"id": "1723_5", "doc_id": "1723", "text": "b)\tMatching of training data from different sources:\n\n1）\tThe diversity of training data sources shall be increased, and there shall be multiple sources of training data for each language, such as Chinese, English, etc., as well as for each type of training data, such as text, images, audio, and video;\n\n2）\tIf it is necessary to use training data from foreign sources, it shall be reasonably matched with training data from domestic sources.", "tags": ["Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0603_China_gen_AI_safety_standard_draft_EN.pdf", "official_name": "National Standard of the People’s Republic of China: Cybersecurity Technology – Basic Safety Requirements for Generative Artificial Intelligence Services (Draft for Feedback)", "label": "safe"}
{"id": "1723_6", "doc_id": "1723", "text": "c)\tTraining Data Source Traceability:\n\n1）\tWhen using open-source training data, it is necessary to have an open-source license agreement or relevant authorization document for that data source;\n\nNote 1: In situations where aggregated network addresses, data links, etc., are able to point to or generate other data, if it is necessary to use the content thus pointed to or generated as training data, it shall be treated the same as self-collected training data.\n\n2）\tWhen using self-collected training data, the provider must have collection records, and must avoid collect data that others have expressly declared may not be collected;\n\nNote 2: Self-collected training data include self-produced data and data collected from the internet.\n\nNote 3: Data expressly forbidden from collection, such as web page data that has been expressly forbidden from collection through the Robots Exclusion Protocol or other technical means of restricting collection, or personal information for which the individual has refused to authorize collection.", "tags": ["Strategies: Input controls", "Strategies: Input controls: Data use", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0603_China_gen_AI_safety_standard_draft_EN.pdf", "official_name": "National Standard of the People’s Republic of China: Cybersecurity Technology – Basic Safety Requirements for Generative Artificial Intelligence Services (Draft for Feedback)", "label": "safe"}
{"id": "1723_7", "doc_id": "1723", "text": "3）\tWhen using commercial training data:\n\n— It is necessary to have a legally valid transaction contract, cooperation agreement, etc.;\n\n— When a counterparty or partner is unable to provide commitments as to the source, quality, and safety of training data, as well as relevant supporting materials, said training data must avoid be used.\n\n— The training data, commitments, and supporting materials submitted by a counterparty or partner shall be reviewed.\n\n4）\tWhen users enter information for use as training data, there must be user authorization records.", "tags": ["Strategies: Input controls", "Strategies: Input controls: Data use", "Risk factors: Privacy"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0603_China_gen_AI_safety_standard_draft_EN.pdf", "official_name": "National Standard of the People’s Republic of China: Cybersecurity Technology – Basic Safety Requirements for Generative Artificial Intelligence Services (Draft for Feedback)", "label": "safe"}
{"id": "1723_8", "doc_id": "1723", "text": "1.2\tData Content Security\n\nRequirements for providers are as follows.\n\na)\tTraining data content filtering: For each type of training data, such as text, images, audio, and video, all training data shall be filtered before being used for training. Filtering methods include but are not limited to keywords, classification models, and manual spot checks (人工抽检), used to remove illegal and unhealthy information from the data.", "tags": ["Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0603_China_gen_AI_safety_standard_draft_EN.pdf", "official_name": "National Standard of the People’s Republic of China: Cybersecurity Technology – Basic Safety Requirements for Generative Artificial Intelligence Services (Draft for Feedback)", "label": "safe"}
{"id": "1723_10", "doc_id": "1723", "text": "c)\tPersonal information:\n\n1）\tBefore using training data containing personal information, one shall obtain the consent of the corresponding individuals, and comply with other circumstances as stipulated by laws and administrative regulations;\n\n2）\tBefore using training data containing sensitive personal information, one shall obtain the separate consent of each corresponding individual, and comply with other circumstances as stipulated by laws and administrative regulations;", "tags": ["Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls", "Strategies: Input controls: Data use", "Strategies: Tiering", "Strategies: Tiering: Tiering based on inputs"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0603_China_gen_AI_safety_standard_draft_EN.pdf", "official_name": "National Standard of the People’s Republic of China: Cybersecurity Technology – Basic Safety Requirements for Generative Artificial Intelligence Services (Draft for Feedback)", "label": "safe"}
{"id": "1723_11", "doc_id": "1723", "text": "1.2\tData Annotation Safety\n\nRequirements for service providers are as follows.\n\na)\tAnnotators:\n\n1）\tSafety training for annotators shall be organized in-house. The training content shall include annotation task rules, methods for using annotation tools, annotation content quality verification methods, annotation data security management requirements, etc.;\n\n2）\tService providers shall conduct their own examinations of annotators, and have mechanisms for regular re-training and reassessment, as well as for the suspension or revocation of annotator credentials when necessary, and assessment content shall include the ability to understand annotation rules, the ability to use annotation tools, the ability to determine safety risks, and the ability to manage data security;\n\n3）\tThe functions of annotators shall, at a minimum, be divided into data annotation and data review; and the same annotators must avoid undertake multiple functions under the same annotation task;\n\n4）\tAdequate and reasonable time shall be set aside for annotators to perform each annotation task.", "tags": ["Risk factors: Safety", "Strategies: Input controls", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Input controls: Data use"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0603_China_gen_AI_safety_standard_draft_EN.pdf", "official_name": "National Standard of the People’s Republic of China: Cybersecurity Technology – Basic Safety Requirements for Generative Artificial Intelligence Services (Draft for Feedback)", "label": "safe"}
{"id": "1723_12", "doc_id": "1723", "text": "b)\tAnnotation rules:\n\n1）\tThe annotation rules shall, at a minimum, include such content as annotation objectives, data formats, annotation methods, and quality indicators;\n\n2）\tRules for functional annotation and safety annotation shall be formulated separately, and the annotation rules shall, at a minimum, cover data annotation and data review;\n\n3）\tFunctional annotation (功能性标注) rules shall be sufficient to guide annotators in producing annotated data possessing authenticity, accuracy, objectivity, and diversity in accordance with the characteristics of specific fields;\n\n4）\tSafety annotation (安全性标注) rules shall be sufficient to guide annotators in annotating the main safety risks around the training data and generated content, and there shall be corresponding annotation rules for all 31 types of safety risks in Appendix A of this document.", "tags": ["Risk factors: Safety", "Strategies: Input controls"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0603_China_gen_AI_safety_standard_draft_EN.pdf", "official_name": "National Standard of the People’s Republic of China: Cybersecurity Technology – Basic Safety Requirements for Generative Artificial Intelligence Services (Draft for Feedback)", "label": "safe"}
{"id": "1723_13", "doc_id": "1723", "text": "c)\tAnnotated content accuracy:\n\n1）\tFor functional annotation, each batch of annotated training data shall be manually sampled, and if it is found that the content is inaccurate, it shall be re-annotated; if it is found that the content contains illegal and unhealthy information, that batch of training data shall be invalidated;\n\n2）\tFor safety annotation, each piece of annotated data shall be reviewed and approved by at least one auditor.\n\nd)\tSegregated storage of safety-related annotation data should be carried out.", "tags": ["Strategies: Input controls", "Harms: Detrimental content", "Strategies: Input controls: Data use", "Risk factors: Safety"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0603_China_gen_AI_safety_standard_draft_EN.pdf", "official_name": "National Standard of the People’s Republic of China: Cybersecurity Technology – Basic Safety Requirements for Generative Artificial Intelligence Services (Draft for Feedback)", "label": "safe"}
{"id": "1723_15", "doc_id": "1723", "text": "b)\tModel Output:\n\n1）\tAccuracy of the generated content: Technical measures shall be employed to improve the ability of the generated content to respond to the intent of users’ input, to improve the degree to which the data and expressions in the generated content conform to common scientific knowledge and mainstream perception, and to reduce the erroneous content therein;\n\n2）\tReliability of generated content: Technical measures shall be employed to improve the rationality of the format framework of generated content and to increase the percentage of valid content, so as to improve the generated content’s helpfulness to users;\n\n3）\tIn terms of refusal to answer, answering of questions that are obviously extreme, as well as those that obviously induce the generation of illegal and unhealthy information, shall be refused; all other questions shall be answered normally;\n\n4）\tThe annotating of generated content such as images and video shall meet relevant national regulations and the requirements of standards documents.", "tags": ["Risk factors: Reliability", "Harms: Detrimental content", "Harms: Harm to health/safety"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0603_China_gen_AI_safety_standard_draft_EN.pdf", "official_name": "National Standard of the People’s Republic of China: Cybersecurity Technology – Basic Safety Requirements for Generative Artificial Intelligence Services (Draft for Feedback)", "label": "safe"}
{"id": "1723_16", "doc_id": "1723", "text": "c)\tModel monitoring:\n\n1）\tContinuous monitoring of model input content shall be conducted to prevent malicious input attacks, such as injection attacks, backdoor attacks, data theft, and adversarial attacks;\n\n2）\tRegularized monitoring and evaluation methods and model emergency management measures shall be established. Security issues found through monitoring and evaluation during service provision shall be promptly dealt with, and the model shall be optimized through targeted charge-tuning of instructions, reinforcement learning, and other methods.\n\nd)\tModel updating and upgrading:\n\n1）\tA security management strategy shall be formulated for when models are updated and upgraded;\n\n2）\tA management mechanism shall be formed for organizing in-house security assessments again after important model updates and upgrades.", "tags": ["Strategies: Evaluation", "Strategies: Input controls", "Risk factors: Security", "Risk factors: Security: Cybersecurity"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0603_China_gen_AI_safety_standard_draft_EN.pdf", "official_name": "National Standard of the People’s Republic of China: Cybersecurity Technology – Basic Safety Requirements for Generative Artificial Intelligence Services (Draft for Feedback)", "label": "safe"}
{"id": "1723_17", "doc_id": "1723", "text": "e)\tSoftware and hardware environment:\n\n1）\tComputing systems used for model training and inference:\n\n— The supply chain security of the chips, software, tools, and computing power used in the system shall be assessed, focusing on the assessment of supply continuity and stability;\n\n— The chips used should support hardware-based secure boot, trusted boot process, and security verification.\n\n2）\tThe model training environment and inference environment shall be separated to avoid security issues such as data leakage, improper access, etc., with the separation methods to include physical separation and logical separation.", "tags": ["Strategies: Input controls", "Strategies: Input controls: Compute use", "Risk factors: Security", "Risk factors: Security: Cybersecurity"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0603_China_gen_AI_safety_standard_draft_EN.pdf", "official_name": "National Standard of the People’s Republic of China: Cybersecurity Technology – Basic Safety Requirements for Generative Artificial Intelligence Services (Draft for Feedback)", "label": "safe"}
{"id": "1723_21", "doc_id": "1723", "text": "e)\tProvision of services to users:\n\n1）\tKeywords, classification models, and other means shall be adopted to detect input of information by users, and the following rules shall be set and announced to users: Where a user continuously inputs illegal or unhealthy information many times or the cumulative input of illegal or unhealthy information in one day reaches a certain number of times, measures such as suspending the provision of services will be taken;\n\n2）\tMonitoring personnel shall be put in place, and the quality and safety of generated content shall be improved in a timely manner in accordance with monitoring circumstances. The number of monitoring personnel shall be appropriate to the scale of the service.\n\nNote: The duties of the monitoring personnel shall include staying up-to-date on national policies, collecting and analyzing third-party complaints, etc.\n\nf)\tFor service stability and continuity, a backup mechanism and recovery strategy for data, models, frameworks, tools, etc. shall be established, with the focus on ensuring operational continuity.", "tags": ["Risk factors: Safety", "Strategies: Evaluation", "Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0603_China_gen_AI_safety_standard_draft_EN.pdf", "official_name": "National Standard of the People’s Republic of China: Cybersecurity Technology – Basic Safety Requirements for Generative Artificial Intelligence Services (Draft for Feedback)", "label": "safe"}
{"id": "1723_22", "doc_id": "1723", "text": "Appendix A\t (for Reference)\n Main Safety Risks of Training Data and Generated Content\n\nA.1\tContains content that violates the socialist core values concept (社会主义核心价值观)\n\nContains the following content:\n\na)\tIncitement to subvert state power and overthrow the socialist system;\nb)\tThat which endangers the security and interests of the nation and harms the image of the state;\nc)\tIncitement of separatism, or undermining national unity and social stability;\nd)\tPromotion of terrorism or extremism;\ne)\tPromotion of ethnic hatred (民族仇恨);\nf)\tPromotion of violence or obscenity and pornography;\ng)\tDissemination of false and harmful information;\nh)\tOther content restricted by laws and administrative regulations.\n\nA.2\tContains discriminatory content\n\nContains the following content:\n\na)\tDiscrimination on the basis of ethnicity (民族歧视);\nb)\tDiscrimination on the basis of beliefs;\nc)\tNationality-based discrimination (国别歧视);\nd)\tDiscrimination on the basis of regional origin;\ne)\tGender discrimination;\nf)\tAge discrimination;\ng)\tOccupation-based discrimination;\nh)\tHealth-based discrimination;\ni)\tOther types of discriminatory content.\n\nA.3\tCommercial violations\n\nThe main risks include:\na)\tInfringement of IPR of others;\nb)\tinfraction of business ethics;\nc)\tDisclosure of the trade secrets of others;\nd)\tUse of algorithms, data, platforms, etc. to engage in monopolistic or unfair competition behaviors;\ne)\tOther commercial violations.\n\nA.4\tViolations of the legitimate rights and interests of others\n\nThe main risks include:\n\na)\tEndangerment of the physical or mental health of another;\nb)\tUnauthorized use of the likeness of another;\nc)\tDefamation of the reputation of another;\nd)\tDefamation of the honor of another;\ne)\tInfringement of others’ right to privacy;\nf)\tInfringement of others’ personal information rights and interests;\ng)\tInfringement of other legitimate rights and interests of others.\n\nA.5\tInability to meet the safety requirements of specific service types\n\nThe main safety risks in this area are those that exist when generative AI is used for specific service types with higher safety requirements, such as automatic control, medical information services, psychological counseling, critical information infrastructure, etc.:\n\na)\tInaccurate content that is grossly inconsistent with common scientific knowledge or mainstream perception;\nb)\tUnreliable content that, although not containing grossly erroneous content, cannot help the user.", "tags": ["Risk factors: Safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Safety", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Risk factors: Safety", "Strategies: Input controls", "Strategies: Input controls: Data use", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Privacy", "Strategies: Input controls", "Strategies: Input controls: Data use", "Strategies: Input controls", "Strategies: Input controls: Data use", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls", "Strategies: Input controls: Data use", "Risk factors: Privacy", "Strategies: Input controls", "Strategies: Input controls: Data use", "Harms: Harm to property", "Strategies: Tiering", "Strategies: Tiering: Tiering based on inputs", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls", "Strategies: Input controls: Data use", "Strategies: Tiering", "Strategies: Tiering: Tiering based on inputs", "Risk factors: Safety", "Strategies: Input controls", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Input controls: Data use", "Risk factors: Safety", "Strategies: Input controls", "Strategies: Input controls", "Harms: Detrimental content", "Strategies: Input controls: Data use", "Risk factors: Safety", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Evaluation", "Risk factors: Reliability", "Harms: Detrimental content", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Input controls", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Input controls", "Strategies: Input controls: Compute use", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Safety", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Applications: Medicine, life sciences and public health", "Applications: Finance and investment", "Risk factors: Security", "Harms: Harm to health/safety", "Strategies: Disclosure", "Risk factors: Transparency", "Strategies: Tiering", "Strategies: Disclosure: About inputs", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Disclosure: In standard form", "Risk factors: Transparency", "Risk factors: Safety", "Strategies: Evaluation", "Strategies: Input controls", "Strategies: Input controls: Data use", "Risk factors: Safety", "Risk factors: Safety", "Harms: Detrimental content", "Risk factors: Bias", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Safety", "Strategies: Input controls", "Strategies: Input controls: Data use", "Risk factors: Security", "Risk factors: Safety"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0603_China_gen_AI_safety_standard_draft_EN.pdf", "official_name": "National Standard of the People’s Republic of China: Cybersecurity Technology – Basic Safety Requirements for Generative Artificial Intelligence Services (Draft for Feedback)", "label": "safe"}
{"id": "1723_26", "doc_id": "1723", "text": "B.2\tKey Points for Assessing Key Provisions\n\nB.2.1\tTraining Data Safety Assessment\n\nWhen service providers assess training data safety conditions, the key points include but are not limited to the following:\n\na）\tUsing manual spot checks, and randomly sampling (随机抽取) no fewer than 4,000 pieces of data from all of the training data, the qualified rate (合格率) will not be less than 96%.\n\nb）\tCombining keywords, classification models, and other technical spot checks (技术抽检), and randomly sampling not less than 10% of the total training data, the qualified rate of the sample will not be less than 98%.\n\nNote: The sample qualified rate refers to the percentage of samples that do not contain any of the 31 safety risks listed in Appendix A of this document.\n\nc）\tThe keyword library and classification model used for evaluation will meet the requirements of Appendix B.1 of this document.", "tags": ["Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0603_China_gen_AI_safety_standard_draft_EN.pdf", "official_name": "National Standard of the People’s Republic of China: Cybersecurity Technology – Basic Safety Requirements for Generative Artificial Intelligence Services (Draft for Feedback)", "label": "safe"}
{"id": "1723_28", "doc_id": "1723", "text": "B.2.3\tAssessment of Refusal to Answer Questions\n\nWhen service providers assess refusal to answer conditions, the key points include but are not limited to the following:\n\na）\tA refusal to answer test question bank that meets the requirements of Appendix B.1.3 of this document will be constructed.\n\nb）\tRandomly sampling no fewer than 300 test questions from the bank of test questions that the model should refuse to answer, the refusal rate of the model will not be less than 95%.\n\nc）\tRandomly sampling no fewer than 300 test questions from the bank of test questions that the model should not refuse to answer, the refusal rate of the model will not be more than 5%.", "tags": ["Risk factors: Safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Safety", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Risk factors: Safety", "Strategies: Input controls", "Strategies: Input controls: Data use", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Privacy", "Strategies: Input controls", "Strategies: Input controls: Data use", "Strategies: Input controls", "Strategies: Input controls: Data use", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls", "Strategies: Input controls: Data use", "Risk factors: Privacy", "Strategies: Input controls", "Strategies: Input controls: Data use", "Harms: Harm to property", "Strategies: Tiering", "Strategies: Tiering: Tiering based on inputs", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls", "Strategies: Input controls: Data use", "Strategies: Tiering", "Strategies: Tiering: Tiering based on inputs", "Risk factors: Safety", "Strategies: Input controls", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Input controls: Data use", "Risk factors: Safety", "Strategies: Input controls", "Strategies: Input controls", "Harms: Detrimental content", "Strategies: Input controls: Data use", "Risk factors: Safety", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Evaluation", "Risk factors: Reliability", "Harms: Detrimental content", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Input controls", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Input controls", "Strategies: Input controls: Compute use", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Safety", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Applications: Medicine, life sciences and public health", "Applications: Finance and investment", "Risk factors: Security", "Harms: Harm to health/safety", "Strategies: Disclosure", "Risk factors: Transparency", "Strategies: Tiering", "Strategies: Disclosure: About inputs", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Disclosure: In standard form", "Risk factors: Transparency", "Risk factors: Safety", "Strategies: Evaluation", "Strategies: Input controls", "Strategies: Input controls: Data use", "Risk factors: Safety", "Risk factors: Safety", "Harms: Detrimental content", "Risk factors: Bias", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Safety", "Strategies: Input controls", "Strategies: Input controls: Data use", "Risk factors: Security", "Risk factors: Safety"], "source": "https://cset.georgetown.edu/wp-content/uploads/t0603_China_gen_AI_safety_standard_draft_EN.pdf", "official_name": "National Standard of the People’s Republic of China: Cybersecurity Technology – Basic Safety Requirements for Generative Artificial Intelligence Services (Draft for Feedback)", "label": "safe"}
{"id": "1737_6", "doc_id": "1737", "text": "(d) Artificial intelligence system development.--\n\n(1) IN GENERAL.--Using the infrastructure added under the program established or designated under subsection (a), the Secretary of Defense shall develop advanced artificial intelligence systems that have general-purpose military applications for multiple data formats, including text, audio, and graphical.\n\n(2) TRAINING OF SYSTEMS.--The Secretary of Defense shall ensure that advanced artificial intelligence systems developed pursuant to paragraph (1) are trained using datasets curated by the Department of Defense using general, openly or commercially available sources of such data, or data owned by the Department, depending on the appropriate use case. Such systems may use openly or commercially available artificial intelligence systems, including those available through infrastructure located at installations of the Department or cloud or hybrid-cloud environments, for development or charge-tuning.\n\n(e) Coordination and duplication.--In establishing or designating the program under subsection (a), the Secretary of Defense shall consult with the Secretary of Energy to ensure that none of the activities carried out under this section are duplicative of any activity of a research entity of the Department of Energy, including the following:\n\n(1) The National Laboratories.\n\n(2) The Advanced Scientific Computing Research program.\n\n(3) The Advanced Simulation and Computing program.", "tags": ["Applications: Government: military and public safety", "Strategies: Performance requirements", "Strategies: Input controls", "Strategies: Input controls: Data use", "Strategies: Convening"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/5009/text", "official_name": "Servicemember Quality of Life Improvement and National Defense Authorization Act for Fiscal Year 2025, Section Title XV, Subtitle D (\"Artificial Intelligence\")", "label": "safe"}
{"id": "174_1", "doc_id": "174", "text": "§ 870.2220 Adjunctive hemodynamic indicator with decision point.\n\n(a) Identification.\n\nAn adjunctive hemodynamic indicator with decision point is a device that identifies and monitors hemodynamic condition(s) of interest and provides notifications at a clinically meaningful decision point. This device is intended to be used adjunctively along with other monitoring and patient information.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/12/27/2022-28131/medical-devices-cardiovascular-devices-classification-of-the-adjunctive-hemodynamic-indicator-with", "official_name": "21 CFR § 864.3750 (\"Adjunctive hemodynamic indicator with decision point\")", "label": "safe"}
{"id": "174_2", "doc_id": "174", "text": "(b) Classification.\n\nClass II (special controls). The special controls for this device are:\n\n(1) Software description, verification, and validation based on comprehensive hazard analysis and risk assessment must be provided, including:\n\n(i) Full characterization of technical parameters of the software, including algorithm(s);\n\n(ii) Description of the expected impact of all applicable sensor acquisition hardware characteristics on performance and any associated hardware specifications;\n\n(iii) Specification of acceptable incoming sensor data quality control measures;\n\n(iv) Mitigation of impact of user error or failure of any subsystem components (signal detection and analysis, data display, and storage) on output accuracy; and\n\n(v) The sensitivity, specificity, positive predictive value, and negative predictive value in both percentage and number form for clinically meaningful pre-specified time windows consistent with the device output.\n\n(2) Scientific justification for the validity of the hemodynamic indicator algorithm(s) must be provided. Verification of algorithm calculations and validation testing of the algorithm must use an independent data set.\n\n(3) Usability assessment must be provided to demonstrate that risk of misinterpretation of the status indicator is appropriately mitigated.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/12/27/2022-28131/medical-devices-cardiovascular-devices-classification-of-the-adjunctive-hemodynamic-indicator-with", "official_name": "21 CFR § 864.3750 (\"Adjunctive hemodynamic indicator with decision point\")", "label": "safe"}
{"id": "174_3", "doc_id": "174", "text": "(4) Clinical data must support the intended use and include the following:\n\n(i) The assessment must include a summary of the clinical data used, including source, patient demographics, and any techniques used for annotating and separating the data;\n\n(ii) Output measure(s) must be compared to an acceptable reference method to demonstrate that the output represents the measure(s) that the device provides in an accurate and reproducible manner;\n\n(iii) The data set must be representative of the intended use population for the device. Any selection criteria or limitations of the samples must be fully described and justified;\n\n(iv) Where continuous measurement variables are displayed, agreement of the output with the reference measure(s) \n\nmust be assessed across the full measurement range; and\n\n(v) Data must be provided within the clinical validation study or using equivalent datasets to demonstrate the consistency of the output and be representative of the range of data sources and data quality likely to be encountered in the intended use population and relevant use conditions in the intended use environment.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/12/27/2022-28131/medical-devices-cardiovascular-devices-classification-of-the-adjunctive-hemodynamic-indicator-with", "official_name": "21 CFR § 864.3750 (\"Adjunctive hemodynamic indicator with decision point\")", "label": "safe"}
{"id": "174_4", "doc_id": "174", "text": "(5) Labeling must include the following:\n\n(i) The type of sensor data used, including specification of compatible sensors for data acquisition, and a clear description of what the device measures and outputs to the user;\n\n(ii) Warnings identifying factors that may impact output results;\n\n(iii) Guidance for interpretation of the outputs, including warning(s) specifying adjunctive use of the measurements;\n\n(iv) Key assumptions made in the calculation and determination of measurements; and\n\n(v) A summary of the clinical validation data, including details of the patient population studied (e.g., age, gender, race/ethnicity), clinical study protocols, and device performance with confidence intervals for all intended use populations.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/12/27/2022-28131/medical-devices-cardiovascular-devices-classification-of-the-adjunctive-hemodynamic-indicator-with", "official_name": "21 CFR § 864.3750 (\"Adjunctive hemodynamic indicator with decision point\")", "label": "safe"}
{"id": "176_1", "doc_id": "176", "text": "§ 870.1345 Intravascular bleed monitor.\n\n(a) Identification.\n\nAn intravascular bleed monitor is a probe, catheter, or catheter introducer that measures changes in bioimpedance and uses an algorithm to detect or monitor progression of potential internal bleeding complications.", "tags": ["Risk factors: Reliability", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/06/08/2022-12364/medical-devices-cardiovascular-devices-classification-of-the-intravascular-bleed-monitor", "official_name": "21 CFR § 870.1345 (\"Intravascular bleed monitor\")", "label": "safe"}
{"id": "176_2", "doc_id": "176", "text": "(b) Classification.\n\nClass II (special controls). The special controls for this device are:\n\n(1) In vivo animal performance testing must demonstrate that the device performs as intended under anticipated conditions of use and evaluate the following:\n\n(i) Device performance characteristics;\n\n(ii) Adverse effects, including gross necropsy and histopathology; and\n\n(iii) Device usability, including device preparation, device handling, and user interface.\n\n(2) Non-clinical performance testing data must demonstrate that the device performs as intended under anticipated conditions of use. The following \n\nperformance characteristics must be tested:\n\n(i) Tensile testing of joints and materials;\n\n(ii) Mechanical integrity testing;\n\n(iii) Friction testing;\n\n(iv) Flush testing;\n\n(v) Air leakage and liquid leakage testing;\n\n(vi) Latching and unlatching testing;\n\n(vii) Kink and bend testing;\n\n(viii) Insertion force testing;\n\n(ix) Torque testing;\n\n(x) Corrosion testing; and\n\n(xi) Dimensional tolerance testing.", "tags": ["Risk factors: Reliability", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/06/08/2022-12364/medical-devices-cardiovascular-devices-classification-of-the-intravascular-bleed-monitor", "official_name": "21 CFR § 870.1345 (\"Intravascular bleed monitor\")", "label": "safe"}
{"id": "176_3", "doc_id": "176", "text": "(3) Performance data must support the sterility and pyrogenicity of the device components intended to be provided sterile.\n\n(4) Performance data must support the shelf life of the device by demonstrating continued sterility, package integrity, and device functionality over the identified shelf life.\n\n(5) The patient contacting components of the device must be demonstrated to be biocompatible.\n\n(6) Software verification, validation, and hazard analysis must be performed.\n\n(7) Performance data must demonstrate electromagnetic compatibility (EMC), electrical safety, thermal safety, and mechanical safety.\n\n(8) Human factors performance evaluation must demonstrate that the user can correctly use the device, based solely on reading the directions for use.\n\n(9) Labeling must include:\n\n(i) Instructions for use;\n\n(ii) A shelf life and storage conditions;\n\n(iii) Compatible procedures;\n\n(iv) A sizing table; and\n\n(v) Quantification of blood detected.", "tags": ["Risk factors: Reliability", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/06/08/2022-12364/medical-devices-cardiovascular-devices-classification-of-the-intravascular-bleed-monitor", "official_name": "21 CFR § 870.1345 (\"Intravascular bleed monitor\")", "label": "safe"}
{"id": "178_1", "doc_id": "178", "text": "§ 870.2210 \n\nAdjunctive predictive cardiovascular indicator.\n\n(a) Identification.\n\nThe adjunctive predictive cardiovascular indicator is a prescription device that uses software algorithms to analyze cardiovascular \n\nvital signs and predict future cardiovascular status or events. This device is intended for adjunctive use with other physical vital sign parameters and patient information and is not intended to independently direct therapy.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/02/14/2022-03096/medical-devices-cardiovascular-devices-classification-of-the-adjunctive-predictive-cardiovascular", "official_name": "21 CFR § 870.2210 (\"Adjunctive predictive cardiovascular indicator\")", "label": "safe"}
{"id": "178_2", "doc_id": "178", "text": "(b) Classification.\n\nClass II (special controls). The special controls for this device are:\n\n(1) A software description and the results of verification and validation testing based on a comprehensive hazard analysis and risk assessment must be provided, including:\n\n(i) A full characterization of the software technical parameters, including algorithms;\n\n(ii) A description of the expected impact of all applicable sensor acquisition hardware characteristics and associated hardware specifications;\n\n(iii) A description of sensor data quality control measures;\n\n(iv) A description of all mitigations for user error or failure of any subsystem components (including signal detection, signal analysis, data display, and storage) on output accuracy;\n\n(v) A description of the expected time to patient status or clinical event for all expected outputs, accounting for differences in patient condition and environment; and\n\n(vi) The sensitivity, specificity, positive predictive value, and negative predictive value in both percentage and number form.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/02/14/2022-03096/medical-devices-cardiovascular-devices-classification-of-the-adjunctive-predictive-cardiovascular", "official_name": "21 CFR § 870.2210 (\"Adjunctive predictive cardiovascular indicator\")", "label": "safe"}
{"id": "178_3", "doc_id": "178", "text": "(2) A scientific justification for the validity of the predictive cardiovascular indicator algorithm(s) must be provided. This justification must include verification of the algorithm calculations and validation using an independent data set.\n\n(3) A human factors and usability engineering assessment must be provided that evaluates the risk of misinterpretation of device output.\n\n(4) A clinical data assessment must be provided. This assessment must fulfill the following:\n\n(i) The assessment must include a summary of the clinical data used, including source, patient demographics, and any techniques used for annotating and separating the data.\n\n(ii) The clinical data must be representative of the intended use population for the device. Any selection criteria or sample limitations must be fully described and justified.\n\n(iii) The assessment must demonstrate output consistency using the expected range of data sources and data quality encountered in the intended use population and environment.\n\n(iv) The assessment must evaluate how the device output correlates with the predicted event or status.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/02/14/2022-03096/medical-devices-cardiovascular-devices-classification-of-the-adjunctive-predictive-cardiovascular", "official_name": "21 CFR § 870.2210 (\"Adjunctive predictive cardiovascular indicator\")", "label": "safe"}
{"id": "178_4", "doc_id": "178", "text": "(5) Labeling must include:\n\n(i) A description of what the device measures and outputs to the user;\n\n(ii) Warnings identifying sensor acquisition factors that may impact measurement results;\n\n(iii) Guidance for interpretation of the measurements, including a statement that the output is adjunctive to other physical vital sign parameters and patient information;\n\n(iv) A specific time or a range of times before the predicted patient status or clinical event occurs, accounting for differences in patient condition and environment;\n\n(v) Key assumptions made during calculation of the output;\n\n(vi) The type(s) of sensor data used, including specification of compatible sensors for data acquisition;\n\n(vii) The expected performance of the device for all intended use populations and environments; and\n\n(viii) Relevant characteristics of the patients studied in the clinical validation (including age, gender, race or ethnicity, and patient condition) and a summary of validation results.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: Accuracy thereof", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/02/14/2022-03096/medical-devices-cardiovascular-devices-classification-of-the-adjunctive-predictive-cardiovascular", "official_name": "21 CFR § 870.2210 (\"Adjunctive predictive cardiovascular indicator\")", "label": "safe"}
{"id": "1782_5", "doc_id": "1782", "text": "(d) New York must establish that the burden of responsibility of prov-\n ing that AI products do not cause harm to New Yorkers will be shouldered\n by the developers and deployers of AI. While government and civil socie-\n ty must act to review and enforce human rights laws around the use of AI,\n the  companies  employing  and profiting from the use of AI must lead in\n ensuring that their products are free from algorithmic discrimination.", "tags": ["Risk factors: Bias", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Strategies: Tiering", "Strategies: Tiering: Tiering based on impact"], "source": "https://www.nysenate.gov/legislation/bills/2025/S1169", "official_name": "New York artificial intelligence act (New York AI act)", "label": "safe"}
{"id": "1782_8", "doc_id": "1782", "text": "(g) Lastly, it is in the interest of all New Yorkers that certain uses\n of AI that infringe on fundamental rights, deepen structural inequality,\n or that result in unequal access to services shall be blocked.", "tags": ["Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Bias", "Strategies: Performance requirements", "Strategies: Tiering", "Strategies: Tiering: Tiering based on impact"], "source": "https://www.nysenate.gov/legislation/bills/2025/S1169", "official_name": "New York artificial intelligence act (New York AI act)", "label": "safe"}
{"id": "1782_10", "doc_id": "1782", "text": "§ 86. UNLAWFUL DISCRIMINATORY PRACTICES.   IT  SHALL  BE  AN  UNLAWFUL\n DISCRIMINATORY PRACTICE:\n   1.  FOR  A DEVELOPER OR DEPLOYER TO USE, SELL, OR SHARE A HIGH-RISK AI\n SYSTEM OR A PRODUCT FEATURING A HIGH-RISK AI SYSTEM THAT PRODUCES  ALGO-\n RITHMIC DISCRIMINATION; OR\n   2.  FOR  A DEVELOPER TO USE, SELL, OR SHARE A HIGH-RISK AI SYSTEM OR A\n PRODUCT FEATURING A HIGH-RISK AI SYSTEM THAT HAS NOT PASSED AN INDEPEND-\n ENT review, IN ACCORDANCE WITH SECTION EIGHTY-SEVEN OF THIS ARTICLE, THAT\n HAS FOUND THAT THE PRODUCT DOES NOT IN FACT PRODUCE ALGORITHMIC DISCRIM-\n INATION.", "tags": ["Strategies: Performance requirements", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: External auditing", "Strategies: Tiering: Tiering based on impact", "Strategies: Tiering"], "source": "https://www.nysenate.gov/legislation/bills/2025/S1169", "official_name": "New York artificial intelligence act (New York AI act)", "label": "safe"}
{"id": "1782_17", "doc_id": "1782", "text": "§  87.  reviews. 1.   PRIOR TO DEPLOYMENT OF A HIGH-RISK AI SYSTEM, SIX\n MONTHS AFTER DEPLOYMENT, AND AT LEAST EVERY EIGHTEEN  MONTHS  THEREAFTER\n FOR  EACH  CALENDAR YEAR A HIGH-RISK AI SYSTEM IS IN USE AFTER THE FIRST\n POST-DEPLOYMENT review, EVERY DEVELOPER OR DEPLOYER  OF  A  HIGH-RISK  AI\n SYSTEM  SHALL  CAUSE  TO  BE CONDUCTED AT LEAST ONE THIRD-PARTY review IN\n adherence WITH THE PROVISIONS OF THIS SECTION TO ENSURE THAT THE  PROD-\n UCT  DOES  NOT  PRODUCE ALGORITHMIC DISCRIMINATION AND COMPLIES WITH THE\n PROVISIONS OF THIS ARTICLE.  REGARDLESS OF FINAL FINDINGS, THE  DEPLOYER\n OR  DEVELOPER  SHALL  DELIVER ALL reviews CONDUCTED UNDER THIS SECTION TO\n THE ATTORNEY GENERAL.\n   2. A DEPLOYER OR DEVELOPER MAY HIRE MORE THAN ONE AUDITOR  TO  FULFILL\n THE REQUIREMENTS OF THIS SECTION.", "tags": ["Strategies: Evaluation: External auditing", "Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation: Conformity assessment", "Strategies: Performance requirements", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure"], "source": "https://www.nysenate.gov/legislation/bills/2025/S1169", "official_name": "New York artificial intelligence act (New York AI act)", "label": "safe"}
{"id": "1782_18", "doc_id": "1782", "text": "3. THE review SHALL INCLUDE THE FOLLOWING:\n   (A) AN ANALYSIS OF DATA MANAGEMENT POLICIES INCLUDING WHETHER PERSONAL\n OR  SENSITIVE  DATA  RELATING  TO A CONSUMER IS SUBJECT TO DATA SECURITY\n PROTECTION STANDARDS THAT COMPLY WITH THE REQUIREMENTS OF SECTION  EIGHT\n HUNDRED NINETY-NINE-BB OF THE GENERAL BUSINESS LAW;\n   (B)  AN  ANALYSIS  OF THE SYSTEM ACCURACY AND RELIABILITY ACCORDING TO\n EACH SPECIFIED USE CASE LISTED IN THE ENTITY'S REPORTING DOCUMENT  FILED\n BY THE DEVELOPER OR DEPLOYER UNDER SECTION EIGHTY-EIGHT OF THIS ARTICLE;\n   (C)  DISPARATE  IMPACTS  AND  A  DETERMINATION  OF WHETHER THE PRODUCT\n PRODUCES ALGORITHMIC DISCRIMINATION IN infraction OF THIS ARTICLE BY EACH\n INTENDED AND FORESEEABLE IDENTIFIED USE AS IDENTIFIED  BY  THE  DEPLOYER\n AND DEVELOPER;\n   (D)  ANALYSIS  OF  HOW  THE TECHNOLOGY COMPLIES WITH EXISTING RELEVANT\n FEDERAL, STATE, AND LOCAL PRIVACY AND DATA PRIVACY LAWS; AND\n   (E) AN EVALUATION OF THE DEVELOPER'S  OR  DEPLOYER'S  DOCUMENTED  RISK\n MANAGEMENT POLICY AND PROGRAM REQUIRED UNDER SECTION EIGHTY-NINE OF THIS\n ARTICLE FOR CONFORMITY WITH SUBDIVISION ONE OF SUCH SECTION EIGHTY-NINE.", "tags": ["Strategies: Evaluation", "Risk factors: Privacy", "Risk factors: Reliability", "Risk factors: Security", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In standard form", "Risk factors: Security: Dissemination"], "source": "https://www.nysenate.gov/legislation/bills/2025/S1169", "official_name": "New York artificial intelligence act (New York AI act)", "label": "safe"}
{"id": "1782_21", "doc_id": "1782", "text": "6.  AN  review  CONDUCTED  UNDER THIS SECTION SHALL BE COMPLETED IN ITS\n ENTIRETY WITHOUT THE ASSISTANCE OF AN AI SYSTEM.\n   7. (A) AN AUDITOR SHALL BE AN INDEPENDENT  ENTITY  INCLUDING  BUT  NOT\n LIMITED  TO  AN  INDIVIDUAL, NON-PROFIT, FIRM, CORPORATION, PARTNERSHIP,\n COOPERATIVE, OR ASSOCIATION.\n   (B) FOR THE PURPOSES OF THIS ARTICLE, NO AUDITOR MAY  BE  COMMISSIONED\n BY  A  DEVELOPER  OR DEPLOYER OF AN AI SYSTEM IF SUCH ENTITY HAS ALREADY\n BEEN COMMISSIONED TO  PROVIDE  ANY  AUDITING  OR  NON-AUDITING  SERVICE,\n INCLUDING BUT NOT LIMITED TO FINANCIAL AUDITING, CYBERSECURITY AUDITING,\n OR CONSULTING SERVICES OF ANY TYPE,  TO THE COMMISSIONING COMPANY IN THE\n PAST TWELVE MONTHS.\n   (C)  FEES  PAID TO AUDITORS MAY NOT BE CONTINGENT ON THE RESULT OF THE\n review AND THE COMMISSIONING COMPANY must avoid PROVIDE ANY encouragements  OR\n BONUSES FOR A POSITIVE review RESULT.", "tags": ["Strategies: Evaluation: External auditing", "Strategies: Evaluation"], "source": "https://www.nysenate.gov/legislation/bills/2025/S1169", "official_name": "New York artificial intelligence act (New York AI act)", "label": "safe"}
{"id": "1782_27", "doc_id": "1782", "text": "6.  FOR  HIGH-RISK  AI  SYSTEMS WHICH ARE ALREADY IN DEPLOYMENT AT THE\n TIME OF THE EFFECTIVE DATE OF THIS  ARTICLE,  DEVELOPERS  AND  DEPLOYERS\n SHALL HAVE EIGHTEEN MONTHS FROM SUCH EFFECTIVE DATE TO COMPLETE AND FILE\n THE REPORTS AND INDEPENDENT review REQUIRED BY THIS ARTICLE.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation"], "source": "https://www.nysenate.gov/legislation/bills/2025/S1169", "official_name": "New York artificial intelligence act (New York AI act)", "label": "safe"}
{"id": "1782_31", "doc_id": "1782", "text": "§  89-B.  ENFORCEMENT.  1.  WHENEVER THERE SHALL BE A infraction OF ANY\n PROVISION OF THIS ARTICLE, AN APPLICATION MAY BE MADE  BY  THE  ATTORNEY\n GENERAL  IN  THE  NAME  OF  THE  PEOPLE OF THE STATE OF NEW YORK, TO THE\n SUPREME COURT HAVING JURISDICTION BY A SPECIAL PROCEEDING  TO  ISSUE  AN\n INJUNCTION, AND UPON NOTICE TO THE RESPONDENT OF NOT LESS THAN TEN DAYS,\n TO  ENJOIN  AND  RESTRAIN  THE  CONTINUANCE OF SUCH infraction; AND IF IT\n SHALL APPEAR TO THE SATISFACTION OF THE COURT THAT THE  RESPONDENT  HAS,\n IN  FACT,  VIOLATED  THIS  ARTICLE,  AN  INJUNCTION MAY BE ISSUED BY THE\n COURT, ENJOINING AND RESTRAINING ANY FURTHER VIOLATIONS, WITHOUT REQUIR-\n ING PROOF THAT ANY PERSON HAS, IN FACT, BEEN INJURED OR DAMAGED THEREBY.\n IN ANY SUCH PROCEEDING, THE COURT MAY MAKE ALLOWANCES  TO  THE  ATTORNEY\n GENERAL  AS  PROVIDED  IN  PARAGRAPH  SIX  OF SUBDIVISION (A) OF SECTION\n EIGHTY-THREE HUNDRED THREE OF THE CIVIL  PRACTICE  LAW  AND  RULES,  AND\n DIRECT  RESTITUTION. WHENEVER THE COURT SHALL DETERMINE THAT A infraction\n OF THIS ARTICLE HAS OCCURRED, THE COURT MAY IMPOSE A  CIVIL  consequence  OF\n NOT MORE THAN TWENTY THOUSAND DOLLARS FOR EACH infraction.", "tags": ["Incentives: Civil liability", "Incentives: Fines"], "source": "https://www.nysenate.gov/legislation/bills/2025/S1169", "official_name": "New York artificial intelligence act (New York AI act)", "label": "safe"}
{"id": "1782_32", "doc_id": "1782", "text": "2.  THERE SHALL BE A PRIVATE RIGHT OF ACTION BY PLENARY PROCEEDING FOR\n ANY PERSON HARMED BY ANY infraction OF THIS ARTICLE BY ANY NATURAL PERSON\n OR ENTITY.  THE COURT SHALL AWARD COMPENSATORY DAMAGES AND LEGAL FEES TO\n THE PREVAILING PARTY.\n   3. IN EVALUATING ANY MOTION TO DISMISS A PLENARY PROCEEDING  COMMENCED\n PURSUANT TO SUBDIVISION TWO OF THIS SECTION, THE COURT SHALL PRESUME THE\n SPECIFIED AI SYSTEM WAS CREATED AND/OR OPERATED IN infraction OF A SPECI-\n FIED  LAW  OR  LAWS  AND  THAT  SUCH  infraction CAUSED THE HARM OR HARMS\n ALLEGED.\n   (A) A DEFENDANT CAN REBUT PRESUMPTIONS MADE PURSUANT TO THIS  SUBDIVI-\n SION  THROUGH CLEAR AND CONVINCING EVIDENCE THAT THE SPECIFIED AI SYSTEM\n DID NOT CAUSE THE HARM OR HARMS  ALLEGED  AND/OR  DID  NOT  VIOLATE  THE\n ALLEGED  LAW OR LAWS. AN ALGORITHMIC review CAN BE CONSIDERED AS EVIDENCE\n IN REBUTTING SUCH PRESUMPTIONS, BUT THE MERE EXISTENCE OF SUCH AN review,\n WITHOUT ADDITIONAL EVIDENCE, must avoid BE CONSIDERED CLEAR AND  CONVINC-\n ING EVIDENCE.\n   (B) WHERE SUCH PRESUMPTIONS ARE NOT REBUTTED PURSUANT TO THIS SUBDIVI-\n SION, THE ACTION must avoid BE DISMISSED.\n   (C) WHERE SUCH PRESUMPTIONS ARE REBUTTED PURSUANT TO THIS SUBDIVISION,\n A MOTION TO DISMISS AN ACTION SHALL BE ADJUDICATED WITHOUT ANY CONSIDER-\n ATION OF THIS SECTION.\n   4.  THE  SUPREME  COURT  IN THE STATE SHALL HAVE JURISDICTION OVER ANY\n ACTION, CLAIM, OR LAWSUIT TO ENFORCE THE PROVISIONS OF THIS ARTICLE.", "tags": ["Incentives: Civil liability"], "source": "https://www.nysenate.gov/legislation/bills/2025/S1169", "official_name": "New York artificial intelligence act (New York AI act)", "label": "safe"}
{"id": "1783_2", "doc_id": "1783", "text": "Oversight of Frontier Models: \n \nI.\tHardware  \nTraining a frontier model would require tremendous computing resources. Entities that sell or rent the use of a large amount of computing hardware, potentially set at the level specified by E.O. 14110, for AI development would report large acquisitions or usage of such computing resources to the oversight entity and exercise due diligence to ensure that customers are known and vetted, particularly with respect to foreign persons.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Input controls", "Strategies: Input controls: Compute circulation", "Risk factors: Security: Dissemination", "Risk factors: Security"], "source": "https://www.king.senate.gov/imo/media/doc/bipartisan_ai_framework_letter.pdf", "official_name": "Framework to Mitigate AI-Enabled Extreme Risks", "label": "safe"}
{"id": "1783_3", "doc_id": "1783", "text": "II.\tDevelopment of Frontier Models  \nDevelopers would notify the oversight entity when developing a frontier model and prior to initiating training runs. Developers would be required to incorporate safeguards against the four extreme risks identified above, and adhere to cybersecurity standards to ensure models are not leaked prematurely or stolen.  \n \nFrontier model developers could be required to report to the oversight entity on steps taken to mitigate the four identified risks and implement cybersecurity standards.", "tags": ["Strategies: Performance requirements", "Strategies: Disclosure", "Risk factors: Security", "Risk factors: Security: Cybersecurity"], "source": "https://www.king.senate.gov/imo/media/doc/bipartisan_ai_framework_letter.pdf", "official_name": "Framework to Mitigate AI-Enabled Extreme Risks", "label": "safe"}
{"id": "1783_4", "doc_id": "1783", "text": "III.\tDeployment of Frontier Models \nFrontier model developers would undergo evaluation and obtain a license from the oversight entity prior to release. This evaluation would only consider whether the frontier model has incorporated sufficient safeguards against the four identified risks.  \n \nA tiered licensing structure would be utilized to determine how widely the frontier model could be shared. For instance, frontier models with low risk could be licensed for open-source deployment, whereas models with higher risks could be licensed for deployment with vetted customers or limited public use.", "tags": ["Strategies: Licensing, registration, and certification", "Strategies: Evaluation", "Strategies: Tiering", "Strategies: Evaluation: Conformity assessment", "Strategies: Tiering: Tiering based on impact", "Risk factors: Security", "Risk factors: Security: Dissemination"], "source": "https://www.king.senate.gov/imo/media/doc/bipartisan_ai_framework_letter.pdf", "official_name": "Framework to Mitigate AI-Enabled Extreme Risks", "label": "safe"}
{"id": "179_1", "doc_id": "179", "text": "§ 870.2790 \n\nPhotoplethysmograph analysis software for over-the-counter use.\n\n(a) Identification.\n\nA photoplethysmograph analysis software device for over-the-counter use analyzes photoplethysmograph data and provides information for identifying irregular heart rhythms. This device is not intended to provide a diagnosis.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation: Impact assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/02/04/2022-02358/medical-devices-cardiovascular-devices-classification-of-the-photoplethysmograph-analysis-software", "official_name": "21 CFR § 870.2790 (\"Photoplethysmograph analysis software for over-the-counter use\")", "label": "safe"}
{"id": "179_2", "doc_id": "179", "text": "(b) Classification.\n\nClass II (special controls). The special controls for this device are:\n\n(1) Clinical performance testing must demonstrate the performance characteristics of the detection algorithm under anticipated conditions of use.\n\n(2) Software verification, validation, and hazard analysis must be performed. Documentation must include a characterization of the technical specifications of the software, including the detection algorithm and its inputs and outputs.\n\n(3) Non-clinical performance testing must demonstrate the ability of the device to detect adequate photoplethysmograph signal quality.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation: Impact assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/02/04/2022-02358/medical-devices-cardiovascular-devices-classification-of-the-photoplethysmograph-analysis-software", "official_name": "21 CFR § 870.2790 (\"Photoplethysmograph analysis software for over-the-counter use\")", "label": "safe"}
{"id": "179_3", "doc_id": "179", "text": "(4) Human factors and usability testing must demonstrate the following:\n\n(i) The user can correctly use the device based solely on reading the device labeling; and\n\n(ii) The user can correctly interpret the device output and understand when to seek medical care.\n\n(5) Labeling must include:\n\n(i) Hardware platform and operating system requirements;\n\n(ii) Situations in which the device may not operate at an expected performance level;\n\n(iii) A summary of the clinical performance testing conducted with the device;\n\n(iv) A description of what the device measures and outputs to the user; and\n\n(v) Guidance on interpretation of any results.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation: Impact assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/02/04/2022-02358/medical-devices-cardiovascular-devices-classification-of-the-photoplethysmograph-analysis-software", "official_name": "21 CFR § 870.2790 (\"Photoplethysmograph analysis software for over-the-counter use\")", "label": "safe"}
{"id": "180_2", "doc_id": "180", "text": "(a) Identification.\n\nAn electrocardiograph software device for over-the-counter use creates, analyzes, and displays electrocardiograph data and can provide information for identifying cardiac arrhythmias. This device is not intended to provide a diagnosis.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Harms: Harm to health/safety", "Strategies: Evaluation: Impact assessment", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/01/18/2022-00827/medical-devices-cardiovascular-devices-classification-of-the-electrocardiograph-software-for", "official_name": "21 CFR § 870.2345 (\"Electrocardiograph software for over-the-counter use\")", "label": "safe"}
{"id": "180_3", "doc_id": "180", "text": "(b) Classification.\n\nClass II (special controls). The special controls for this device are:\n\n(1) Clinical performance testing under anticipated conditions of use must demonstrate the following:\n\n(i) The ability to obtain an electrocardiograph of sufficient quality for display and analysis; and\n\n(ii) The performance characteristics of the detection algorithm as reported by sensitivity and either specificity or positive predictive value.\n\n(2) Software verification, validation, and hazard analysis must be performed. Documentation must include a characterization of the technical specifications of the software, including the detection algorithm and its inputs and outputs.\n\n(3) Non-clinical performance testing must validate detection algorithm performance using a previously adjudicated data set.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Harms: Harm to health/safety", "Strategies: Evaluation: Impact assessment", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/01/18/2022-00827/medical-devices-cardiovascular-devices-classification-of-the-electrocardiograph-software-for", "official_name": "21 CFR § 870.2345 (\"Electrocardiograph software for over-the-counter use\")", "label": "safe"}
{"id": "1802_15", "doc_id": "1802", "text": "4. Technological measures to address risks \n\nResponding to the above risks, AI developers, service providers, and system users should prevent risks by taking technological measures in the fields of training data, computing infrastructures, models and algorithms, product services, and application scenarios.\n\n\n4.1 Addressing AI’s inherent safety risks \n\n\n4.1.1 Addressing risks from models and algorithms\n\n(a)\tExplainability and predictability of AI should be constantly improved to provide clear explanation for the internal structure, reasoning logic, technical interfaces, and output results of AI systems, accurately reflecting the process by which AI systems produce outcomes.\n\n(b)\tSecure development standards should be established and implemented in the design, exploratory work, deployment, and maintenance processes to eliminate as many security flaws and discrimination tendencies in models and algorithms as possible and enhance robustness.", "tags": ["Risk factors: Interpretability and explainability", "Harms: Discrimination", "Risk factors: Security", "Risk factors: Reliability: Robustness", "Risk factors: Reliability", "Strategies: Performance requirements"], "source": "https://www.tc260.org.cn/upload/2024-09-09/1725849192841090989.pdf", "official_name": "AI Safety Governance Framework v1.0 (National Technical Committee 260 on Cybersecurity of SAC)", "label": "safe"}
{"id": "1802_16", "doc_id": "1802", "text": "4.1.2 Addressing risks from data\n\n(a)\tSecurity rules on data collection and usage, and on processing personal information should be abided by in all procedures of training data and user interaction data, including data collection, storage, usage, processing, transmission, provision, publication, and deletion. This aims to fully ensure user’s legitimate rights stipulated by laws and regulations, such as their rights to control, to be informed, and to choose.\n\n(b)\tProtection of IPR should be strengthened to prevent infringement on IPR in stages such as selecting training data and result outputs. \n\n(c)\tTraining data should be strictly selected to ensure exclusion of sensitive data in high-risk fields such as nuclear, biological, and chemical weapons and missiles. \n\n(d)\tData security management should be strengthened to comply with data security and personal information protection standards and regulations if training data contains sensitive personal information and important data.\n\n(e)\tTo use truthful, precise, objective, and diverse training data from legitimate sources, and filter ineffective, wrong, and biased data in a timely manner.\n\n(f)\tThe cross-border provision of AI services should comply with the regulations on cross-border data flow. The external provision of AI models and algorithms should comply with export control requirements.", "tags": ["Strategies: Input controls", "Risk factors: Privacy", "Strategies: Input controls: Data use", "Strategies: Input controls: Data circulation", "Risk factors: Security", "Risk factors: Security: Dissemination"], "source": "https://www.tc260.org.cn/upload/2024-09-09/1725849192841090989.pdf", "official_name": "AI Safety Governance Framework v1.0 (National Technical Committee 260 on Cybersecurity of SAC)", "label": "safe"}
{"id": "1802_18", "doc_id": "1802", "text": "4.2 Addressing safety risks in AI applications\n\n\n4.2.1 Addressing cyberspace risks\n\n(a)\tA security protection mechanism should be established to prevent model from being interfered and tampered during operation to ensure reliable outputs.\n\n(b)\tA data safeguard should be set up to make sure that AI systems comply with applicable laws and regulations when outputting sensitive personal information and important data. 4.2.2 Addressing real-world risks \n\n(a)\tTo establish service limitations according to users’ actual application scenarios and cut AI systems’ features that might be abused. AI systems should not provide services that go beyond the preset scope. \n\n(b)\tTo improve the ability to trace the end use of AI systems to prevent high-risk application scenarios such as manufacturing of weapons of mass destruction, like nuclear, biological, chemical weapons and missiles.", "tags": ["Risk factors: Security", "Risk factors: Reliability", "Risk factors: Privacy", "Applications: Medicine, life sciences and public health", "Strategies: Performance requirements"], "source": "https://www.tc260.org.cn/upload/2024-09-09/1725849192841090989.pdf", "official_name": "AI Safety Governance Framework v1.0 (National Technical Committee 260 on Cybersecurity of SAC)", "label": "safe"}
{"id": "1802_20", "doc_id": "1802", "text": "4.2.4 Addressing ethical risks\n\n(a)\tTraining data should be filtered and outputs should be verified during algorithm design, model training and optimization, service provision and other processes, in an effort to prevent discrimination based on ethnicities, beliefs, nationalities, region, gender, age, occupation and health factors, among others. \n\n(b)\tAI systems applied in key sectors, such as government departments, critical information infrastructure, and areas directly affecting public safety and people's health and safety, should be equipped with high-efficient emergency management and control measures.", "tags": ["Harms: Discrimination", "Risk factors: Bias", "Strategies: Performance requirements", "Risk factors: Safety", "Harms: Harm to health/safety"], "source": "https://www.tc260.org.cn/upload/2024-09-09/1725849192841090989.pdf", "official_name": "AI Safety Governance Framework v1.0 (National Technical Committee 260 on Cybersecurity of SAC)", "label": "safe"}
{"id": "1802_27", "doc_id": "1802", "text": "(f)\tService providers should increase awareness of AI risk prevention, establish and improve a real-time risk monitoring and management mechanism, and continuously track operational security risks.\n\n(g)\tService providers should assess the ability of AI products and services to withstand or overcome adverse conditions under faults, attacks, or other anomalies, and prevent unexpected results and behavioral errors, ensuring that a minimum level of effective functionality is maintained.\n\n(h)\tService providers should promptly report safety and security incidents and vulnerabilities detected in AI system operations to competent authorities.\n\n(i)\tService providers should stipulate in contracts or service agreements that they have the right to take corrective measures or terminate services early upon detecting misuse and abuse not conforming to usage intention and stated limitations.\n\n(j)\tService providers should assess the impact of AI products on users, preventing harm to users' mental and physical health, life, and property.", "tags": ["Risk factors: Security", "Strategies: Evaluation: Adversarial testing", "Strategies: Evaluation", "Risk factors: Reliability", "Risk factors: Reliability: Robustness", "Harms: Harm to health/safety", "Harms: Harm to property", "Strategies: Disclosure", "Strategies: Disclosure: About incidents", "Risk factors: Safety"], "source": "https://www.tc260.org.cn/upload/2024-09-09/1725849192841090989.pdf", "official_name": "AI Safety Governance Framework v1.0 (National Technical Committee 260 on Cybersecurity of SAC)", "label": "safe"}
{"id": "1802_28", "doc_id": "1802", "text": "6.3 Safety guidelines for users in key areas\n\n(a)\tFor users in key sectors such as government departments, critical information infrastructure, and areas directly affecting public safety and people's health and safety, they should prudently assess the long-term and potential impacts of applying AI technology in the target application scenarios and conduct risk assessments and grading to avoid technology abuse.\n\n(b)\tUsers should regularly perform system reviews on the applicable scenarios, safety, reliability, and controllability of AI systems, while enhancing awareness of risk prevention and response capabilities.\n\n(c)\tUsers should fully understand its data processing and privacy protection measures before using an AI product.\n\n(d)\tUsers should use high-security passwords and enable multi-factor authentication mechanisms to enhance account security.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Risk factors: Reliability", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Harms: Harm to health/safety", "Strategies: Performance requirements"], "source": "https://www.tc260.org.cn/upload/2024-09-09/1725849192841090989.pdf", "official_name": "AI Safety Governance Framework v1.0 (National Technical Committee 260 on Cybersecurity of SAC)", "label": "safe"}
{"id": "1802_29", "doc_id": "1802", "text": "(e)\tUsers should enhance their capabilities in areas such as network security and supply chain security to reduce the risk of AI systems being attacked and important data being stolen or leaked, as well as ensure uninterrupted business.\n\n(f)\tUsers should properly limit data access, develop data backup and recovery plans, and regularly check data processing flow.\n\n(g)\tUsers should ensure that operations comply with confidentiality provisions and use encryption technology and other protective measures when processing sensitive data.\n\n(h)\tUsers should effectively supervise the behavior and impact of AI, and ensure that AI products and services operate under human authorization and remain subject to human control.\n\n(i)\tUsers should avoid complete reliance on AI for decision making, monitor and record instances where users turn down AI decisions, and analyze inconsistencies in decision-making. They should have the capability to swiftly shift to human-based or traditional methods in the event of an accident.", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Evaluation", "Strategies: Performance requirements"], "source": "https://www.tc260.org.cn/upload/2024-09-09/1725849192841090989.pdf", "official_name": "AI Safety Governance Framework v1.0 (National Technical Committee 260 on Cybersecurity of SAC)", "label": "safe"}
{"id": "1803_6", "doc_id": "1803", "text": "We call on governments to:\n1. Respect International Obligations and Commitments: Design, develop, procure, deploy, use, and decommission AI systems in a manner consistent with the Universal Declaration of Human Rights and international law, including the UN Charter and international human rights law.", "tags": ["Strategies: Performance requirements", "Harms: Violation of civil or human rights, including privacy"], "source": "https://pl.usembassy.gov/freedom-online-coalition-joint-statement-on-responsible-government-practices-for-ai-technologies/", "official_name": "Freedom Online Coalition Joint Statement on Responsible Government Practices for AI Technologies", "label": "safe"}
{"id": "1803_7", "doc_id": "1803", "text": "2. Assess Impacts of AI Systems in High-Risk Contexts* Prior to Deployment and Use: Identify, assess, manage, and address potential risks to human rights, equity, fairness, and safety before deployment and use of an AI system on a case-by-case basis, and refrain from deploying AI systems where risks are incompatible with the protection of international human rights. The assessment may include:\n- Assessing the intended purpose and reasonably foreseeable uses of AI systems as well as their expected benefits to help ensure that AI is well-suited to accomplish the relevant task.\n- Assessing the potential risks of using AI in a given context, including by assessing the possible failure modes of the AI system and of the broader system, both in isolation and as a result of human users and other likely variables outside the scope of the system itself, documenting which stakeholders will be most impacted by the AI system, and enabling the meaningful participation of impacted stakeholders throughout the value chain of the AI system.\n- Evaluating the quality and representativeness of the data used in AI systems’ design, development, training, testing, and operation and its fitness to the AI system’s intended purpose, insofar as possible, including evaluation of:\n  - The data collection, preparation, storage, and retention process, as well as the provenance of any data used to train, charge-tune, or operate the AI system;\n  - The quality and representativeness of the data for its intended purpose;\n  - How the data is relevant to the task being automated and may reasonably be expected to be useful for the AI system’s development, testing, and operation; and\n  - Whether the data contains sufficient breadth to address the range of real-world inputs the AI system might encounter and how data gaps, data inaccuracies, and other shortcomings can be addressed.\n- Testing the AI system for performance in realistic conditions or contexts to ensure the AI, as well as components that rely on it, will perform its intended purpose in real-world contexts, and considering leveraging pilots and limited releases with strong monitoring, evaluation, and safeguards in place to carry out the testing before wider releases.\n- Testing or performing internal reviews for accuracy and discriminatory bias, particularly pertaining to race, ethnicity, disability, gender, sexuality, and gender identity and expression.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Risk factors: Bias", "Risk factors: Safety", "Strategies: Evaluation: Conformity assessment", "Strategies: Convening", "Strategies: Evaluation: Adversarial testing", "Risk factors: Reliability", "Risk factors: Reliability: Robustness", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on impact"], "source": "https://pl.usembassy.gov/freedom-online-coalition-joint-statement-on-responsible-government-practices-for-ai-technologies/", "official_name": "Freedom Online Coalition Joint Statement on Responsible Government Practices for AI Technologies", "label": "safe"}
{"id": "1803_8", "doc_id": "1803", "text": "3. Conduct ongoing monitoring of AI systems in high-impact contexts throughout their use: Identify, assess, and mitigate AI systems that may impact human rights, equity, fairness, or safety during use by conducting ongoing monitoring to identify, for example, degradation of AI systems’ functionality and to detect changes in the AI system’s impact on equity, fairness, human rights, and safety throughout the entire AI value chain, and ceasing use of AI systems as soon as is practicable where an AI systems’ risks to human rights or safety exceed an acceptable level and where mitigation strategies do not sufficiently reduce risk. Incorporate feedback mechanisms, including from affected stakeholders and/or by participating in external reviews or participating in third-party evaluations, to allow evidence-based discovery and reporting by end-users and third parties of technical vulnerabilities and misuses of the AI system, and take action to correct and address them.", "tags": ["Strategies: Evaluation: Post-market monitoring", "Risk factors: Bias", "Risk factors: Safety", "Strategies: Evaluation: External auditing", "Strategies: Convening", "Strategies: Evaluation", "Strategies: Performance requirements"], "source": "https://pl.usembassy.gov/freedom-online-coalition-joint-statement-on-responsible-government-practices-for-ai-technologies/", "official_name": "Freedom Online Coalition Joint Statement on Responsible Government Practices for AI Technologies", "label": "safe"}
{"id": "1803_10", "doc_id": "1803", "text": "5. Communicate and Respond to the Public: Publicize available policies regarding how governments will protect human rights in the context of their AI activities. Establish processes for public disclosure of high-risk uses of AI systems and seek out and incorporate feedback from stakeholders or the public on uses of AI systems that impact equity, fairness, human rights, and safety, including by providing and maintaining options to opt out of AI-enabled decisions when appropriate.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Convening", "Risk factors: Bias", "Risk factors: Safety", "Strategies: Performance requirements", "Risk factors: Transparency", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on impact", "Harms: Violation of civil or human rights, including privacy"], "source": "https://pl.usembassy.gov/freedom-online-coalition-joint-statement-on-responsible-government-practices-for-ai-technologies/", "official_name": "Freedom Online Coalition Joint Statement on Responsible Government Practices for AI Technologies", "label": "safe"}
{"id": "1803_13", "doc_id": "1803", "text": "AI can help address society’s greatest challenges and further progress towards the 2030 Agenda and Sustainable Development Goals. Together, we pledge to develop and use AI responsibly, and call upon all governments to join us.\n*For the purpose of this Joint Statement, High-Risk AI refers to AI systems that impact human rights and/or safety, which may be more likely in sectors such as healthcare, law enforcement and justice, and provision of public benefits.", "tags": ["Risk factors: Safety", "Applications: Medicine, life sciences and public health", "Applications: Government: benefits and welfare", "Applications: Government: judicial and law enforcement"], "source": "https://pl.usembassy.gov/freedom-online-coalition-joint-statement-on-responsible-government-practices-for-ai-technologies/", "official_name": "Freedom Online Coalition Joint Statement on Responsible Government Practices for AI Technologies", "label": "safe"}
{"id": "1804_6", "doc_id": "1804", "text": "5. Iterative\nRisk assessments should inform concrete decisions and be conducted at regular intervals to adapt\nto progress in advanced AI systems and AI safety research. Risks should be assessed and mitigated\nacross the AI lifecycle, as appropriate, including before advanced AI systems are deployed and\nthroughout the development process in an iterative, holistic manner. Risks and harms should also\nbe monitored post-deployment to account for flaws and vulnerabilities that emerge as advanced AI\nsystems are integrated into products and services, including the risk associated with downstream\nmisuse and interaction with other deployed systems. Information from ongoing monitoring should\nbe incorporated into risk assessment processes.", "tags": ["Risk factors: Safety", "Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring"], "source": "https://www.nist.gov/system/files/documents/2024/11/20/Joint%20Statement%20on%20Risk%20Assessment%20of%20Advanced%20AI%20Systems.pdf", "official_name": "Joint Statement on Risk Assessment of Advanced AI Systems (International Network of AI Safety Institutes)", "label": "safe"}
{"id": "1804_7", "doc_id": "1804", "text": "6. Reproducible\nRisk assessments should be, to the extent possible, reproducible and appropriately documented.\nMethodologies and results that are reproducible allow for independent third-party evaluators to\nreplicate, verify and validate risk assessments. This can help improve risk assessment processes,\ndecrease error in results, and increase interoperability among methods and actors.", "tags": ["Strategies: Evaluation: External auditing", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation"], "source": "https://www.nist.gov/system/files/documents/2024/11/20/Joint%20Statement%20on%20Risk%20Assessment%20of%20Advanced%20AI%20Systems.pdf", "official_name": "Joint Statement on Risk Assessment of Advanced AI Systems (International Network of AI Safety Institutes)", "label": "safe"}
{"id": "1805_1", "doc_id": "1805", "text": "PART 850—PROVISIONS PERTAINING TO U.S. INVESTMENTS IN CERTAIN NATIONAL SECURITY TECHNOLOGIES AND PRODUCTS IN COUNTRIES OF CONCERN\n\nSubpart A—General\n\n§ 850.101\n\nScope.\n\n(a) This part implements Executive Order 14105 of August 9, 2023, “Addressing United States Investments in Certain National Security Technologies and Products in Countries of Concern” (the Order), directing the Secretary of the Treasury (the Secretary), in consultation with the Secretary of Commerce and, as appropriate, the heads of other relevant executive departments and agencies, to issue, subject to public notice and comment, regulations that require U.S. persons to provide notification of information relative to certain transactions involving covered foreign persons and that restrict U.S. persons from engaging in certain other transactions involving covered foreign persons.\n\n(b) The regulations identify certain types of transactions that are covered transactions—that is, transactions that are either notifiable or restricted. Additionally, the regulations identify other instances where a U.S. person has obligations with respect to certain transactions. The regulations prescribe exceptions to the definition of covered transaction. A transaction that meets an exception is not a covered transaction and is referred to as an excepted transaction. Finally, the regulations prescribe a process for the Secretary to excused certain covered transactions from the rules otherwise prohibiting or requiring notification of covered transactions on a case-by-case basis.\n\n(c) The regulations identify categories of covered transactions that are notifiable transactions. A notifiable transaction is a transaction by a U.S. person or its controlled foreign entity with or resulting in the establishment of a covered foreign person that engages in a covered activity that the Secretary, in consultation with the Secretary of Commerce and, as appropriate, the heads of other relevant agencies, has determined may contribute to the threat to the national security of the United States identified in the Order, or the engagement of a person of a country of concern in a covered activity that the Secretary, in consultation with the Secretary of Commerce and, as appropriate, the heads of other relevant agencies, has determined may contribute to the threat to the national security of the United States identified in the Order. The regulations require a U.S. person to notify the Department of the Treasury of each such notifiable transaction by such U.S. person or its controlled foreign entity. The regulations also require a U.S. person to provide prompt notice to the Department of the Treasury upon acquiring actual knowledge after the completion date of a transaction of facts or circumstances that would have caused the transaction to be a covered transaction if the U.S. person had had such knowledge on the completion date. Additionally, any person who makes a representation, statement, or certification under this part is required to promptly notify the Department of the Treasury upon learning of a material omission or inaccuracy in such representation, statement, or certification.", "tags": ["Applications: Security", "Applications: Government: military and public safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Applications: Government: military and public safety", "Incentives: Access to business opportunities", "Incentives: Civil liability"], "source": "https://www.federalregister.gov/documents/2024/11/15/2024-25422/provisions-pertaining-to-us-investments-in-certain-national-security-technologies-and-products-in", "official_name": "Provisions Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern ", "label": "safe"}
{"id": "1805_2", "doc_id": "1805", "text": "(d) The regulations identify categories of covered transactions that are restricted transactions. A restricted transaction is a transaction by a U.S. person with or resulting in the establishment of a covered foreign person that engages in a covered activity that the Secretary, in consultation with the Secretary of Commerce and, as appropriate, the heads of other relevant agencies, has determined poses a particularly acute national security threat because of its potential to significantly advance the military, intelligence, surveillance, or cyber-enabled capabilities of a country of concern, or engagement of a person of a country of concern in a covered activity that the Secretary, in consultation with the Secretary of Commerce and, as appropriate, the heads of other relevant agencies, has determined poses a particularly acute national security threat because of its potential to significantly advance the military, intelligence, surveillance, or cyber-enabled capabilities of a country of concern. The regulations restrict a U.S. person from engaging in a restricted transaction and also restrict a U.S. person from knowingly directing a transaction that the U.S. person knows would be a restricted transaction if engaged in by a U.S. person. The regulations also require a U.S. person to take all reasonable steps to restrict and prevent any transaction by its controlled foreign entity that would be a restricted transaction if undertaken by a U.S. person. (e) Pursuant to the Order, the Secretary shall, as appropriate:\n\n(1) Communicate with the Congress and the public with respect to the implementation of the Order;\n\n(2) Consult with the Secretary of Commerce on industry engagement and analysis of notifiable transactions;\n\n(3) Consult with the Secretary of State, the Secretary of Defense, the Secretary of Commerce, the Secretary of Energy, and the Director of National Intelligence on the implications for military, intelligence, surveillance, or cyber-enabled capabilities of covered national security technologies and products in the Order and potential covered national security technologies and products;\n\n(4) Engage, together with the Secretary of State and the Secretary of Commerce, with allies and partners regarding the national security risks posed by countries of concern advancing covered national security technologies and products;\n\n(5) Consult with the Secretary of State on foreign policy considerations related to the implementation of the Order, including but not limited to the issuance and amendment of regulations; and\n\n(6) Investigate, in consultation with the heads of relevant agencies, as appropriate, violations of the Order or the regulations in this part and pursue available civil consequences for such violations.", "tags": ["Applications: Security", "Applications: Government: military and public safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Applications: Government: military and public safety", "Incentives: Access to business opportunities", "Incentives: Civil liability"], "source": "https://www.federalregister.gov/documents/2024/11/15/2024-25422/provisions-pertaining-to-us-investments-in-certain-national-security-technologies-and-products-in", "official_name": "Provisions Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern ", "label": "safe"}
{"id": "1805_4", "doc_id": "1805", "text": "Subpart B—Definitions\n\n§ 850.201\n\nAdvanced packaging.\n\nThe term advanced packaging means to package integrated circuits in a manner that supports the two-and-one-half-dimensional (2.5D) or three-dimensional (3D) assembly of integrated circuits, such as by directly attaching one or more die or wafer using through-silicon vias, die or wafer bonding, heterogeneous integration, or other advanced methods and materials.\n\n§ 850.202\n\nAI system.\n\nThe term AI system means:\n\n(a) A machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations, or decisions influencing real or virtual environments—i.e., a system that:\n\n(1) Uses data inputs to perceive real and virtual environments;\n\n(2) Abstracts such perceptions into models through automated or algorithmic statistical analysis; and\n\n(3) Uses model inference to make a classification, prediction, recommendation, or decision.\n\n(b) Any data system, software, hardware, application, tool, or utility that operates in whole or in part using a system described in paragraph (a) of this section.", "tags": ["Applications: Security", "Applications: Government: military and public safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Applications: Government: military and public safety", "Incentives: Access to business opportunities", "Incentives: Civil liability"], "source": "https://www.federalregister.gov/documents/2024/11/15/2024-25422/provisions-pertaining-to-us-investments-in-certain-national-security-technologies-and-products-in", "official_name": "Provisions Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern ", "label": "safe"}
{"id": "1805_5", "doc_id": "1805", "text": "§ 850.203\n\nCertification.\n\n(a) The term certification means a written statement signed by the chief executive officer or other duly authorized designee of the person filing a notification or providing other information that certifies under the consequences provided in the False Statements Accountability Act of 1996, as amended (18 U.S.C. 1001) that the notification or other information filed or provided:\n\n(1) Fully complies with the regulations in this part; and\n\n(2) Is accurate and complete in all material respects to the best knowledge of the person filing a notification or other information.\n\n(b) For purposes of this section, a duly authorized designee is:\n\n(1) In the case of a partnership, any general partner thereof;\n\n(2) In the case of a corporation, any officer thereof; and\n\n(3) In the case of any entity lacking partners and officers, any individual within the organization exercising executive functions similar to those of a general partner of a partnership or an officer of a corporation or otherwise authorized by the board of directors or equivalent to provide such certification.\n\n(c) In each case described in paragraphs (b)(1) through (3) of this section, such designee must possess actual authority to make the certification on behalf of the person filing a notification or other information.\n\nNote 1 to § 850.203:\n\nA template for certifications may be found at the Outbound Investment Security Program section of the Department of the Treasury website.", "tags": ["Applications: Security", "Applications: Government: military and public safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Applications: Government: military and public safety", "Incentives: Access to business opportunities", "Incentives: Civil liability"], "source": "https://www.federalregister.gov/documents/2024/11/15/2024-25422/provisions-pertaining-to-us-investments-in-certain-national-security-technologies-and-products-in", "official_name": "Provisions Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern ", "label": "safe"}
{"id": "1805_6", "doc_id": "1805", "text": "§ 850.204\n\nCompletion date.\n\nThe term completion date means:\n\n(a) With respect to a covered transaction other than under § 850.210(a)(6), the earliest date upon which any interest, asset, property, or right is conveyed, assigned, delivered, or otherwise transferred to a U.S. person, or as applicable, its controlled foreign entity; or\n\n(b) With respect to a covered transaction under § 850.210(a)(6), the earliest date upon which any interest, asset, property, or right in the relevant covered foreign person is conveyed, assigned, delivered, or otherwise transferred to the applicable support pool.\n\n§ 850.205\n\nContingent equity interest.\n\nThe term contingent equity interest means a financial interest (including debt) that currently does not constitute an equity interest but is convertible into, or provides the right to acquire, an equity interest upon the occurrence of a contingency or defined event or at the discretion of the U.S. person that holds the financial interest.\n\n§ 850.206\n\nControlled foreign entity.\n\n(a) The term controlled foreign entity means any entity incorporated in, or otherwise organized under the laws of, a country other than the United States of which a U.S. person is a parent.\n\n(b) For purposes of this term, the following rules shall apply in determining whether an entity is a parent of another entity in a tiered ownership structure:\n\n(1) Where the relationship between an entity and another entity is that of parent and subsidiary, the holdings of voting interest or voting power of the board, as applicable, of a subsidiary shall be fully attributed to the parent.\n\n(2) Where the relationship between an entity and another entity is not that of parent and subsidiary (i.e., because the holdings of voting interest or voting power of the board, as applicable, of the first entity in the second entity is 50 percent or less), then the indirect downstream holdings of voting interest or voting power of the board, as applicable, attributed to the first entity shall be determined proportionately.\n\n(3) Where the circumstances in paragraphs (b)(1) and (2) of this section apply (i.e., because a U.S. person holds both direct and indirect downstream holdings in the same entity), any holdings of voting interest shall be aggregated for the purposes of applying this definition, and any holdings of voting power of the board shall be aggregated for the purposes of applying this definition. Voting interest must avoid be aggregated with voting power of the board for the purposes of applying this definition.", "tags": ["Applications: Security", "Applications: Government: military and public safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Applications: Government: military and public safety", "Incentives: Access to business opportunities", "Incentives: Civil liability"], "source": "https://www.federalregister.gov/documents/2024/11/15/2024-25422/provisions-pertaining-to-us-investments-in-certain-national-security-technologies-and-products-in", "official_name": "Provisions Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern ", "label": "safe"}
{"id": "1805_7", "doc_id": "1805", "text": "§ 850.207\n\nCountry of concern.\n\nThe term country of concern has the meaning given to it in the Annex to the Order.\n\n§ 850.208\n\nCovered activity.\n\nThe term covered activity means, in the context of a particular transaction, any of the activities referred to in the definition of notifiable transaction in § 850.217 or restricted transaction in § 850.224.\n\n§ 850.209\n\nCovered foreign person.\n\n(a) The term covered foreign person means:\n\n(1) A person of a country of concern that engages in a covered activity; or\n\n(2) A person that directly or indirectly holds a board seat on, a voting or equity interest (other than through securities or interests that would satisfy the conditions in § 850.501(a) if held by a U.S. person) in, or any contractual power to direct or cause the direction of the management or policies of any person or persons described in paragraph (a)(1) of this section from or through which it:\n\n(i) Derives more than 50 percent of its revenue individually, or as aggregated across such persons from each of which it derives at least $50,000 (or equivalent) of its revenue, on an annual basis;\n\n(ii) Derives more than 50 percent of its net income individually, or as aggregated across such persons from each of which it derives at least $50,000 (or equivalent) of its net income, on an annual basis;\n\n(iii) Incurs more than 50 percent of its capital expenditure individually, or as aggregated across such persons from each of which it incurs at least $50,000 (or equivalent) of its capital expenditure, on an annual basis; or\n\n(iv) Incurs more than 50 percent of its operating expenses individually, or as aggregated across such persons from each of which it incurs at least $50,000 (or equivalent) of its operating expenses, on an annual basis.\n\n(3) With respect to a covered transaction described in § 850.210(a)(5), the person of a country of concern that participates in the joint venture is deemed to be a covered foreign person by virtue of its participation in the joint venture.\n\n(b) For purposes of paragraph (a)(2) of this section:\n\n(1) Calculations shall be based on an audited financial statement from the most recent year. If an audited financial statement is not available, the most recent unaudited financial statement shall be used instead. If no financial statement is available, an independent appraisal shall be used instead. If no independent appraisal is available, a good-faith estimate shall be used instead.\n\n(2) Where an amount is not denominated in U.S. dollars, the U.S. dollar equivalent shall be determined based on the most recent published rate of exchange available on the Department of the Treasury's website.\n\nNote 1 to § 850.209: \n\nReferences in this section to revenue, net income, capital expenditure, or operating expenses refer to overall revenue, net income, capital expenditure, or operating expenses, as applicable, without subtracting amounts attributable to persons described in paragraph (a)(1) of this section of less than $50,000 (or equivalent).", "tags": ["Applications: Security", "Applications: Government: military and public safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Applications: Government: military and public safety", "Incentives: Access to business opportunities", "Incentives: Civil liability"], "source": "https://www.federalregister.gov/documents/2024/11/15/2024-25422/provisions-pertaining-to-us-investments-in-certain-national-security-technologies-and-products-in", "official_name": "Provisions Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern ", "label": "safe"}
{"id": "1805_9", "doc_id": "1805", "text": "§ 850.211\n\nDevelop.\n\nExcept as used in § 850.210(a)(4), the term develop means to engage in any stages prior to serial production, such as design or substantive modification, design research, design analyses, design concepts, assembly and testing of prototypes, pilot production schemes, design data, process of transforming design data into a product, configuration design, integration design, and layouts.\n\n§ 850.212\n\nEntity.\n\nThe term entity means any branch, partnership, association, estate, joint venture, trust, corporation or division of a corporation, group, sub-group, or other organization (whether or not organized under the laws of any State or foreign state).\n\n§ 850.213\n\nExcepted transaction.\n\nThe term excepted transaction means a transaction that meets the criteria in § 850.501.\n\n§ 850.214\n\nFabricate.\n\nThe term fabricate means to form devices such as transistors, poly capacitors, non-metal resistors, and diodes on a wafer of semiconductor material.\n\n§ 850.215\n\nKnowingly directing.\n\nThe term knowingly directing has the definition set forth in § 850.303.", "tags": ["Applications: Security", "Applications: Government: military and public safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Applications: Government: military and public safety", "Incentives: Access to business opportunities", "Incentives: Civil liability"], "source": "https://www.federalregister.gov/documents/2024/11/15/2024-25422/provisions-pertaining-to-us-investments-in-certain-national-security-technologies-and-products-in", "official_name": "Provisions Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern ", "label": "safe"}
{"id": "1805_11", "doc_id": "1805", "text": "§ 850.218\n\nPackage.\n\nThe term package means to assemble various components, such as the integrated circuit die, lead frames, interconnects, and substrate materials to safeguard the semiconductor device and provide electrical connections between different parts of the die.\n\n§ 850.219\n\nParent.\n\nThe term parent means, with respect to an entity:\n\n(a) A person who or which directly or indirectly holds more than 50 percent of:\n\n(1) The outstanding voting interest in the entity; or\n\n(2) The voting power of the board of the entity;\n\n(b) The general partner, managing member, or equivalent of the entity; or\n\n(c) The investment adviser to any entity that is a pooled investment support pool, with “investment adviser” as defined in the Investment Advisers Act of 1940 (15 U.S.C. 80b-2(a)(11)).\n\nNote 1 to § 850.219:\n\nAny entity that meets the conditions of paragraph (a), (b), or (c) of this section with respect to another entity is the parent, even if the parent entity is an intermediate entity and not the ultimate parent. \n\n§ 850.220\n\nPerson.\n\nThe term person means any individual or entity.\n\n§ 850.221\n\nPerson of a country of concern.\n\nThe term person of a country of concern means:\n\n(a) Any individual that:\n\n(1) Is a citizen or permanent resident of a country of concern;\n\n(2) Is not a U.S. citizen; and\n\n(3) Is not a permanent resident of the United States;\n\n(b) An entity with a principal place of business in, headquartered in, or incorporated in or otherwise organized under the laws of, a country of concern;\n\n(c) The government of a country of concern, including any political subdivision, political party, agency, or instrumentality thereof; any person acting for or on behalf of the government of a country of concern; or any entity with respect to which the government of a country of concern holds individually or in the aggregate, directly or indirectly, 50 percent or more of the entity's outstanding voting interest, voting power of the board, or equity interest, or otherwise possesses the power to direct or cause the direction of the management and policies of such entity (whether through the ownership of voting securities, by contract, or otherwise);\n\n(d) Any entity in which one or more persons identified in paragraph (a), (b), or (c) of this section, individually or in the aggregate, directly or indirectly, \n\nholds at least 50 percent of any of the following interests of such entity: outstanding voting interest, voting power of the board, or equity interest; or\n\n(e) Any entity in which one or more persons identified in paragraph (d) of this section, individually or in the aggregate, directly or indirectly, holds at least 50 percent of any of the following interests of such entity: outstanding voting interest, voting power of the board, or equity interest.", "tags": ["Applications: Security", "Applications: Government: military and public safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Applications: Government: military and public safety", "Incentives: Access to business opportunities", "Incentives: Civil liability"], "source": "https://www.federalregister.gov/documents/2024/11/15/2024-25422/provisions-pertaining-to-us-investments-in-certain-national-security-technologies-and-products-in", "official_name": "Provisions Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern ", "label": "safe"}
{"id": "1805_13", "doc_id": "1805", "text": "§ 850.225\n\nQuantum computer.\n\nThe term quantum computer means a computer that performs computations that harness the collective properties of quantum states, such as superposition, interference, or entanglement.\n\n§ 850.226\n\nRelevant agencies.\n\nThe term relevant agencies means the Departments of State, Defense, Justice, Commerce, Energy, and Homeland Security, the Office of the United States Trade Representative, the Office of Science and Technology Policy, the Office of the Director of National Intelligence, the Office of the National Cyber Director, and any other department, agency, or office the Secretary determines appropriate.\n\n§ 850.227\n\nSubsidiary.\n\nThe term subsidiary means, with respect to a person, an entity of which such person is a parent.\n\n§ 850.228\n\nUnited States.\n\nThe term United States or U.S. means the United States of America, the States of the United States of America, the District of Columbia, and any commonwealth, territory, dependency, or possession of the United States of America, or any subdivision of the foregoing, and includes the territorial sea of the United States of America. For purposes of this part, an entity organized under the laws of the United States of America, one of the States, the District of Columbia, or a commonwealth, territory, dependency, or possession of the United States is an entity organized “in the United States.”\n\n§ 850.229U.S. person. The term U.S. person means any United States citizen, lawful permanent resident, entity organized under the laws of the United States or any jurisdiction within the United States, including any foreign branch of any such entity, or any person in the United States.\n\nSubpart C—restricted Transactions and Other restricted Activities\n\n§ 850.301\n\nUndertaking a restricted transaction.\n\nA U.S. person may not engage in a restricted transaction unless an exception for that transaction has been granted under § 850.502.", "tags": ["Applications: Security", "Applications: Government: military and public safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Applications: Government: military and public safety", "Incentives: Access to business opportunities", "Incentives: Civil liability"], "source": "https://www.federalregister.gov/documents/2024/11/15/2024-25422/provisions-pertaining-to-us-investments-in-certain-national-security-technologies-and-products-in", "official_name": "Provisions Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern ", "label": "safe"}
{"id": "1805_14", "doc_id": "1805", "text": "§ 850.302\n\nActions of a controlled foreign entity.\n\n(a) A U.S. person shall take all reasonable steps to restrict and prevent any transaction by its controlled foreign entity that would be a restricted transaction if engaged in by a U.S. person.\n\n(b) If a controlled foreign entity engages in a transaction that would be a restricted transaction if engaged in by a U.S. person, in determining whether the relevant U.S. person took all reasonable steps to restrict and prevent such transaction, the Department of the Treasury will consider, among other factors, any of the following with respect to a U.S. person and its controlled foreign entity:\n\n(1) The execution of agreements with respect to adherence with this part between the subject U.S. person and its controlled foreign entity;\n\n(2) The existence and exercise of governance or shareholder rights by the U.S. person with respect to the controlled foreign entity, where applicable;\n\n(3) The existence and implementation of periodic training and internal reporting requirements by the U.S. person and its controlled foreign entity with respect to adherence with this part;\n\n(4) The implementation of appropriate and documented internal controls, including internal policies, procedures, or guidelines that are periodically reviewed internally, by the U.S. person and its controlled foreign entity; and\n\n(5) Implementation of a documented testing and/or auditing process of internal policies, procedures, or guidelines.\n\nNote 1 to § 850.302:\n\nFindings of violations of this section and decisions related to enforcement and consequences will be made based on a consideration of the totality of relevant facts and circumstances, including whether the U.S. person has taken the steps described in paragraph (b) of this section and whether such steps were reasonable in light of the relevant facts and circumstances.", "tags": ["Applications: Security", "Applications: Government: military and public safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Applications: Government: military and public safety", "Incentives: Access to business opportunities", "Incentives: Civil liability"], "source": "https://www.federalregister.gov/documents/2024/11/15/2024-25422/provisions-pertaining-to-us-investments-in-certain-national-security-technologies-and-products-in", "official_name": "Provisions Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern ", "label": "safe"}
{"id": "1805_15", "doc_id": "1805", "text": "§ 850.303\n\nKnowingly directing an otherwise restricted transaction.\n\n(a) A U.S. person is restricted from knowingly directing a transaction by a non-U.S. person that the U.S. person knows at the time of the transaction would be a restricted transaction if engaged in by a U.S. person. For purposes of this section, a U.S. person “knowingly directs” a transaction when the U.S. person has authority, individually or as part of a group, to make or substantially participate in decisions on behalf of a non-U.S. person, and exercises that authority to direct, order, decide upon, or approve a transaction. Such authority exists when a U.S. person is an officer, director, or otherwise possesses executive responsibilities at a non-U.S. person.\n\n(b) A U.S. person that has the authority described in paragraph (a) of this section and recuses themself from each of the following activities will not be considered to have exercised their authority to direct, order, decide upon, or approve a transaction:\n\n(1) Participating in formal approval and decision-making processes related to the transaction, including making a recommendation;\n\n(2) Reviewing, editing, commenting on, approving, and signing relevant transaction documents; and\n\n(3) Engaging in negotiations with the investment target (or, as applicable, the relevant transaction counterparty, such as a joint venture partner).\n\nSubpart D—Notifiable Transactions and Other Notifiable Activities\n\n§ 850.401\n\nUndertaking a notifiable transaction.\n\nA U.S. person that undertakes a notifiable transaction shall file a notification of that transaction with the Department of the Treasury pursuant to § 850.404.\n\n§ 850.402\n\nNotification of actions of a controlled foreign entity.\n\nA U.S. person shall file a notification with the Department of the Treasury pursuant to § 850.404 with respect to any transaction by a controlled foreign entity of that U.S. person that would be a notifiable transaction if engaged in by a U.S. person.\n\n§ 850.403 \n\nNotification of post-transaction knowledge.\n\nA U.S. person that acquires actual knowledge after the completion date of a transaction of a fact or circumstance such that the transaction would have been a covered transaction if such knowledge had been possessed by the relevant U.S. person at the time of the transaction shall promptly, and in no event later than 30 calendar days following the acquisition of such knowledge, submit a notification pursuant to § 850.404. This requirement applies regardless of whether the transaction would have been a notifiable transaction or a restricted transaction.\n\nNote 1 to § 850.403:\n\nA U.S. person's submission of a notification pursuant to this section must avoid preclude a finding by the Department of the Treasury that as a factual matter the U.S. person had relevant knowledge of the transaction's status at the time of the transaction.", "tags": ["Applications: Security", "Applications: Government: military and public safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Applications: Government: military and public safety", "Incentives: Access to business opportunities", "Incentives: Civil liability"], "source": "https://www.federalregister.gov/documents/2024/11/15/2024-25422/provisions-pertaining-to-us-investments-in-certain-national-security-technologies-and-products-in", "official_name": "Provisions Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern ", "label": "safe"}
{"id": "1805_18", "doc_id": "1805", "text": "§ 850.502 \n\nNational interest exception.\n\n(a) The Secretary, in consultation with the Secretary of Commerce, the Secretary of State, and the heads of relevant agencies, as appropriate, may determine that a covered transaction is in the national interest of the United States and therefore is excused from applicable provisions in subparts C and D of this part (excluding §§ 850.406, 850.603, and 850.604). Such a determination may be made following a request by a U.S. person on its own behalf or on behalf of its controlled foreign entity.\n\n(b) Any determination pursuant to paragraph (a) of this section will be based on a consideration of the totality of the relevant facts and circumstances and may be informed by, among other considerations, the transaction's effect on critical U.S. supply chain needs; domestic production needs in the United States for projected national defense requirements; United States' technological leadership globally in areas affecting U.S. national security; and impact on U.S. national security if the U.S. person is restricted from undertaking the transaction.\n\n(c) A U.S. person seeking a national interest exception shall submit relevant information to the Department of the Treasury regarding the transaction and shall articulate the basis for the request, including the U.S. person's analysis of the transaction's potential impact on the national interest of the United States and the certification referred to in § 850.203. Information and other documents submitted by the U.S. person to the Department of the Treasury under this section shall be deemed part of the national interest exception request. The U.S. person shall follow the instructions posted on the Department of the Treasury's Outbound Investment Security Program website. No communications or submissions other than those described in this section shall constitute a request for a national interest exception. The Department of the Treasury may request additional information that may include some or all of the information required under § 850.405.\n\n(d) A determination that a covered transaction is excused under this section may be subject to binding conditions.\n\n(e) No determination pursuant to paragraph (a) of this section will be valid unless provided to the subject U.S. person in writing and signed by the Assistant Secretary or Deputy Assistant Secretary of the Treasury for Investment Security.\n\nNote 1 to § 850.502:\n\nA process and related information for exception requests will be made available on the Department of the Treasury's Outbound Investment Security Program website.\n\n§ 850.503 \n\nIEEPA statutory exception.\n\nConduct referred to in 50 U.S.C. 1702(b) must avoid be regulated or restricted, directly or indirectly, by this part.\n\nSubpart F—Violations", "tags": ["Applications: Security", "Applications: Government: military and public safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Applications: Government: military and public safety", "Incentives: Access to business opportunities", "Incentives: Civil liability"], "source": "https://www.federalregister.gov/documents/2024/11/15/2024-25422/provisions-pertaining-to-us-investments-in-certain-national-security-technologies-and-products-in", "official_name": "Provisions Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern ", "label": "safe"}
{"id": "1805_19", "doc_id": "1805", "text": "§ 850.601 \n\nTaking actions restricted by this part.\n\nThe taking of any action restricted by this part is a infraction of this part.\n\n§ 850.602 \n\nFailure to fulfill requirements.\n\nFailure to take any action required by this part, and within the time frame and in the manner specified by this part, as applicable, is a infraction of this part.\n\n§ 850.603 \n\nMisrepresentation, concealment, and omission of facts.\n\nWith respect to any information submission to or communication with the Department of the Treasury pursuant to any provision of this part, the making of any materially false or misleading representation, statement, or certification, or falsifying, concealing or omitting any material fact is a infraction of this part.\n\n§ 850.604 \n\nEvasions; attempts; causing violations; conspiracies.\n\n(a) Any action on or after the effective date of this part that evades or avoids, has the purpose of evading or avoiding, causes a infraction of, or attempts to violate any of the prohibitions set forth in this part is restricted.\n\n(b) Any conspiracy formed to violate the prohibitions set forth in this part is restricted.\n\nSubpart G—consequences and Disclosures", "tags": ["Applications: Security", "Applications: Government: military and public safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Applications: Government: military and public safety", "Incentives: Access to business opportunities", "Incentives: Civil liability"], "source": "https://www.federalregister.gov/documents/2024/11/15/2024-25422/provisions-pertaining-to-us-investments-in-certain-national-security-technologies-and-products-in", "official_name": "Provisions Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern ", "label": "safe"}
{"id": "1805_20", "doc_id": "1805", "text": "§ 850.701 \n\nconsequences.\n\n(a) Section 206 of IEEPA applies to any person subject to the jurisdiction of the United States who violates, attempts to violate, conspires to violate, or causes a infraction of any order, regulation, or prohibition issued by or pursuant to the direction or authorization of the Secretary pursuant to this part or otherwise under IEEPA.\n\n(1) A civil consequence may be imposed on any person who violates, attempts to violate, conspires to violate, or causes a infraction of any order, regulation, or prohibition issued under IEEPA, including any provision of this part in an amount not to exceed the greater of:\n\n(i) $250,000, as such amount is adjusted pursuant to the Federal Civil consequences Inflation Adjustment Act of 1990, as amended (Pub. L. 101-410, 28 U.S.C. 2461 note); or\n\n(ii) An amount that is twice the amount of the transaction that is the basis of the infraction with respect to which the consequence is imposed.\n\n(2) A person who willfully commits, willfully attempts to commit, willfully conspires to commit, or aids or abets in the commission of a infraction, attempt to violate, conspiracy to violate, or causing of a infraction of any order, regulation, or prohibition issued under IEEPA, including any provision of this part, shall, upon conviction, be fined not more than $1,000,000, or if a natural person, be imprisoned for not more than 20 years, or both.\n\n(b) The Secretary may refer potential criminal violations of the Order, or of this part, to the Attorney General.\n\n(c) The civil consequences provided for in IEEPA are subject to adjustment pursuant to the Federal Civil consequences Inflation Adjustment Act of 1990, as amended (Pub. L. 101-410, 28 U.S.C. 2461 note). Notice of the maximum consequence which may be assessed under this section will be published in the \n\nFederal Register\n\nand on Treasury's Outbound Investment Security Program website on an annual basis on or before January 15 of each calendar year.\n\n(d) The criminal consequences provided for in IEEPA are subject to adjustment pursuant to 18 U.S.C. 3571.\n\n(e) The consequences available under this section are without prejudice to other consequences, civil or criminal, and forfeiture of property, available under other applicable law.\n\n(f) Pursuant to 18 U.S.C. 1001, whoever, in any matter within the jurisdiction of the executive, legislative, or judicial branch of the Government of the United States, knowingly and willfully falsifies, conceals or covers up by any trick, scheme, or device a material fact; makes any materially false, fictitious, or fraudulent statement or representation; or makes or uses any false writing or document knowing the same to contain any materially false, fictitious, or fraudulent statement or entry shall be fined under title 18, United States Code, or imprisoned not more than 5 years, or both.", "tags": ["Applications: Security", "Applications: Government: military and public safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Applications: Government: military and public safety", "Incentives: Access to business opportunities", "Incentives: Civil liability"], "source": "https://www.federalregister.gov/documents/2024/11/15/2024-25422/provisions-pertaining-to-us-investments-in-certain-national-security-technologies-and-products-in", "official_name": "Provisions Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern ", "label": "safe"}
{"id": "1805_21", "doc_id": "1805", "text": "§ 850.702 \n\nAdministrative collection; referral to United States Department of Justice.\n\nThe imposition of a monetary consequence under this part creates a debt due to the U.S. Government. The Department of the Treasury may take action to collect the consequence assessed if not paid. In addition or instead, the matter may be referred to the Department of Justice for appropriate action to recover the consequence.\n\n§ 850.703 \n\nDivestment.\n\n(a) The Secretary, in consultation with the heads of relevant agencies, as appropriate, may take any action authorized under IEEPA to nullify, void, or otherwise compel the divestment of any restricted transaction entered into after the effective date of this part.\n\n(b) The Secretary may refer any action taken under paragraph (a) of this section to the Attorney General to seek appropriate relief to enforce such action.\n\n§ 850.704 \n\nVoluntary self-disclosure.\n\n(a) Any person who has engaged in conduct that may constitute a infraction of this part may submit a voluntary self-disclosure of that conduct to the Department of the Treasury.\n\n(b) In determining the appropriate response to any infraction, the Department of the Treasury will consider the submission and the timeliness of any voluntary self-disclosure.\n\n(c) In assessing the timeliness of a voluntary self-disclosure, the Department of the Treasury will consider whether it has learned of the conduct prior to the voluntary self-disclosure. The Department of the Treasury may consider disclosure of a infraction to another government agency other than the Department of the Treasury as a voluntary self-disclosure based on a case-by-case assessment.\n\n(d) Notwithstanding the foregoing, identification to the Department of the Treasury of conduct that may constitute a infraction of this part may not be assessed to be a voluntary self-disclosure in one or more of the following circumstances:\n\n(1) A third party has provided a prior disclosure to the Department of the Treasury of the conduct or similar conduct related to the same pattern or practice, regardless of whether the disclosing person knew of the third party's prior disclosure;\n\n(2) The disclosure includes materially false or misleading information;\n\n(3) The disclosure, when considered along with supplemental information timely provided by the disclosing person, is materially incomplete;\n\n(4) The disclosure is not self-initiated, including when the disclosure results from a suggestion or order of a Federal or state agency or official;\n\n(5) The disclosure is a response to an administrative subpoena or other inquiry from the Department of the Treasury or another government agency;\n\n(6) The disclosure is made about the conduct of an entity by an individual in such entity without the authorization of such entity's senior management; or\n\n(7) The filing is made pursuant to a required notification under this part, including § 850.403 or § 850.406.\n\n(e) A voluntary self-disclosure to the Department of the Treasury must take the form of a written notice describing the conduct that may constitute a infraction and each of the persons involved. A voluntary self-disclosure must include, or be followed within a reasonable period of time by, a report of sufficient detail to afford a complete understanding of the conduct that may constitute the infraction. A person making a voluntary self-disclosure must respond in a timely manner to any follow-up inquiries by the Department of the Treasury.", "tags": ["Incentives: Access to business opportunities", "Incentives: Civil liability"], "source": "https://www.federalregister.gov/documents/2024/11/15/2024-25422/provisions-pertaining-to-us-investments-in-certain-national-security-technologies-and-products-in", "official_name": "Provisions Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern ", "label": "safe"}
{"id": "1805_22", "doc_id": "1805", "text": "Subpart H—Provision and Handling of Information\n\n§ 850.801 \n\nConfidentiality.\n\n(a) Except to the extent required by law or otherwise provided in paragraphs (b) through (d) of this section, information or documentary materials not otherwise publicly available that are submitted to the Department of the Treasury under this part must avoid be disclosed to the public.\n\n(b) Notwithstanding paragraph (a) of this section, except to the extent restricted by law, the Department of the Treasury may disclose information or documentary materials that are not otherwise publicly available, subject to appropriate confidentiality and classification requirements, when such information or documentary materials are:\n\n(1) Relevant to any judicial or administrative action or proceeding;\n\n(2) Provided to Congress or to any duly authorized committee or subcommittee of Congress; or\n\n(3) Provided to any domestic governmental entity, or to any foreign governmental entity of a United States partner or ally, where the information or documentary materials are important to the national security analysis or actions of such governmental entity or the Department of the Treasury.\n\n(c) Notwithstanding paragraph (a) of this section, the Department of the Treasury may disclose to third parties information or documentary materials that are not otherwise publicly available when the person who submitted or filed the information or documentary materials has consented to its disclosure to such third parties.\n\n(d) Notwithstanding paragraph (a) of this section, the Department of the Treasury may disclose information that is not already publicly available, when such disclosure of information is determined by the Secretary to be in the national interest. Any determination under this paragraph (d) may not be delegated below the level of the Assistant Secretary of the Treasury.\n\n(e) The Department of the Treasury may use the information gathered pursuant to this part to fulfill its obligations under the Order, which may include publication of anonymized data.\n\n§ 850.802 \n\nLanguage of information.\n\nAll materials or information filed with the Department of the Treasury under this part shall be submitted in English. If supplementary or additional materials were originally written in a foreign language, they shall be submitted in their original language. Where English versions of those documents exist, they shall also be submitted.", "tags": ["Applications: Security", "Applications: Government: military and public safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Applications: Government: military and public safety", "Incentives: Access to business opportunities", "Incentives: Civil liability"], "source": "https://www.federalregister.gov/documents/2024/11/15/2024-25422/provisions-pertaining-to-us-investments-in-certain-national-security-technologies-and-products-in", "official_name": "Provisions Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern ", "label": "safe"}
{"id": "1805_23", "doc_id": "1805", "text": "Subpart I—Other Provisions\n\n§ 850.901 \n\nDelegation of authorities of the Secretary of the Treasury.\n\nAny action that the Secretary is authorized to take pursuant to the Order and any further executive orders relating to the national emergency declared in the Order may be taken by the Assistant Secretary of the Treasury for Investment Security or their designee or by any other person to whom the Secretary has delegated the authority so to act, as appropriate.\n\n§ 850.902 \n\nAmendment, modification, or revocation.\n\n(a) Except as otherwise provided by law, and in consultation with the Secretary of Commerce and, as appropriate, the heads of other relevant agencies, the Secretary may amend, modify, or revoke provisions of this part at any time.\n\n(b) Except as otherwise provided by law, any instructions, orders, forms, regulations, or rulings issued pursuant to this part may be amended, modified, or revoked at any time.\n\n(c) Unless otherwise specifically provided, any amendment, modification, or revocation of any provision in or appendix to this part does not affect any act done or omitted, or any civil or criminal proceeding commenced or pending, prior to such amendment, modification, or \n\nrevocation. All consequences, forfeitures, and liabilities under any such instructions, orders, forms, regulations, or rulings pursuant to this part continue and may be enforced as if such amendment, modification, or revocation had not been made.\n\n§ 850.903 \n\nSeverability.\n\nThe provisions of this part are separate and severable from one another. If any of the provisions of this part, or the application thereof to any person or circumstance, is held to be invalid, such invalidity must avoid affect other provisions or application of such provisions to other persons or circumstances that can be given effect without the invalid provision or application.\n\n§ 850.904 \n\nReports to be furnished on demand.\n\n(a) Any person is required to furnish under oath, in the form of reports or otherwise, at any time as may be required by the Department of the Treasury, complete information regarding any act or transaction subject to the provisions of this part, regardless of whether such act or transaction is effected pursuant to a national interest exception under § 850.502. Except as provided otherwise, the Department of the Treasury may, through any person or agency, conduct investigations, hold hearings, administer oaths, examine witnesses, receive evidence, take depositions, and require by subpoena the attendance and testimony of witnesses and the production of any books, contracts, letters, papers, and other hard copy or electronic documents relating to any matter under investigation, regardless of whether any report has been required or filed under this section.\n\n(b) For purposes of paragraph (a) of this section, the term document includes any written, recorded, or graphic matter or other means of preserving thought or expression (including in electronic format), and all tangible things stored in any medium from which information can be processed, transcribed, or obtained directly or indirectly.\n\n(c) Persons providing documents to the Department of the Treasury pursuant to this section must do so in a usable format agreed upon by the Department of the Treasury.", "tags": ["Applications: Security", "Applications: Government: military and public safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on inputs", "Applications: Government: military and public safety", "Incentives: Access to business opportunities", "Incentives: Civil liability"], "source": "https://www.federalregister.gov/documents/2024/11/15/2024-25422/provisions-pertaining-to-us-investments-in-certain-national-security-technologies-and-products-in", "official_name": "Provisions Pertaining to U.S. Investments in Certain National Security Technologies and Products in Countries of Concern ", "label": "safe"}
{"id": "1806_5", "doc_id": "1806", "text": "Model Prompts and Outputs: All prompts of a FM and retention of outputs, regardless of whether that model has been acquired by an IC element or instead is accessed through a publicly available interface, must be for an authorized purpose (and not a restricted use) and must be consistent with the AI Framework, the AG Guidelines, and applicable policies and procedures.\n\n\nA prompt that returns information reasonably understood to be information that an IC element does not currently hold as collected or acquired information should presumptively be considered “collection” for purposes of the AG Guidelines if the returned information is copied, saved, supplemented, or used . A prompt that returns information reasonably understood solely to be information an IC element already has collected will not be considered “ collection\" for purposes of the AG Guidelines absent some articulable, case-specific reason to consider it thus.\n\n\nAny covered information considered collected should be treated as subject to the IC element's AG Guidelines and other applicable policies, including for the collection of or access to, and use, dissemination and retention of, U.S. person information, as well as the CAI Framework as applicable.\n\n\nQuery rules contained in AG guidelines may also apply to prompts if such rules apply to data used to train or modify a FM. For example, if an IC element were to modify a model by charge-tuning it on data collected pursuant to E.O.12333, the applicable querying rules under E.O. 12333 would apply.", "tags": ["Strategies: Input controls", "Strategies: Input controls: Data circulation", "Strategies: Input controls: Data use", "Strategies: Disclosure", "Strategies: Disclosure: About inputs"], "source": "https://static01.nyt.com/newsgraphics/documenttools/4bdb5f4804e797e6/6fe2f6c0-full.pdf?ref=forever-wars.com", "official_name": "Common Intelligence Community Interim Guidance Regarding the Acquisition and Use of Foundation AI Models", "label": "safe"}
{"id": "1806_7", "doc_id": "1806", "text": "General Requirements and Definitions:\nThis Interim Guidance is supplemental to and does not supersede any applicable law, including E.O. 12333 and the AG Guidelines.\n\n\nFoundation models refer to AI systems that are trained on broad data and can be applied across a wide-range of use cases.\n\n\nLarge language models are a kind of foundation model that can achieve general-purpose language generation and other natural language processing tasks.\n\n\nFrontier models refer to any general-purpose AI system near the cutting-edge of performance, as measured by widely accepted publicly available benchmarks, or similar assessments of reasoning, science, and overall capabilities. Certain foundation models may qualify as frontier AI models.\n\n\nModel weights refer to numerical parameters that collectively determine how prompts are processed and converted into outputs within an AI model.\n\n\nCovered information refers to the category of data to which an individual agency's specific AG guidelines apply, including U.S. person information.\n\n\nModification refers to the process whereby an IC element alters the model weights of a FM by, for example, further training it on data collected by IC elements.\n\n\nAugmentation refers to the process whereby an IC element expands the utility of a FM without altering its model weights by, for example, connecting it to additional data,  resources, and tools.", "tags": ["Strategies: Tiering", "Strategies: Tiering: Tiering based on impact", "Risk factors: Privacy", "Strategies: Input controls: Data use", "Strategies: Input controls"], "source": "https://static01.nyt.com/newsgraphics/documenttools/4bdb5f4804e797e6/6fe2f6c0-full.pdf?ref=forever-wars.com", "official_name": "Common Intelligence Community Interim Guidance Regarding the Acquisition and Use of Foundation AI Models", "label": "safe"}
{"id": "181_1", "doc_id": "181", "text": "PART 7—SECURING THE INFORMATION AND COMMUNICATIONS TECHNOLOGY AND SERVICES SUPPLY CHAIN\n\nSubpart A—General\n\n§ 7.1 Purpose.\n\nThese regulations set forth the procedures by which the Secretary may: (a) Determine whether any acquisition, importation, transfer, installation, dealing in, or use of any information and communications technology or service (ICTS Transaction) that has been designed, developed, manufactured, or supplied by persons owned by, controlled by, or subject to the jurisdiction or direction of foreign adversaries poses certain undue or unacceptable risks as identified in the Executive Order; (b) issue a determination to restrict an ICTS Transaction; (c) direct the timing and manner of the cessation of the ICTS Transaction; and (d) consider factors that may mitigate the risks posed by the ICTS Transaction. The Secretary will evaluate ICTS Transactions under this rule, which include classes of transactions, on a case-by-case basis. The Secretary, in consultation with appropriate agency heads specified in Executive Order 13873 and other relevant governmental bodies, as appropriate, shall make an initial determination as to whether to restrict a given ICTS Transaction or propose mitigation measures, by which the ICTS Transaction may be permitted. Parties may submit information in response to the initial determination, including a response to the initial determination and any supporting materials and/or proposed measures to remediate or mitigate the risks identified in the initial determination as posed by the ICTS Transaction at issue. Upon consideration of the parties' submissions, the Secretary will issue a final determination prohibiting the transaction, not prohibiting the transaction, or permitting the transaction subject to the adoption of measures determined by the Secretary to sufficiently mitigate the risks associated with the ICTS Transaction. The Secretary shall also engage in coordination and information sharing, as appropriate, with international partners on the application of these regulations.", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Harms: Harm to infrastructure", "Harms: Ecological harm", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Convening", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines", "Incentives: Imprisonment", "Applications: Transportation", "Applications: Government: military and public safety"], "source": "https://www.federalregister.gov/documents/2021/01/19/2021-01234/securing-the-information-and-communications-technology-and-services-supply-chain", "official_name": "15 CFR part 7 (\"Securing the Information and Communications Technology and Services Supply Chain\")", "label": "safe"}
{"id": "181_2", "doc_id": "181", "text": "§ 7.2 Definitions.\n\nAppropriate agency heads means the Secretary of the Treasury, the Secretary of State, the Secretary of Defense, the Attorney General, the Secretary of Homeland Security, the United States Trade Representative, the Director of National Intelligence, the Administrator of General Services, the Chairman of the Federal Communications Commission, and the heads of any other executive departments and agencies the Secretary determines is appropriate.\n\nCommercial item has the same meaning given to it in Federal Acquisition Regulation (48 CFR part 2.101).\n\nDepartment means the United States Department of Commerce.\n\nEntity means a partnership, association, trust, joint venture, corporation, group, subgroup, or other non-U.S. governmental organization.\n\nExecutive Order means Executive Order 13873, May 15, 2019, “Securing the Information and Communications Technology and Services Supply Chain”.\n\nForeign adversary means any foreign government or foreign non-government person determined by the Secretary to have engaged in a long-term pattern or serious instances of conduct significantly adverse to the national security of the United States or security and safety of United States persons.", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Harms: Harm to infrastructure", "Harms: Ecological harm", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Convening", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines", "Incentives: Imprisonment", "Applications: Transportation", "Applications: Government: military and public safety"], "source": "https://www.federalregister.gov/documents/2021/01/19/2021-01234/securing-the-information-and-communications-technology-and-services-supply-chain", "official_name": "15 CFR part 7 (\"Securing the Information and Communications Technology and Services Supply Chain\")", "label": "safe"}
{"id": "181_3", "doc_id": "181", "text": "ICTS Transaction means any acquisition, importation, transfer, installation, dealing in, or use of any information and communications technology or service, including ongoing activities, such as managed services, data transmission, software updates, repairs, or the platforming or data hosting of applications for consumer download. An ICTS Transaction includes any other transaction, the structure of which is designed or intended to evade or circumvent the application of the Executive Order. The term ICTS Transaction includes a class of ICTS Transactions.\n\nIEEPA means the International Emergency Economic Powers Act (50 U.S.C. 1701,  et seq.).\n\nInformation and communications technology or services or ICTS means any hardware, software, or other product or service, including cloud-computing services, primarily intended to fulfill or enable the function of information or data processing, storage, retrieval, or communication by electronic means (including electromagnetic, magnetic, and photonic), including through transmission, storage, or display.\n\nParty or parties to a transaction means a person engaged in an ICTS Transaction, including the person acquiring the ICTS and the person from whom the ICTS is acquired. Party or parties to a transaction include entities designed, or otherwise used with the intention, to evade or circumvent application of the Executive Order. For purposes of this rule, this definition does not include common carriers, except to the extent that a common carrier knew or should have known (as the term “knowledge” is defined in 15 CFR 772.1) that it was providing transportation services of ICTS to one or more of the parties to a transaction that has been restricted in a final written determination made by the Secretary or, if permitted subject to mitigation measures, in infraction of such mitigation measures.\n\nPerson means an individual or entity.\n\nPerson owned by, controlled by, or subject to the jurisdiction or direction of a foreign adversary means any person, wherever located, who acts as an agent, representative, or employee, or any person who acts in any other capacity at the order, request, or under the direction or control, of a foreign adversary or of a person whose activities are directly or indirectly supervised, directed, controlled, financed, or subsidized in whole or in majority part by a foreign adversary; any person, wherever located, who is a citizen or resident of a nation-state controlled by a foreign adversary; any corporation, partnership, association, or other organization organized under the laws of a nation-state controlled by a foreign adversary; and any corporation, partnership, association, or other organization, wherever organized or doing business, that is owned or controlled by a foreign adversary.\n\nSecretary means the Secretary of Commerce or the Secretary's designee.", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Harms: Harm to infrastructure", "Harms: Ecological harm", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Convening", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines", "Incentives: Imprisonment", "Applications: Transportation", "Applications: Government: military and public safety"], "source": "https://www.federalregister.gov/documents/2021/01/19/2021-01234/securing-the-information-and-communications-technology-and-services-supply-chain", "official_name": "15 CFR part 7 (\"Securing the Information and Communications Technology and Services Supply Chain\")", "label": "safe"}
{"id": "181_4", "doc_id": "181", "text": "Sensitive personal data means:\n\n(1) Personally-identifiable information, including:\n\n(i) Financial data that could be used to analyze or determine an individual's financial distress or hardship;\n\n(ii) The set of data in a consumer report, as defined under 15 U.S.C. 1681a, unless such data is obtained from a consumer reporting agency for one or more purposes identified in 15 U.S.C. 1681b(a);\n\n(iii) The set of data in an application for health insurance, long-term care insurance, professional responsibility insurance, mortgage insurance, or life insurance;\n\n(iv) Data relating to the physical, mental, or psychological health condition of an individual;\n\n(v) Non-public electronic communications, including email, messaging, or chat communications, between or among users of a U.S. business's products or services if a primary purpose of such product or service is to facilitate third-party user communications;\n\n(vi) Geolocation data collected using positioning systems, cell phone towers, or WiFi access points such as via a mobile application, vehicle GPS, other onboard mapping tool, or wearable electronic device;\n\n(vii) Biometric enrollment data including facial, voice, retina/iris, and palm/fingerprint templates;\n\n(viii) Data stored and processed for generating a Federal, State, Tribal, Territorial, or other government identification card;\n\n(ix) Data concerning U.S. Government personnel security clearance status; or\n\n(x) The set of data in an application for a U.S. Government personnel security clearance or an application for employment in a position of public trust; or\n\n(2) Genetic information, which includes the results of an individual's genetic tests, including any related genetic sequencing data, whenever such results, in isolation or in combination with previously released or publicly available data, constitute identifiable data. Such results must avoid include data derived from databases maintained by the U.S. Government and routinely provided to private parties for purposes of research. For purposes of this paragraph, “genetic test” shall have the meaning provided in 42 U.S.C. 300gg-91(d)(17).\n\nUndue or unacceptable risk means those risks identified in Section 1(a)(ii) of the Executive Order.\n\nUnited States person means any United States citizen; any permanent resident alien; or any entity organized under the laws of the United States or any jurisdiction within the United States (including such entity's foreign branches).", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Harms: Harm to infrastructure", "Harms: Ecological harm", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Convening", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines", "Incentives: Imprisonment", "Applications: Transportation", "Applications: Government: military and public safety"], "source": "https://www.federalregister.gov/documents/2021/01/19/2021-01234/securing-the-information-and-communications-technology-and-services-supply-chain", "official_name": "15 CFR part 7 (\"Securing the Information and Communications Technology and Services Supply Chain\")", "label": "safe"}
{"id": "181_6", "doc_id": "181", "text": "§ 7.4 Determination of foreign adversaries.\n\n(a) The Secretary has determined that the following foreign governments or foreign non-government persons have engaged in a long-term pattern or serious instances of conduct significantly adverse to the national security of the United States or security and safety of United States persons and, therefore, constitute foreign adversaries solely for the purposes of the Executive Order, this rule, and any subsequent rule:\n\n(1) The People's Republic of China, including the Hong Kong Special Administrative Region (China);\n\n(2) Republic of Cuba (Cuba);\n\n(3) Islamic Republic of Iran (Iran);\n\n(4) Democratic People's Republic of Korea (North Korea);\n\n(5) Russian Federation (Russia); and\n\n(6) Venezuelan politician Nicolás Maduro (Maduro Regime).\n\n(b) The Secretary's determination of foreign adversaries is solely for the purposes of the Executive Order, this rule, and any subsequent rule promulgated pursuant to the Executive Order. Pursuant to the Secretary's discretion, the list of foreign adversaries will be revised as determined to be necessary. Such revisions will be effective immediately upon publication in the Federal Register without prior notice or opportunity for public comment.\n\n(c) The Secretary's determination is based on multiple sources, including:\n\n(1) National Security Strategy of the United States;\n\n(2) The Director of National Intelligence's 2016-2019 Worldwide Threat Assessments of the U.S. Intelligence Community;\n\n(3) The 2018 National Cyber Strategy of the United States of America; and\n\n(4) Reports and assessments from the U.S. Intelligence Community, the U.S. Departments of Justice, State and Homeland Security, and other relevant sources.\n\n(d) (d) The Secretary will periodically review this list in consultation with appropriate agency heads and may add to, subtract from, supplement, or otherwise amend this list. Any amendment to this list will apply to any ICTS Transaction that is initiated, pending, or completed on or after the date that the list is amended.", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Harms: Harm to infrastructure", "Harms: Ecological harm", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Convening", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines", "Incentives: Imprisonment", "Applications: Transportation", "Applications: Government: military and public safety"], "source": "https://www.federalregister.gov/documents/2021/01/19/2021-01234/securing-the-information-and-communications-technology-and-services-supply-chain", "official_name": "15 CFR part 7 (\"Securing the Information and Communications Technology and Services Supply Chain\")", "label": "safe"}
{"id": "181_7", "doc_id": "181", "text": "§ 7.5 Effect on other laws.\n\nNothing in this part shall be construed as altering or affecting any other authority, process, regulation, investigation, enforcement measure, or review provided by or established under any other provision of Federal law, including prohibitions under the National Defense Authorization Act of 2019, the Federal Acquisition Regulations, or IEEPA, or any other authority of the President or the Congress under the Constitution of the United States.\n\n§ 7.6 Amendment, modification, or revocation.\n\nExcept as otherwise provided by law, any determinations, prohibitions, or decisions issued under this part may be amended, modified, or revoked, in whole or in part, at any time.\n\n§ 7.7 Public disclosure of records.\n\nPublic requests for agency records related to this part will be processed in accordance with the Department of Commerce's Freedom of Information Act regulations, 15 CFR part 4, or other applicable law and regulation.", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Harms: Harm to infrastructure", "Harms: Ecological harm", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Convening", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines", "Incentives: Imprisonment", "Applications: Transportation", "Applications: Government: military and public safety"], "source": "https://www.federalregister.gov/documents/2021/01/19/2021-01234/securing-the-information-and-communications-technology-and-services-supply-chain", "official_name": "15 CFR part 7 (\"Securing the Information and Communications Technology and Services Supply Chain\")", "label": "safe"}
{"id": "181_8", "doc_id": "181", "text": "Subpart B—Review of ICTS Transactions\n\n§ 7.100 General.\n\nIn implementing this part, the Secretary of Commerce may:\n\n(a) Consider any and all relevant information held by, or otherwise made available to, the Federal Government that is not otherwise restricted by law for use for this purpose, including:\n\n(1) Publicly available information;\n\n(2) Confidential business information, as defined in 19 CFR 201.6, or proprietary information;\n\n(3) Classified National Security Information, as defined in Executive Order 13526 (December 29, 2009) and its predecessor executive orders, and Controlled Unclassified Information, as defined in Executive Order 13556 (November 4, 2010);\n\n(4) Information obtained from state, local, tribal, or foreign governments or authorities;\n\n(5) Information obtained from parties to a transaction, including records related to such transaction that any party uses, processes, or retains, or would be expected to use, process, or retain, in their ordinary course of business for such a transaction;\n\n(6) Information obtained through the authority granted under sections 2(a) and (c) of the Executive Order and IEEPA, as set forth in U.S.C. 7.101;\n\n(7) Information provided by any other U.S. Government national security body, in each case only to the extent necessary for national security purposes, and subject to applicable confidentiality and classification requirements, including the Committee for the Assessment of Foreign Participation in the United States Telecommunications Services Sector and the Federal Acquisitions Security Council and its designated information-sharing bodies; and\n\n(8) Information provided by any other U.S. Government agency, department, or other regulatory body, including the Federal Communications Commission, Department of Homeland Security, and Department of Justice;\n\n(b) Consolidate the review of any ICTS Transactions with other transactions already under review where the Secretary determines that the transactions raise the same or similar issues, or that are otherwise properly consolidated;", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Harms: Harm to infrastructure", "Harms: Ecological harm", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Convening", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines", "Incentives: Imprisonment", "Applications: Transportation", "Applications: Government: military and public safety"], "source": "https://www.federalregister.gov/documents/2021/01/19/2021-01234/securing-the-information-and-communications-technology-and-services-supply-chain", "official_name": "15 CFR part 7 (\"Securing the Information and Communications Technology and Services Supply Chain\")", "label": "safe"}
{"id": "181_9", "doc_id": "181", "text": "(c) In consultation with the appropriate agency heads, in determining whether an ICTS Transaction involves ICTS designed, developed, manufactured, or supplied, by persons owned by, controlled by, or subject to the jurisdiction or direction of a foreign adversary, consider the following:\n\n(1) Whether the person or its suppliers have headquarters, research, development, manufacturing, test, distribution, or service facilities, or other operations in a foreign country, including one controlled by, or subject to the jurisdiction of, a foreign adversary;\n\n(2) Ties between the person—including its officers, directors or similar officials, employees, consultants, or contractors—and a foreign adversary;\n\n(3) Laws and regulations of any foreign adversary in which the person is headquartered or conducts operations, including exploratory work, manufacturing, packaging, and distribution; and\n\n(4) Any other criteria that the Secretary deems appropriate;\n\n(d) In consultation with the appropriate agency heads, in determining whether an ICTS Transaction poses an undue or unacceptable risk, consider the following:\n\n(1) Threat assessments and reports prepared by the Director of National Intelligence pursuant to section 5(a) of the Executive Order;\n\n(2) Removal or exclusion orders issued by the Secretary of Homeland Security, the Secretary of Defense, or the Director of National Intelligence (or their designee) pursuant to recommendations of the Federal Acquisition Security Council, under 41 U.S.C. 1323;\n\n(3) Relevant provisions of the Defense Federal Acquisition Regulation (48 CFR ch. 2) and the Federal Acquisition Regulation (48 CFR ch. 1), and their respective supplements;\n\n(4) The written assessment produced pursuant to section 5(b) of the Executive Order, as well as the entities, hardware, software, and services that present vulnerabilities in the United States as determined by the Secretary of Homeland Security pursuant to that section;\n\n(5) Actual and potential threats to execution of a “National Critical Function” identified by the Department of Homeland Security Cybersecurity and Infrastructure Security Agency;\n\n(6) The nature, degree, and likelihood of consequence to the United States public and private sectors that could occur if ICTS vulnerabilities were to be exploited; and\n\n(7) Any other source or information that the Secretary deems appropriate; and\n\n(e) In the event the Secretary finds that unusual and extraordinary harm to the national security of the United States is likely to occur if all of the procedures specified herein are followed, the Secretary may deviate from these procedures in a manner tailored to protect against that harm.", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Harms: Harm to infrastructure", "Harms: Ecological harm", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Convening", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines", "Incentives: Imprisonment", "Applications: Transportation", "Applications: Government: military and public safety"], "source": "https://www.federalregister.gov/documents/2021/01/19/2021-01234/securing-the-information-and-communications-technology-and-services-supply-chain", "official_name": "15 CFR part 7 (\"Securing the Information and Communications Technology and Services Supply Chain\")", "label": "safe"}
{"id": "181_10", "doc_id": "181", "text": "§ 7.101 Information to be furnished on demand.\n\n(a) Pursuant to the authority granted to the Secretary under sections 2(a), 2(b), and 2(c) of the Executive Order and IEEPA, persons involved in an ICTS Transaction may be required to furnish under oath, in the form of reports or otherwise, at any time as may be required by the Secretary, complete information relative to any act or transaction, subject to the provisions of this part. The Secretary may require that such reports include the production of any books, contracts, letters, papers, or other hard copy or electronic documents relating to any such act, transaction, or property, in the custody or control of the persons required to make such reports. Reports with respect to transactions may be required either before, during, or after such transactions. The Secretary may, through any person or agency, conduct investigations, hold hearings, administer oaths, examine witnesses, receive evidence, take depositions, and require by subpoena the attendance and testimony of witnesses and the production of any books, contracts, letters, papers, and other hard copy or documents relating to any matter under investigation, regardless of whether any report has been required or filed in connection therewith.\n\n(b) For purposes of paragraph (a) of this section, the term “document” includes any written, recorded, or graphic matter or other means of preserving thought or expression (including in electronic format), and all tangible things stored in any medium from which information can be processed, transcribed, or obtained directly or indirectly, including correspondence, memoranda, notes, messages, contemporaneous communications such as text and instant messages, letters, emails, spreadsheets, metadata, contracts, bulletins, diaries, chronological data, minutes, books, reports, examinations, charts, ledgers, books of account, invoices, air waybills, bills of lading, worksheets, receipts, printouts, papers, schedules, affidavits, presentations, transcripts, surveys, graphic representations of any kind, drawings, photographs, graphs, video or sound recordings, and motion pictures or other film.\n\n(c) Persons providing documents to the Secretary pursuant to this section must produce documents in a format useable to the Department of Commerce, which may be detailed in the request for documents or otherwise agreed to by the parties.", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Harms: Harm to infrastructure", "Harms: Ecological harm", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Convening", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines", "Incentives: Imprisonment", "Applications: Transportation", "Applications: Government: military and public safety"], "source": "https://www.federalregister.gov/documents/2021/01/19/2021-01234/securing-the-information-and-communications-technology-and-services-supply-chain", "official_name": "15 CFR part 7 (\"Securing the Information and Communications Technology and Services Supply Chain\")", "label": "safe"}
{"id": "181_11", "doc_id": "181", "text": "§ 7.102 Confidentiality of information.\n\n(a) Information or documentary materials, not otherwise publicly or commercially available, submitted or filed with the Secretary under this part will not be released publicly except to the extent required by law.\n\n(b) The Secretary may disclose information or documentary materials that are not otherwise publicly or commercially available and referenced in paragraph (a) in the following circumstances:\n\n(1) Pursuant to any administrative or judicial proceeding;\n\n(2) Pursuant to an act of Congress;\n\n(3) Pursuant to a request from any duly authorized committee or subcommittee of Congress;\n\n(4) Pursuant to any domestic governmental entity, or to any foreign governmental entity of a United States ally or partner, information or documentary materials, not otherwise publicly or commercially available and important to the national security analysis or actions of the Secretary, but only to the extent necessary for national security purposes, and subject to appropriate confidentiality and classification requirements;\n\n(5) Where the parties or a party to a transaction have consented, the information or documentary material that are not otherwise publicly or commercially available may be disclosed to third parties; and\n\n(6) Any other purpose authorized by law.\n\n(c) This section shall continue to apply with respect to information and documentary materials that are not otherwise publicly or commercially available and submitted to or obtained by the Secretary even after the Secretary issues a final determination pursuant to § 7.109 of this part.\n\n(d) The provisions of 18 U.S.C. 1905, relating to charges and imprisonment and other consequences, shall apply with respect to the disclosure of information or documentary material provided to the Secretary under these regulations.", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Harms: Harm to infrastructure", "Harms: Ecological harm", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Convening", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines", "Incentives: Imprisonment", "Applications: Transportation", "Applications: Government: military and public safety"], "source": "https://www.federalregister.gov/documents/2021/01/19/2021-01234/securing-the-information-and-communications-technology-and-services-supply-chain", "official_name": "15 CFR part 7 (\"Securing the Information and Communications Technology and Services Supply Chain\")", "label": "safe"}
{"id": "181_12", "doc_id": "181", "text": "§ 7.103 Initial review of ICTS Transactions.\n\n(a) Upon receipt of any information identified in § 7.100(a), upon written request of an appropriate agency head, or at the Secretary's discretion, the Secretary may consider any referral for review of a transaction (referral).\n\n(b) In considering a referral pursuant to paragraph (a), the Secretary shall assess whether the referral falls within the scope of § 7.3(a) of this part and involves ICTS designed, developed, manufactured, or supplied by persons owned by, controlled by, or subject to the jurisdiction or direction of a foreign adversary, and determine whether to:\n\n(1) Accept the referral and commence an initial review of the transaction;\n\n(2) Request additional information, as identified in § 7.100(a), from the referring entity regarding the referral; or\n\n(3) Reject the referral.\n\n(c) Upon accepting a referral pursuant to paragraph (b) of this section, the Secretary shall conduct an initial review of the ICTS Transaction and assess whether the ICTS Transaction poses an undue or unacceptable risk, which may be determined by evaluating the following criteria:\n\n(1) The nature and characteristics of the information and communications technology or services at issue in the ICTS Transaction, including technical capabilities, applications, and market share considerations;\n\n(2) The nature and degree of the ownership, control, direction, or jurisdiction exercised by the foreign adversary over the design, development, manufacture, or supply at issue in the ICTS Transaction;\n\n(3) The statements and actions of the foreign adversary at issue in the ICTS Transaction;\n\n(4) The statements and actions of the persons involved in the design, development, manufacture, or supply at issue in the ICTS Transaction;\n\n(5) The statements and actions of the parties to the ICTS Transaction;\n\n(6) Whether the ICTS Transaction poses a discrete or persistent threat;\n\n(7) The nature of the vulnerability implicated by the ICTS Transaction;\n\n(8) Whether there is an ability to otherwise mitigate the risks posed by the ICTS Transaction;\n\n(9) The severity of the harm posed by the ICTS Transaction on at least one of the following:\n\n(i) Health, safety, and security;\n\n(ii) Critical infrastructure;\n\n(iii) Sensitive data;\n\n(iv) The economy;\n\n(v) Foreign policy;\n\n(vi) The natural environment; and\n\n(vii) National Essential Functions (as defined by Federal Continuity Directive-2 (FCD-2)); and\n\n(10) The likelihood that the ICTS Transaction will in fact cause threatened harm.\n\n(d) If the Secretary finds that an ICTS Transaction does not meet the criteria of paragraph (b) of this section:\n\n(1) The transaction shall no longer be under review; and\n\n(2) Future review of the transaction must avoid be precluded, where additional information becomes available to the Secretary.", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Harms: Harm to infrastructure", "Harms: Ecological harm", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Convening", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines", "Incentives: Imprisonment", "Applications: Transportation", "Applications: Government: military and public safety"], "source": "https://www.federalregister.gov/documents/2021/01/19/2021-01234/securing-the-information-and-communications-technology-and-services-supply-chain", "official_name": "15 CFR part 7 (\"Securing the Information and Communications Technology and Services Supply Chain\")", "label": "safe"}
{"id": "181_13", "doc_id": "181", "text": "§ 7.104 First interagency consultation.\n\nUpon finding that an ICTS Transaction likely meets the criteria set forth in § 7.103(c) during the initial review under § 7.103, the Secretary shall notify the appropriate agency heads and, in consultation with them, shall determine whether the ICTS Transaction meets the criteria set forth in § 7.103(c).\n\n§ 7.105 Initial determination.\n\n(a) If, after the consultation required by § 7.104, the Secretary determines that the ICTS Transaction does not meet the criteria set forth in § 7.103(c):\n\n(1) The transaction shall no longer be under review; and\n\n(2) Future review of the transaction must avoid be precluded, where additional information becomes available to the Secretary.\n\n(b) If, after the consultation required by § 7.104, the Secretary determines that the ICTS Transaction meets the criteria set forth in § 7.103(c), the Secretary shall:\n\n(1) Make an initial written determination, which shall be dated and signed by the Secretary, that:\n\n(i) Explains why the ICTS Transaction meets the criteria set forth in § 7.103(c); and\n\n(ii) Sets forth whether the Secretary has initially determined to restrict the ICTS Transaction or to propose mitigation measures, by which the ICTS Transaction may be permitted; and\n\n(2) Notify the parties to the ICTS Transaction either through publication in the Federal Register or by serving a copy of the initial determination on the parties via registered U.S. mail, facsimile, and electronic transmission, or third-party commercial carrier, to an addressee's last known address or by personal delivery.\n\n(c) Notwithstanding the fact that the initial determination to restrict or propose mitigation measures on an ICTS Transaction may, in whole or in part, rely upon classified national security information, or sensitive but unclassified information, the initial determination will contain no classified national security information, nor reference thereto, and, at the Secretary's discretion, may not contain sensitive but unclassified information.", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Harms: Harm to infrastructure", "Harms: Ecological harm", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Convening", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines", "Incentives: Imprisonment", "Applications: Transportation", "Applications: Government: military and public safety"], "source": "https://www.federalregister.gov/documents/2021/01/19/2021-01234/securing-the-information-and-communications-technology-and-services-supply-chain", "official_name": "15 CFR part 7 (\"Securing the Information and Communications Technology and Services Supply Chain\")", "label": "safe"}
{"id": "181_14", "doc_id": "181", "text": "§ 7.106 Recordkeeping requirement.\n\nUpon notification that an ICTS Transaction is under review or that an initial determination concerning an ICTS Transaction has been made, a notified person must immediately take steps to retain any and all records relating to such transaction.\n\n§ 7.107 Procedures governing response and mitigation.\n\nWithin 30 days of service of the Secretary's notification pursuant to § 7.105, a party to an ICTS Transaction may respond to the Secretary's initial determination or assert that the circumstances resulting in the initial determination no longer apply, and thus seek to have the initial determination rescinded or mitigated pursuant to the following administrative procedures:\n\n(a) A party may submit arguments or evidence that the party believes establishes that insufficient basis exists for the initial determination, including any prohibition of the ICTS Transaction;\n\n(b) A party may propose remedial steps on the party's part, such as corporate reorganization, disgorgement of control of the foreign adversary, engagement of a adherence monitor, or similar steps, which the party believes would negate the basis for the initial determination;\n\n(c) Any submission must be made in writing;\n\n(d) A party responding to the Secretary's initial determination may request a meeting with the Department, and the Department may, at its discretion, agree or decline to conduct such meetings prior to making a final determination pursuant to § 7.109;\n\n(e) This rule creates no right in any person to obtain access to information in the possession of the U.S. Government that was considered in making the initial determination to restrict the ICTS Transaction, to include classified national security information or sensitive but unclassified information; and\n\n(f) (f) If the Department receives no response from the parties within 30 days after service of the initial determination to the parties, the Secretary may determine to issue a final determination without the need to engage in the consultation process provided in section 7.108 of this rule.", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Harms: Harm to infrastructure", "Harms: Ecological harm", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Convening", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines", "Incentives: Imprisonment", "Applications: Transportation", "Applications: Government: military and public safety"], "source": "https://www.federalregister.gov/documents/2021/01/19/2021-01234/securing-the-information-and-communications-technology-and-services-supply-chain", "official_name": "15 CFR part 7 (\"Securing the Information and Communications Technology and Services Supply Chain\")", "label": "safe"}
{"id": "181_15", "doc_id": "181", "text": "§ 7.108 Second interagency consultation.\n\n(a) Upon receipt of any submission by a party to an ICTS Transaction under § 7.107, the Secretary shall consider \n\nwhether and how any information provided—including proposed mitigation measures—affects an initial determination of whether the ICTS Transaction meets the criteria set forth in § 7.103(c).\n\n(b) After considering the effect of any submission by a party to an ICTS Transaction under § 7.107 consistent with paragraph (a), the Secretary shall consult with and seek the consensus of all appropriate agency heads prior to issuing a final determination as to whether the ICTS Transaction shall be restricted, not restricted, or permitted pursuant to the adoption of negotiated mitigation measures.\n\n(c) If consensus is unable to be reached, the Secretary shall notify the President of the Secretary's proposed final determination and any appropriate agency head's opposition thereto.\n\n(d) After receiving direction from the President regarding the Secretary's proposed final determination and any appropriate agency head's opposition thereto, the Secretary shall issue a final determination pursuant to § 7.109.", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Harms: Harm to infrastructure", "Harms: Ecological harm", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Convening", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines", "Incentives: Imprisonment", "Applications: Transportation", "Applications: Government: military and public safety"], "source": "https://www.federalregister.gov/documents/2021/01/19/2021-01234/securing-the-information-and-communications-technology-and-services-supply-chain", "official_name": "15 CFR part 7 (\"Securing the Information and Communications Technology and Services Supply Chain\")", "label": "safe"}
{"id": "181_16", "doc_id": "181", "text": "§ 7.109 Final determination.\n\n(a) For each transaction for which the Secretary issues an initial determination that an ICTS Transaction is restricted, the Secretary shall issue a final determination as to whether the ICTS Transaction is:\n\n(1) restricted;\n\n(2) Not restricted; or\n\n(3) Permitted, at the Secretary's discretion, pursuant to the adoption of negotiated mitigation measures.\n\n(b) Unless the Secretary determines in writing that additional time is necessary, the Secretary shall issue the final determination within 180 days of accepting a referral and commencing the initial review of the ICTS Transaction pursuant to § 7.103.\n\n(c) If the Secretary determines that an ICTS Transaction is restricted, the Secretary shall have the discretion to direct the least restrictive means necessary to tailor the prohibition to address the undue or unacceptable risk posed by the ICTS Transaction.\n\n(d) The final determination shall:\n\n(1) Be written, signed, and dated;\n\n(2) Describe the Secretary's determination;\n\n(3) Be unclassified and contain no reference to classified national security information;\n\n(4) Consider and address any information received from a party to the ICTS Transaction;\n\n(5) Direct, if applicable, the timing and manner of the cessation of the ICTS Transaction;\n\n(6) Explain, if applicable, that a final determination that the ICTS Transaction is not restricted does not preclude the future review of transactions related in any way to the ICTS Transaction;\n\n(7) Include, if applicable, a description of the mitigation measures agreed upon by the party or parties to the ICTS Transaction and the Secretary; and\n\n(8) State the consequences a party will face if it fails to comply fully with any mitigation agreement or direction, including violations of IEEPA, or other violations of law.\n\n(e) The written, signed, and dated final determination shall be sent to:\n\n(1) The parties to the ICTS Transaction via registered U.S. mail and electronic mail; and\n\n(2) The appropriate agency heads.\n\n(f) The results of final written determinations to restrict an ICTS Transaction shall be published in the \n\nFederal Register. The publication shall omit any confidential business information.\n\n§ 7.110 Classified national security information.\n\nIn any review of a determination made under this part, if the determination was based on classified national security information, such information may be submitted to the reviewing court ex parte and in camera.\n\nThis section does not confer or imply any right to review in any tribunal, judicial or otherwise.", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Harms: Harm to infrastructure", "Harms: Ecological harm", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Convening", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines", "Incentives: Imprisonment", "Applications: Transportation", "Applications: Government: military and public safety"], "source": "https://www.federalregister.gov/documents/2021/01/19/2021-01234/securing-the-information-and-communications-technology-and-services-supply-chain", "official_name": "15 CFR part 7 (\"Securing the Information and Communications Technology and Services Supply Chain\")", "label": "safe"}
{"id": "181_17", "doc_id": "181", "text": "Subpart C—Enforcement\n\n§ 7.200 consequences.\n\n(a) Maximum consequences.\n\n(1) Civil consequence.\n\nA civil consequence not to exceed the amount set forth in Section 206 of IEEPA, 50 U.S.C. 1705, may be imposed on any person who violates, attempts to violate, conspires to violate, or causes any knowing infraction of any final determination or direction issued pursuant to this part, including any infraction of a mitigation agreement issued or other condition imposed under this part. IEEPA provides for a maximum civil consequence not to exceed the greater of $250,000, subject to inflationary adjustment, or an amount that is twice the amount of the transaction that is the basis of the infraction with respect to which the consequence is imposed.\n\n(2) Criminal consequence.\n\nA person who willfully commits, willfully attempts to commit, or willfully conspires to commit, or aids and abets in the commission of a infraction of any final determination, direction, or mitigation agreement shall, upon conviction of a infraction of IEEPA, be fined not more than $1,000,000, or if a natural person, may be imprisoned for not more than 20 years, or both.\n\n(3) The Secretary may impose a civil consequence of not more than the maximum statutory consequence amount, which, when adjusted for inflation, is $307,922, or twice the amount of the transaction that is the basis of the infraction, per infraction on any person who violates any final determination, direction, or mitigation agreement issued pursuant to this part under IEEPA.\n\n(i) Notice of the consequence, including a written explanation of the penalized conduct specifying the laws and regulations allegedly violated and the amount of the proposed consequence, and notifying the recipient of a right to make a written petition within 30 days as to why a consequence should not be imposed, shall be served on the notified party or parties.\n\n(ii) The Secretary shall review any presentation and issue a final administrative decision within 30 days of receipt of the petition.\n\n(4) Any civil consequences authorized in this section may be recovered in a civil action brought by the United States in U.S. district court.\n\n(b) Adjustments to consequence amounts.\n\n(1) The civil consequences provided in IEEPA are subject to adjustment pursuant to the Federal Civil consequences Inflation Adjustment Act of 1990 (Pub. L. 101-410, as amended, 28 U.S.C. 2461 note).\n\n(2) The criminal consequences provided in IEEPA are subject to adjustment pursuant to 18 U.S.C. 3571.\n\n(c) The consequences available under this section are without prejudice to other consequences, civil or criminal, available under law. Attention is directed to 18 U.S.C. 1001, which provides that whoever, in any matter within the jurisdiction of any department or agency in the United States, knowingly and willfully falsifies, conceals, or covers up by any trick, scheme, or device a material fact, or makes any false, fictitious, or fraudulent statements or representations, or makes or uses any false writing or document knowing the same to contain any false, fictitious, or fraudulent statement or entry, shall be fined under title 18, United States Code, or imprisoned not more than 5 years, or both.", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Harms: Harm to infrastructure", "Harms: Ecological harm", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Convening", "Incentives: Criminal liability", "Incentives: Civil liability", "Incentives: Fines", "Incentives: Imprisonment", "Applications: Transportation", "Applications: Government: military and public safety"], "source": "https://www.federalregister.gov/documents/2021/01/19/2021-01234/securing-the-information-and-communications-technology-and-services-supply-chain", "official_name": "15 CFR part 7 (\"Securing the Information and Communications Technology and Services Supply Chain\")", "label": "safe"}
{"id": "1815_26", "doc_id": "1815", "text": "Sec. 551.106.  CIVIL consequence; INJUNCTION.  \n\n(a)  The attorney general may bring an action in the name of this state to restrain or enjoin the person from violating this chapter and seek injunctive relief.\n\n(b)  The attorney general may recover reasonable attorney's fees and other reasonable expenses incurred in investigating and bringing an action under this section.\n\n(c)  The attorney general may assess and collect an administrative charge against a developer or deployer who fails to timely cure a infraction or who breaches a written statement provided to the attorney general, other than those for a restricted use, of not less than $50,000 and not more than $100,000 per uncured infraction.\n\n(d)  The attorney general may assess and collect an administrative charge against a developer or deployer who fails to timely cure a infraction of a restricted use, or whose infraction is determined to be uncurable, of not less than $80,000 and not more than $200,000 per infraction.\n\n(e)  A developer or deployer who was found in infraction of and continues to operate with the provisions of this chapter shall be assessed an administrative charge of not less than $2,000 and not more than $40,000 per day.\n\n(f)  There is a rebuttable presumption that a developer, distributor, or deployer used reasonable care as required under this chapter if the developer, distributor, or deployer complied with their duties under Subchapter A.", "tags": ["Incentives: Civil liability", "Incentives: Fines"], "source": "https://capitol.texas.gov/BillLookup/Text.aspx?LegSess=89R&Bill=HB1709", "official_name": "Texas Responsible Artificial Intelligence Governance Act (HB 1709)", "label": "safe"}
{"id": "1815_27", "doc_id": "1815", "text": "Sec. 551.107.  ENFORCEMENT ACTIONS BY STATE AGENCIES.  \n\nA state agency may sanction an individual licensed, registered, or certified by that agency for violations of Subchapter B, including:\n(1)  the suspension, probation, or revocation of a license, registration, certificate, or other form of permission to engage in an activity; and\n(2)  monetary consequences up to $100,000.\n\nSec. 551.108.  CONSUMER RIGHTS AND REMEDIES.  \n\nA consumer may appeal a consequential decision made by a high-risk artificial intelligence system which has an adverse impact on their health, safety, or fundamental rights, and shall have the right to obtain from the deployer clear and meaningful explanations of the role of the high-risk artificial intelligence system in the decision-making procedure and the main elements of the decision taken.", "tags": ["Incentives: Fines", "Incentives: Access to business opportunities", "Harms: Harm to health/safety"], "source": "https://capitol.texas.gov/BillLookup/Text.aspx?LegSess=89R&Bill=HB1709", "official_name": "Texas Responsible Artificial Intelligence Governance Act (HB 1709)", "label": "safe"}
{"id": "1820_1", "doc_id": "1820", "text": "PART 732—STEPS FOR USING THE EAR\n\n1. The authority citation for part 732 continues to read as follows: \n\nAuthority: 50 U.S.C. 4801-4852; 50 U.S.C. 4601 et seq.;50 U.S.C. 1701 et seq.;E.O. 13026, 61 FR 58767, 3 CFR, 1996 Comp., p. 228; E.O. 13222, 66 FR 44025, 3 CFR, 2001 Comp., p. 783. \n\n2. Supplement No. 3 to part 732 is amended by adding paragraph 28 under “Red Flags” to read as follows: \n\nSupplement No. 3 to Part 732—BIS's “Know Your Customer” Guidance and Red Flags\n\n* * * * * \n\nRed Flags\n\n* * * * * \n\n28. You will be providing Infrastructure-as-a-Service (IaaS) products or services, or other computing products or services, to assist in training an AI model with model weights captured by ECCN 4E091 for an entity headquartered, or whose ultimate parent is headquartered, in any destination other than those listed in paragraph (a) of supplement no. 5 to part 740 of the EAR. Such assistance creates a substantial risk that such AI model weights, due to their digital nature, will be exported or reexported to a destination for which a license is required and, if a license is not obtained, that the IaaS provider will have aided and abetted in a infraction of the EAR. In such cases, the IaaS provider should inquire if the customer intends to export the model and if so, apply for a license as required or inform the customer of their obligation to do so prior to export.", "tags": ["Strategies: Input controls", "Strategies: Input controls: Compute circulation"], "source": "https://www.federalregister.gov/documents/2025/01/15/2025-00636/framework-for-artificial-intelligence-diffusion", "official_name": "Framework for Artificial Intelligence Diffusion", "label": "safe"}
{"id": "1826_6", "doc_id": "1826", "text": "§ 1551. Required documentation. 1. (a) Beginning on January first, two thousand twenty-seven, each developer of a high-risk artificial intelligence  decision  system  shall  use reasonable care to protect consumers from any known or reasonably foreseeable risks of algorithmic  discrimination  arising  from  the  intended  and contracted uses of a high-risk artificial intelligence  decision  system.  In  any  enforcement  action brought  on  or after such date by the attorney general pursuant to this article, there shall be a rebuttable presumption that a  developer  used reasonable care as required pursuant to this subdivision if:\n(i) the developer complied with the provisions of this section; and\n(ii)  an  independent  third  party identified by the attorney general pursuant to paragraph (b) of this subdivision and retained by the developer completed bias and governance reviews for the  high-risk  artificial intelligence decision system.\n\n\n(b) No later than January first, two thousand twenty-six, and at least annually thereafter, the attorney general shall:\n(i)  identify independent third parties who, in the attorney general's opinion, are qualified to complete bias and governance  reviews  for  the purposes of subparagraph (ii) of paragraph (a) of this subdivision; and\n(ii) publish a list of such independent third parties available on the attorney general's website.", "tags": ["Harms: Discrimination", "Risk factors: Bias", "Strategies: Evaluation", "Strategies: Evaluation: External auditing"], "source": "https://nyassembly.gov/leg/?bn=A00768&term=2025&Text=Y", "official_name": "New York 2025-A768", "label": "safe"}
{"id": "1826_10", "doc_id": "1826", "text": "§  1552. Risk management. 1. (a) Beginning on January first, two thousand twenty-seven, each deployer of a high-risk artificial  intelligence decision  system shall use reasonable care to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination.  In any  enforcement  action  brought  on or after said date by the attorney general pursuant to this article, there shall be a  rebuttable  presumption  that  a  deployer  of a high-risk artificial intelligence decision system used reasonable care as required pursuant to this subdivision if:\n(i) the deployer complied with the provisions of this section; and\n(ii) an independent third party identified  by  the  attorney  general  pursuant  to  paragraph  (b)  of  this  subdivision  and retained by the  deployer completed bias and governance reviews for the high-risk  artificial intelligence decision system.\n\n(b)  No  later  than  January first, two thousand twenty-seven, and at least annually thereafter, the attorney general shall:\n(i) identify the independent third parties who, in the attorney general's opinion, are qualified to complete bias and governance  reviews  for the  purposes of subparagraph (ii) of paragraph (a) of this subdivision;\nand\n(ii) make a list of such independent third parties  available  on  the attorney general's web site.", "tags": ["Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: External auditing"], "source": "https://nyassembly.gov/leg/?bn=A00768&term=2025&Text=Y", "official_name": "New York 2025-A768", "label": "safe"}
{"id": "1826_25", "doc_id": "1826", "text": "§  1556.  Enforcement.  1.  The  attorney general shall have exclusive authority to enforce the provisions of this article.\n\n\n2. Except as provided in subdivision six of this section,  during  the period beginning on January first, two thousand twenty-seven, and ending on January first, two thousand twenty-eight, the attorney general shall, prior  to initiating any action for a infraction of this section, issue a notice of infraction to the developer, deployer, or other person  if  the  attorney  general determines that it is possible to cure such infraction. If the developer, deployer, or other person fails to cure such infraction  within sixty days after receipt of such notice of infraction, the  attorney general may bring an action pursuant to this section.\n\n\n3. Except as provided in subdivision six of this section, beginning on January  first,  two thousand twenty-eight, the attorney general may, in determining whether to financial support allocation a developer, deployer, or other person  the opportunity  to  cure  a  infraction described in subdivision two of this section, consider:\n(a) the number of violations;\n(b) the size and complexity  of  the  developer,  deployer,  or  other person;\n(c)  the  nature  and  extent of the developer's, deployer's, or other person's business;\n(d) the substantial likelihood of injury to the public;\n(e) the safety of persons or property; and\n(f) whether such infraction was likely caused  by  human  or  technical error.\n \n4.  Nothing  in this article shall be construed as providing the basis for a private right of action for violations of the provisions  of  this  article.\n\n\n5.  Except  as provided in subdivisions one, two, three, four, and six of this section, a infraction of the  requirements  established  in  this article  shall  constitute  an  unfair  trade  practice  for purposes of section three hundred forty-nine of this chapter and shall  be  enforced solely  by the attorney general; provided, however, that subdivision (h) of section three hundred forty-nine of this chapter must avoid  apply  to any such infraction.", "tags": ["Incentives: Civil liability"], "source": "https://nyassembly.gov/leg/?bn=A00768&term=2025&Text=Y", "official_name": "New York 2025-A768", "label": "safe"}
{"id": "1826_26", "doc_id": "1826", "text": "6.  (a)  In  any  action  commenced  by  the  attorney general for any infraction of this article, it shall be an affirmative defense  that  the developer, deployer, or other person:\n(i)  discovers  a  infraction  of any provision of this article through red-teaming;\n(ii) no later than sixty days after discovering such infraction through red-teaming:\n(A) cures such infraction; and\n(B) provides to the attorney general, in a form and manner  prescribed  by  the  attorney general, notice that such infraction has been cured and evidence that any harm caused by such infraction has been mitigated; and\n(iii) is otherwise in adherence with the latest version of:\n (A) the Artificial Intelligence Risk Management Framework published by the national institute of standards and technology;\n(B) ISO/IEC 42001 of the  international  organization  for  standardization and the international electrotechnical commission;\n(C)  a nationally or internationally recognized risk management frame-work for artificial intelligence decision systems, other than  the  risk management  frameworks described in clauses (A) and (B) of this subparagraph, that imposes requirements that are substantially  equivalent  to, and  at  least as stringent as, the requirements established pursuant to this article; or\n(D) any risk management framework for artificial intelligence decision systems that is substantially equivalent to, and at least  as  stringent  as,  the  risk  management frameworks described in clauses (A), (B), and (C) of this subparagraph.\n\n\n(b) The developer, deployer, or  other  person  bears  the  burden  of demonstrating  to the attorney general that the requirements established pursuant to paragraph (a) of this subdivision have been satisfied.\n\n\n(c) Nothing in this  article,  including,  but  not  limited  to,  the enforcement  authority  granted to the attorney general pursuant to this section, shall be construed to preempt or otherwise  affect  any  right, claim,  remedy,  presumption,  or defense available at law or in equity. Any rebuttable presumption or affirmative defense  established  pursuant to this article shall apply only to an enforcement action brought by the attorney  general  pursuant  to  this section and must avoid apply to any  right, claim, remedy, presumption, or defense available  at  law  or  in equity.", "tags": ["Harms: Discrimination", "Incentives: Civil liability"], "source": "https://nyassembly.gov/leg/?bn=A00768&term=2025&Text=Y", "official_name": "New York 2025-A768", "label": "safe"}
{"id": "1828_5", "doc_id": "1828", "text": "\"Intentional and substantial modification\" means any deliberate change made to (i) an artificial intelligence system that results, at the time when the change is implemented and any time thereafter, in any new material risk of algorithmic discrimination or (ii) a general-purpose artificial intelligence model that affects adherence of the general-purpose artificial intelligence model, materially changes the purpose of the general-purpose artificial intelligence model, or results in any new reasonably foreseeable risk of algorithmic discrimination. \"Intentional and substantial modification\" does not include (a) any customization made by deployers that (1) is based on legitimate nondiscriminatory business justifications, (2) is within the scope and purpose of the artificial intelligence tool, and (3) that does not result in a material change to the risks of algorithmic discrimination or (b) any change made to a high-risk artificial intelligence system, or the performance of a high-risk artificial intelligence system, if (1) the high-risk artificial intelligence system continues to learn after such high-risk artificial intelligence system is offered, sold, leased, licensed, given, or otherwise made available to a deployer, or deployed, and (2) such change (A) is made to such high-risk artificial intelligence system as a result of any learning described in clause (b) (1) and (B) was predetermined by the deployer or the third party contracted by the deployer and included within the initial impact assessment of such high-risk artificial intelligence system as required in § 59.1-609.\n\n\"Machine learning\" means the development of algorithms to build data-derived statistical models that are capable of drawing inferences from previously unseen data without explicit human instruction.\n\n\"Person\" includes any individual, corporation, partnership, association, cooperative, limited responsibility company, trust, joint venture, or any other legal or commercial entity and any successor, representative, agent, agency, or instrumentality thereof. \"Person\" does not include any government or political subdivision.\n\n\"Principal basis\" means the use of an output of a high-risk artificial intelligence system to make a decision without (i) human review, oversight, involvement, or intervention or (ii) meaningful consideration by a human.\n\n\"Red-teaming\" means adversarial testing to identify the potential adverse behaviors or outcomes of an artificial intelligence system, identify how such behaviors or outcomes occur, and stress test the safeguards against such behaviors or outcomes.", "tags": ["Harms: Discrimination", "Risk factors: Bias", "Risk factors: Reliability", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing"], "source": "https://lis.virginia.gov/bill-details/20251/HB2094/text/HB2094", "official_name": "Virginia 2025-HB2094", "label": "safe"}
{"id": "1828_9", "doc_id": "1828", "text": "H. 1. Each developer of a high-risk generative artificial intelligence system that generates or substantially modifies synthetic content shall ensure that the outputs of such high-risk artificial intelligence system (i) are identifiable and detectable in a manner that is accessible by consumers using industry-standard tools or tools provided by the developer; (ii) comply with any applicable accessibility requirements, as synthetic content, to the extent reasonably feasible; and (iii) apply such identification at the time the output is generated;\n\n2. If such synthetic content is an audio, image, or video format that forms part of an evidently artistic, creative, satirical, fictional analogous work or program, such requirement for identifying outputs of high-risk artificial intelligence systems pursuant to subdivision 1 shall be limited to a manner that does not hinder the display or enjoyment of such work or program.\n\n3. The identification of outputs required by subdivision 1 must avoid apply to (i) synthetic content that consists exclusively of text, is published to inform the public on any matter of public interest, or is unlikely to mislead a reasonable person consuming such synthetic content or (ii) the outputs of a high-risk artificial intelligence system that performs an assistive function for standard editing, does not substantially alter the input data provided by the developer, or is used to detect, prevent, investigate, or prosecute any crime as authorized by law.\n\nI. Where multiple developers directly contribute to the development of a high-risk artificial intelligence system, each developer shall be subject to the obligations and operating standards applicable to developers pursuant to this section solely with respect to its activities contributing to the development of the high-risk artificial intelligence system.", "tags": ["Risk factors: Transparency", "Strategies: Performance requirements", "Harms: Detrimental content", "Strategies: Tiering", "Strategies: Tiering: Tiering based on impact"], "source": "https://lis.virginia.gov/bill-details/20251/HB2094/text/HB2094", "official_name": "Virginia 2025-HB2094", "label": "safe"}
{"id": "1828_16", "doc_id": "1828", "text": "§ 59.1-611. Enforcement; civil consequences.\n\nA. The Attorney General shall have exclusive authority to enforce the provisions of this chapter.\n\nB. Whenever the Attorney General has reasonable cause to believe that any person has engaged in or is engaging in any infraction of this chapter, the Attorney General is empowered to issue a civil investigative demand. The provisions of § 59.1-9.10 shall apply mutatis mutandis to civil investigative demands issued   pursuant to this section. In rendering and furnishing any information requested pursuant to a civil investigative demand issued pursuant to this section, a developer or deployer may redact or omit any trade secrets or information protected from disclosure by state or federal law. If a developer or deployer refuses to disclose, redacts, or omits information based on the exception from disclosure of trade secrets, such developer or deployer shall affirmatively state to the Attorney General that the basis for nondisclosure, redaction, or omission is because such information is a trade secret. To the extent that any information requested pursuant to a civil investigative demand issued pursuant to this section is subject to attorney-client privilege or work-product protection, disclosure of such information pursuant to the civil investigative demand must avoid constitute a waiver of such privilege or protection. Any information, statement, or documentation provided to the Attorney General pursuant to this section shall be excused from disclosure under the Virginia Freedom of Information Act (§ 2.2-3700 et seq.).\n\nC. Notwithstanding any contrary provision of law, the Attorney General may cause an action to be brought in the appropriate circuit court in the name of the Commonwealth to enjoin any infraction of this chapter. The circuit court having jurisdiction may enjoin such infraction notwithstanding the existence of an adequate alternative remedy at law.\n\nD. Any person who violates the provisions of this chapter shall be subject to a civil consequence in an amount not to exceed $1,000 plus reasonable attorney fees, expenses, and costs, as determined by the court. Any person who willfully violates the provisions of this chapter shall be subject to a civil consequence in an amount not less than $1,000 and not more than $10,000 plus reasonable attorney fees, expenses, and costs, as determined by the court. Such civil consequences shall be paid into the Literary support pool.\n\nE. Each infraction of this chapter shall constitute a separate infraction and shall be subject to any civil consequences imposed under this section.", "tags": ["Incentives: Civil liability", "Incentives: Fines"], "source": "https://lis.virginia.gov/bill-details/20251/HB2094/text/HB2094", "official_name": "Virginia 2025-HB2094", "label": "safe"}
{"id": "1829_3", "doc_id": "1829", "text": "Section 3. Deployer Responsibilities\n\n(a) Risk Management Policy: Deployers of high-risk AI systems must implement and maintain a risk management program that:\n\n(1) Identifies and mitigates known or foreseeable risks of algorithmic discrimination; (2) Aligns with industry standards, such as the National Institute of Standards and Technology (NIST) AI Risk Management Framework.\n\n(b) Impact Assessments:\n\n(1) Deployers must complete an annual impact assessment for each high-risk AI system, including: (i) The purpose and intended use of the system; (ii) Data categories used and outputs generated; (iii) Potential risks of discrimination and mitigation measures.\n\n(2) Impact assessments must be updated after any substantial modification to the system. State-provided templates for these assessments will be made available to reduce adherence burdens.\n\n(c) Consumer Protections: Deployers must:\n\n(1) Notify consumers when an AI system materially influences a consequential decision; (2) Provide consumers with: (i) The purpose of the system; (ii) An explanation of how the system influenced the decision; (iii) A process to appeal or correct adverse decisions.\n\n(d) Transparency: Deployers must publicly disclose the types of high-risk AI systems in use and their risk mitigation strategies.", "tags": ["Risk factors: Bias", "Strategies: Performance requirements", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Disclosure", "Risk factors: Transparency"], "source": "https://malegislature.gov/Bills/194/HD396", "official_name": "Massachusetts HD 396 (2025-2026)", "label": "safe"}
{"id": "1829_6", "doc_id": "1829", "text": "SECTION 6. Enforcement\n\n(a) Attorney General Authority: The Attorney General has exclusive authority to enforce this Chapter. Violations are deemed unfair or deceptive trade practices under Chapter 93A.\n\n(b) Affirmative Defense: A developer or deployer may defend against enforcement if:\n\n(1) They identify and remedy violations through testing, internal review, or consumer feedback; (2) They demonstrate adherence with recognized AI risk management standards.\n\n(c) No Private Right of Action: This Chapter does not create a private right of action for consumers.\n\nSECTION 7. Rulemaking Authority\n\nThe Attorney General may issue rules to: (1) Define documentation and impact assessment requirements (2) Set standards for risk management programs and consumer notifications; (3) Designate recognized AI risk management frameworks.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Financial loss", "Strategies: Disclosure", "Harms: Discrimination", "Risk factors: Bias", "Strategies: Performance requirements", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Disclosure", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Risk factors: Transparency", "Applications: Government: other applications/unspecified", "Strategies: New institution", "Strategies: Disclosure"], "source": "https://malegislature.gov/Bills/194/HD396", "official_name": "Massachusetts HD 396 (2025-2026)", "label": "safe"}
{"id": "183_1", "doc_id": "183", "text": "§ 892.2070 Medical image analyzer.\n\n(a) Identification.\n\nMedical image analyzers, including computer-assisted/aided detection (CADe) devices for mammography breast cancer, ultrasound breast lesions, radiograph lung nodules, and radiograph dental caries detection, is a prescription device that is intended to identify, mark, highlight, or in any other manner direct the clinicians' attention to portions of a radiology image that may reveal abnormalities during interpretation of patient radiology images by the clinicians. This device incorporates pattern recognition and data analysis capabilities and operates on previously acquired medical images. This device is not intended to replace the review by a qualified radiologist, and is not intended to be used for triage, or to recommend diagnosis.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Security: Cybersecurity", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2020/01/22/2020-00494/radiology-devices-reclassification-of-medical-image-analyzers", "official_name": "21 CFR § 892.2070 (\"Medical image analyzer\")", "label": "safe"}
{"id": "183_2", "doc_id": "183", "text": "(b) Classification.\n\nClass II (special controls). The special controls for this device are:\n\n(1) Design verification and validation must include:\n\n(i) A detailed description of the image analysis algorithms including a description of the algorithm inputs and outputs, each major component or block, and algorithm limitations.\n\n(ii) A detailed description of pre-specified performance testing methods and dataset(s) used to assess whether the device will improve reader performance as intended and to characterize the standalone device performance. Performance testing includes one or more standalone tests, side-by-side comparisons, or a reader study, as applicable.\n\n(iii) Results from performance testing that demonstrate that the device improves reader performance in the intended use population when used in accordance with the instructions for use. The performance assessment must be based on appropriate diagnostic accuracy measures (e.g., receiver operator characteristic plot, sensitivity, specificity, predictive value, and diagnostic likelihood ratio). The test dataset must contain a sufficient number of cases from important cohorts (e.g., subsets defined by clinically relevant confounders, effect modifiers, concomitant diseases, and subsets defined by image acquisition characteristics) such that the performance estimates and confidence intervals of the device for these individual subsets can be characterized for the intended use population and imaging equipment.\n\n(iv) Appropriate software documentation (e.g., device hazard analysis; software requirements specification document; software design specification document; traceability analysis; description of verification and validation activities including system level test protocol, pass/fail criteria, and results; and cybersecurity).", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Security: Cybersecurity", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2020/01/22/2020-00494/radiology-devices-reclassification-of-medical-image-analyzers", "official_name": "21 CFR § 892.2070 (\"Medical image analyzer\")", "label": "safe"}
{"id": "183_3", "doc_id": "183", "text": "(2) Labeling must include the following:\n\n(i) A detailed description of the patient population for which the device is indicated for use.\n\n(ii) A detailed description of the intended reading protocol.\n\n(iii) A detailed description of the intended user and user training that addresses appropriate reading protocols for the device.\n\n(iv) A detailed description of the device inputs and outputs.\n\n(v) A detailed description of compatible imaging hardware and imaging protocols.\n\n(vi) Discussion of warnings, precautions, and limitations must include situations in which the device may fail or may not operate at its expected performance level (e.g., poor image quality or for certain subpopulations), as applicable.\n\n(vii) Device operating instructions.\n\n(viii) A detailed summary of the performance testing, including: test methods, dataset characteristics, results, and a summary of sub-analyses on case distributions stratified by relevant confounders, such as lesion and organ characteristics, disease stages, and imaging equipment.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Security: Cybersecurity", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2020/01/22/2020-00494/radiology-devices-reclassification-of-medical-image-analyzers", "official_name": "21 CFR § 892.2070 (\"Medical image analyzer\")", "label": "safe"}
{"id": "1831_1", "doc_id": "1831", "text": "The wave of artificial intelligence (Al) is sweeping across the globe, actively generating technological dividends and exerting profound influence on global economic and social development as well as the progress of human civilization. At the same time, we are acutely aware that Al brings about unpredictable risks and complicated challenges. To seize this new round of development opportunities, we solemnly launch the Al Safety Commitments. Through industry self-regulation, we will leverage high-level safety and security measures to support high-quality development, and collaborate to promote the robust development of Al. We deeply understand that self-discipline commitments are key to earning the trust of the global community. Guided by the Commitments as our code of conduct, and subject to the oversight of all stakeholders, we will continuously improve and refine our approach. By doing so, we will ensure that the application of Al technologies always remains people-centered and aligned with the principle of Al for good.", "tags": ["Risk factors: Safety", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Reliability", "Risk factors: Safety", "Strategies: Tiering", "Applications: Education", "Applications: Medicine, life sciences and public health", "Applications: Finance and investment", "Strategies: Tiering: Tiering based on domain of application", "Risk factors: Security", "Strategies: Input controls", "Risk factors: Security", "Risk factors: Privacy", "Risk factors: Reliability", "Strategies: Input controls: Data use", "Risk factors: Security: Dissemination", "Strategies: Evaluation", "Risk factors: Security", "Strategies: Evaluation: Adversarial testing", "Risk factors: Transparency", "Strategies: Disclosure", "Risk factors: Interpretability and explainability", "Strategies: Disclosure: In standard form", "Risk factors: Security", "Risk factors: Bias", "Harms: Discrimination", "Strategies: Disclosure", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Evaluation"], "source": "https://mp.weixin.qq.com/s/s-XFKQCWhu0uye4opgb3Ng", "official_name": "Artificial Intelligence Safety Commitments (China AI Industry Alliance)", "label": "safe"}
{"id": "1831_3", "doc_id": "1831", "text": "Commitment II: Conduct safety and security testing for Al models to enhance the performance, safety, and reliability. Through dedicated simulation and red-teaming experts to rigorously test Al models prior to their release or update. For large models in particular, prioritize safety and reliability evaluations focusing on their general understanding, reasoning, and decision-making capabilities, as well as their performance in critical domains such as industry, education, healthcare, finance, and law.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Reliability", "Risk factors: Safety", "Strategies: Tiering", "Applications: Education", "Applications: Medicine, life sciences and public health", "Applications: Finance and investment", "Strategies: Tiering: Tiering based on domain of application", "Risk factors: Security"], "source": "https://mp.weixin.qq.com/s/s-XFKQCWhu0uye4opgb3Ng", "official_name": "Artificial Intelligence Safety Commitments (China AI Industry Alliance)", "label": "safe"}
{"id": "1831_4", "doc_id": "1831", "text": "Commitment III: Implement measures to safeguard the security of training data and operational data. Establish data security protection policies and deploy corresponding technical measures to detect and promptly address data poisoning incidents, ensuring the accuracy and reliability of training data. Encrypt operational data and enforce access controls to protect business secrets, user privacy, and user-uploaded database, ensuring access is restricted to authorized use only. Prevent unauthorized outputs by Al models, thereby safeguarding data security and privacy rights.", "tags": ["Strategies: Input controls", "Risk factors: Security", "Risk factors: Privacy", "Risk factors: Reliability", "Strategies: Input controls: Data use", "Risk factors: Security: Dissemination"], "source": "https://mp.weixin.qq.com/s/s-XFKQCWhu0uye4opgb3Ng", "official_name": "Artificial Intelligence Safety Commitments (China AI Industry Alliance)", "label": "safe"}
{"id": "1831_8", "doc_id": "1831", "text": "List of the first companies to sign the \"Artificial Intelligence Safety Commitments\":\n\nAlibaba (China) Co., Ltd.\nBeijing Baidu Network Technology Co., Ltd.\nBeijing Volcano Engine Technology Co., Ltd.\nBeijing Megvii Technology Co., Ltd.\nBeijing Zero One Everything Technology Co., Ltd.\nBeijing DeepQuest AI Basic Technology Research Co., Ltd.\nBeijing Zhipu Huazhang Technology Co., Ltd.\nFourth Paradigm (Beijing) Technology Co., Ltd.\nHuawei Technologies Co., Ltd.\niFlytek Co., Ltd.\nAnt Technology Group Co., Ltd.\n360 Security Technology Co., Ltd.\nShanghai SenseTime Intelligent Technology Co., Ltd.\nShanghai Xiyu Technology Co., Ltd.\nShenzhen Tencent Computer Systems Co., Ltd.\nChina Telecom Artificial Intelligence Technology Co., Ltd.\nChina Mobile Communications Group Co., Ltd.", "tags": ["Risk factors: Safety", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Reliability", "Risk factors: Safety", "Strategies: Tiering", "Applications: Education", "Applications: Medicine, life sciences and public health", "Applications: Finance and investment", "Strategies: Tiering: Tiering based on domain of application", "Risk factors: Security", "Strategies: Input controls", "Risk factors: Security", "Risk factors: Privacy", "Risk factors: Reliability", "Strategies: Input controls: Data use", "Risk factors: Security: Dissemination", "Strategies: Evaluation", "Risk factors: Security", "Strategies: Evaluation: Adversarial testing", "Risk factors: Transparency", "Strategies: Disclosure", "Risk factors: Interpretability and explainability", "Strategies: Disclosure: In standard form", "Risk factors: Security", "Risk factors: Bias", "Harms: Discrimination", "Strategies: Disclosure", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Evaluation"], "source": "https://mp.weixin.qq.com/s/s-XFKQCWhu0uye4opgb3Ng", "official_name": "Artificial Intelligence Safety Commitments (China AI Industry Alliance)", "label": "safe"}
{"id": "1962_2", "doc_id": "1962", "text": "Principle 2: Design your AI system for security as well as functionality and performance  \nPrimarily applies to: System Operators and Developers \n\n\n [OWASP 2024, MITRE 2024, WEF 2024, ENISA 2023, NCSC 2023, BSI1 2023, Cisco 2022, Microsoft 2022, G7 2023, HHS 2021, OpenAI2 2024, ASD 2023, ICO 2020].  \n\n\n2.1 As part of deciding whether to create an AI system, a System Operator and/or Developer shall conduct a thorough assessment that includes determining and documenting the business requirements and/or problem they are seeking to address, along with associated AI security risks and mitigation strategies.5  \n\n\n2.1.1 Where the Data Custodian is part of a Developer’s organisation, they shall be included in internal discussions when determining the requirements and data needs of an AI system.    \n\n\n2.2: Developers and System Operators shall ensure that AI systems are designed and implemented to withstand adversarial AI attacks, unexpected inputs and AI system failure. \n\n\n2.3 To support the process of preparing data, security auditing and incident response for an AI system, Developers shall document and create an review trail in relation to the AI system. This shall include the operation, and lifecycle management of models, datasets and prompts incorporated into the system. \n\n\n2.4 If a Developer or System Operator uses an external component, they shall conduct an AI security risk assessment and due diligence process in line with their existing software development processes, that assesses AI specific risks.6 \n\n\n2.5 Data Custodians shall ensure that the intended usage of the system is appropriate to the sensitivity of the data it was trained on as well as the controls intended to ensure the security of the data.   \n\n\n2.5.1 Organisations should ensure that employees are encouraged to proactively report and identify any potential security risks in AI systems and ensure appropriate safeguards are in place. \n\n\n2.6 Where the AI system will be interacting with other systems or data sources, (be they internal or external), Developers and System Operators shall ensure that the permissions granted to the AI system on other systems are only provided as required for functionality and are risk assessed.   \n\n\n2.7 If a Developer or System Operator chooses to work with an external provider, they shall undertake a due diligence assessment and should ensure that the provider is adhering to this Code of Practice.", "tags": ["Strategies: Evaluation", "Risk factors: Reliability", "Risk factors: Reliability: Robustness", "Risk factors: Security", "Strategies: Evaluation: Conformity assessment", "Strategies: Performance requirements"], "source": "https://www.gov.uk/government/publications/ai-cyber-security-code-of-practice/code-of-practice-for-the-cyber-security-of-ai#code-of-practice-principles", "official_name": "Code of Practice for the Cyber Security of AI", "label": "safe"}
{"id": "1962_3", "doc_id": "1962", "text": "Principle 3: Evaluate the threats and manage the risks to your AI system  \nPrimarily applies to: Developers and System Operators   \n\n\n[OWASP 2024, WEF 2024, Nvidia 2023, ENISA 2023, Google 2023, G7 2023, NCSC 2023, Deloitte 2023], MITRE, OWASP, NIST Risk Taxonomy, ISO 27001] \n\n\n3.1 Developers and System Operators shall analyse threats and manage security risks to their systems. Threat modelling should include regular reviews and updates and address AI-specific attacks, such as data poisoning, model inversion, and membership inference.  \n\n\n3.1.1 The threat modelling and risk management process shall be conducted to address any security risks that arise when a new setting or configuration option is implemented or updated at any stage of the AI lifecycle. \n\n\n3.1.2 Developers shall manage the security risks associated with AI models that provide superfluous functionalities, where increased functionality leads to increased risk. For example, where a multi-modal model is being used but only single modality is used for system function.  \n\n\n3.1.3 System Operators shall apply controls to risks identified through the analysis based on a range of considerations, including the cost of implementation in line with their corporate risk tolerance.   \n\n\n3.2 Where AI security threats are identified that cannot be resolved by Developers, this shall be communicated to System Operators so they can threat model their systems. System Operators shall communicate this information to End-users, so they are made aware of these threats. This communication should include detailed descriptions of the risks, potential impacts, and recommended actions to address or monitor these threats.    \n\n\n3.3 Where an external entity has responsibility for AI security risks identified within an organisations infrastructure, System Operators should attain assurance that these parties are able to address such risks.   \n\n\n3.4 Developers and System Operators should continuously monitor and review their system infrastructure according to risk appetite. It is important to recognise that a higher level of risk will remain in AI systems despite the application of controls to mitigate against them.", "tags": ["Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Risk factors: Security", "Strategies: Evaluation: Post-market monitoring"], "source": "https://www.gov.uk/government/publications/ai-cyber-security-code-of-practice/code-of-practice-for-the-cyber-security-of-ai#code-of-practice-principles", "official_name": "Code of Practice for the Cyber Security of AI", "label": "safe"}
{"id": "1962_4", "doc_id": "1962", "text": "Principle 4: Enable human responsibility for AI systems \nPrimarily applies to: Developers and System Operators  \n\n\n[OWASP 2024, MITRE 2024, BSI1 2023, Microsoft 2022]  \n\n\n4.1 When designing an AI system, Developers and/or System Operators should incorporate and maintain capabilities to enable human oversight.7  \n\n\n4.2 Developers should design systems to make it easy for humans to assess outputs that they are responsible for in said system (such as by ensuring that models outputs are explainable or interpretable).  \n\n\n4.3 Where human oversight is a risk control, Developers and/or System Operators shall design, develop, verify and maintain technical measures to reduce the risk through such oversight. \n\n\n4.4 Developers should verify that the security controls specified by the Data Custodian have been built into the system.   \n\n\n4.5 Developers and System Operators should make End-users aware of restricted use cases of the AI system.", "tags": ["Risk factors: Interpretability and explainability", "Strategies: Performance requirements", "Strategies: Disclosure"], "source": "https://www.gov.uk/government/publications/ai-cyber-security-code-of-practice/code-of-practice-for-the-cyber-security-of-ai#code-of-practice-principles", "official_name": "Code of Practice for the Cyber Security of AI", "label": "safe"}
{"id": "1962_5", "doc_id": "1962", "text": "Secure Development  \n\n\nPrinciple 5: Identify, track and protect your assets  \nPrimarily applies to: Developers, System Operators and Data Custodians \n\n\n[OWASP 2024, Nvidia 2023, NCSC 2023, BSI1 2023, Cisco 2022, Deloitte 2023, Amazon 2023, G7 2023, ICO 2020]  \n\n\n5.1 Developers, Data Custodians and System Operators shall maintain a comprehensive inventory of their assets (including their interdependencies/connectivity). \n\n\n5.2 As part of broader software security practices, Developers, Data Custodians and System Operators shall have processes and tools to track, authenticate, manage version control and secure their assets due to the increased complexities of AI specific assets.   \n\n\n5.3 System Operators shall develop and tailor their disaster recovery plans to account for specific attacks aimed at AI systems.  \n\n\n5.3.1 System Operators should ensure that a known good state can be restored.   \n\n\n5.4 Developers, System Operators, Data Custodians and End-users shall protect sensitive data, such as training or test data, against unauthorised access.   \n\n\n5.4.1 Developers, Data Custodians and System Operators shall apply checks and sanitisation to data and inputs when designing the model based on their access to said data and inputs and where those data and inputs are stored. This shall be repeated when model revisions are made in response to user feedback or continuous learning.  \n\n\n5.4.2 Where training data or model weights could be confidential, Developers shall put proportionate protections in place.", "tags": ["Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Privacy", "Strategies: Input controls: Data circulation", "Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://www.gov.uk/government/publications/ai-cyber-security-code-of-practice/code-of-practice-for-the-cyber-security-of-ai#code-of-practice-principles", "official_name": "Code of Practice for the Cyber Security of AI", "label": "safe"}
{"id": "1962_7", "doc_id": "1962", "text": "Principle 7: Secure your supply chain  \nPrimarily applies to: Developers, System Operators and Data Custodians \n\n\n[Software Bill of Materials (SBOM), CISA, OWASP 2024, NCSC 2023, Microsoft 2022, ASD 2023]  \n\n\n7.1 Developers and System Operators shall follow secure software supply chain processes for their AI model and system development.  \n\n\n7.2 System Operators that choose to use or adapt any models, or components, which are not well-documented or secured shall be able to justify their decision to use such models or components through documentation (for example if there was no other supplier for said component).  \n\n\n7.2.1 In this case, Developers and System Operators shall have mitigating controls and undertake a risk assessment linked to such models or components.  \n\n\n7.2.2 System Operators shall share this documentation with End-users in an accessible way. \n\n\n7.3 Developers and System Operators shall re-run evaluations on released models that they intend on using. \n\n\n7.4 System Operators shall communicate their intention to update models to End-users in an accessible way prior to models being updated.", "tags": ["Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Evaluation: Post-market monitoring", "Strategies: Input controls", "Strategies: Input controls: Compute use", "Strategies: Disclosure: About evaluation"], "source": "https://www.gov.uk/government/publications/ai-cyber-security-code-of-practice/code-of-practice-for-the-cyber-security-of-ai#code-of-practice-principles", "official_name": "Code of Practice for the Cyber Security of AI", "label": "safe"}
{"id": "1962_9", "doc_id": "1962", "text": "Principle 9: Conduct appropriate testing and evaluation  \nPrimarily applies to: Developers and System Operators \n\n\n[OWASP 2024, WEF 2024, Nvidia 2023, NCSC 2023, ENISA 2023, Google 2023, G7 2023]  \n\n\n9.1 Developers shall ensure that all models, applications and systems that are released to System Operators and/or End-users have been tested as part of a security assessment process.   \n\n\n9.2 System Operators shall conduct testing prior to the system being deployed with support from Developers.   \n\n\n9.2.1 For security testing, System Operators and Developers should use independent security testers with technical skills relevant to their AI systems. \n\n\n9.3 Developers should ensure that the findings from the testing and evaluation are shared with System Operators, to inform their own testing and evaluation.   \n\n\n9.4 Developers should evaluate model outputs to ensure they do not allow System Operators or End-users to reverse engineer non-public aspects of the model or the training data.  \n\n\n9.4.1 Additionally, Developers should evaluate model outputs to ensure they do not provide System Operators or End-users with unintended influence over the system.", "tags": ["Strategies: Performance requirements", "Strategies: Evaluation", "Risk factors: Security", "Strategies: Evaluation: External auditing", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Risk factors: Security: Cybersecurity"], "source": "https://www.gov.uk/government/publications/ai-cyber-security-code-of-practice/code-of-practice-for-the-cyber-security-of-ai#code-of-practice-principles", "official_name": "Code of Practice for the Cyber Security of AI", "label": "safe"}
{"id": "1962_12", "doc_id": "1962", "text": "Principle 12: Monitor your system’s behaviour  \nPrimarily applies to: Developers and System Operators  \n\n\n[OWASP 2024, WEF 2024, Nvidia 2023, ENISA 2023, BSI1 2023, Cisco 2022, Deloitte 2023, G7 2023, Amazon 2023, ICO 2020]  \n\n\n12.1 System Operators shall log system and user actions to support security adherence, incident investigations, and vulnerability remediation. \n\n\n12.2 System Operators should analyse their logs to ensure that AI models continue to produce desired outputs and to detect anomalies, security breaches, or unexpected behaviour over time (such as due to data drift or data poisoning). \n\n\n12.3 System Operators and Developers should monitor internal states of their AI systems where this could better enable them to address security threats, or to enable future security analytics.  \n\n\n12.4 System Operators and Developers should monitor the performance of their models and system over time so that they can detect sudden or gradual changes in behaviour that could affect security.", "tags": ["Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation", "Risk factors: Reliability", "Risk factors: Security"], "source": "https://www.gov.uk/government/publications/ai-cyber-security-code-of-practice/code-of-practice-for-the-cyber-security-of-ai#code-of-practice-principles", "official_name": "Code of Practice for the Cyber Security of AI", "label": "safe"}
{"id": "1970_1", "doc_id": "1970", "text": "[Preliminary material omitted. Most footnotes omitted.]\n\nSection 01\nIntroduction\n\n1.1\tScope\n\nIn line with the Frontier AI Safety Commitments, which Meta signed in May 2024, our Frontier AI Framework relates to our forthcoming most advanced models and systems that match or exceed the capabilities present in the most advanced models. It defines processes to manage and mitigate the risk of frontier AI models or systems producing catastrophic outcomes, and to keep risks of such outcomes within tolerable levels. This Framework is one component of our wider AI governance program. It deals with catastrophic outcomes that could arise as a direct result of the development or release of the frontier AI model. The Framework does not, therefore, reflect the full spectrum of risks that we assess for, nor all of the evaluations that we conduct.\n\nOur Framework is structured around a set of catastrophic outcomes. We have used threat modelling to develop threat scenarios pertaining to each of our catastrophic outcomes. We have identified the key capabilities that would enable the threat actor to realize a threat scenario. We have taken into account both state and non-state actors, and our threat scenarios distinguish between high- or low-skill actors.\n\nWe define our thresholds based on the extent to which frontier AI would uniquely enable the execution of any of the threat scenarios we have identified as being potentially sufficient to produce a catastrophic outcome. If a frontier AI is assessed to have reached the critical risk threshold and cannot be mitigated, we will stop development and implement the measures outlined in Table 1. Our high and moderate risk thresholds are defined in terms of the level of uplift a model provides towards realising a threat scenario. We will develop Frontier AI in line with the processes outlined in this Framework, and implement the measures outlined in Table 1. Section 3 on Outcomes & Thresholds provides more information about how we define our thresholds.\n \nThis is the first iteration of our Frontier AI Framework. We expect to update it in the future to reflect developments in both the technology and our understanding of how to manage its risks and benefits. Alongside updates to the Framework, we also identify areas that would benefit from further research and investment to improve our ability to continue to safely develop and release advanced AI models.", "tags": ["Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment"], "source": "https://ai.meta.com/static-resource/meta-frontier-ai-framework", "official_name": "Meta Frontier AI Framework (Version 1.1)", "label": "safe"}
{"id": "1970_4", "doc_id": "1970", "text": "2.1.2\tEvaluate and mitigate\n\nConduct an AI risk assessment:\nOur AI risk assessment process systematically evaluates potential risks associated with frontier AI, documenting mitigation strategies and residual risks across a set of applicable risk categories.\n\nThe risk assessment process involves multi-disciplinary engagement, including internal and, where appropriate, external experts from various disciplines (which could include engineering, product management, adherence and privacy, legal, and policy) and company leaders from multiple disciplines.\n\nThe risk assessment also considers the planned release (i.e. internal deployment, limited release, or full release), as this informs the type of pre-release evaluation we undertake.\n\nEvaluate for performance and safety:\nOur evaluations can involve a combination of automated and human evaluations, as well as red teaming and uplift studies. Throughout development, we monitor performance against our expectations for the reference class as well as the enabling capabilities we have identified in our threat scenarios, and use these indicators as triggers for further evaluations as capabilities develop.\n\nAI model evaluation is a nascent science, and as capabilities develop new evaluations are developed. As such, we do not have a fixed set of evaluations that we apply to each frontier AI. Rather, we implement relevant evaluations based on capabilities and the latest research.\n \nAs an example, once a model demonstrates a sufficient standard of coding ability, we would typically evaluate the potential of the model to present cybersecurity risks. While we expect that the appropriate evaluations for cybersecurity will change over time, we have developed and open sourced an evaluation called CyberSecEval that is designed for this purpose. For both cyber and chemical and biological risks, we conduct red teaming exercises once a model achieves certain levels of performance in capabilities relevant to these domains, involving external experts when appropriate.\n\nWe design our evaluations to account for how the model will be released, including assessing how its capabilities might be enhanced. See section 4.2 for more details.\n\nImplement mitigations:\nOur mitigation strategy is informed by the risks we've identified in the risk assessment, evaluation results, the mitigations that have been applied to existing models in the same class, and the release approach. Our Llama research papers provide more details on mitigations we have implemented for previous releases. Section 4 of this framework provides more details on mitigation techniques we employ.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Convening", "Strategies: Evaluation: Adversarial testing", "Strategies: Evaluation: Conformity assessment"], "source": "https://ai.meta.com/static-resource/meta-frontier-ai-framework", "official_name": "Meta Frontier AI Framework (Version 1.1)", "label": "safe"}
{"id": "1970_6", "doc_id": "1970", "text": "2.2\tTransparency\n\nOne of the major benefits of an open approach to AI exploratory work is that it provides a greater degree of transparency as to how a model works, which in turn can lead to a better understanding of, and trust in, AI. We see this as a key benefit of sharing model weights, as well as research papers and model cards.\n \nIn line with the processes set out in this Framework, we intend to continue to openly release models to the ecosystem. We also plan to continue sharing relevant information about how we develop and evaluate our models responsibly, including through artefacts like model cards and research papers, and by providing guidance to model deployers through resources like our Responsible Use Guides.\n\nIn addition to promoting accountability, open sourcing advanced models makes it possible for us to not only work with outside experts to improve our own evaluation of risk but also for the broader community to independently assess the capabilities of our models. Given the iterative nature of AI development, we believe that this will not only help improve the efficacy, safety, and trustworthiness of our models but improve the state of the art in risk evaluation more generally.", "tags": ["Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Convening", "Strategies: Evaluation: External auditing"], "source": "https://ai.meta.com/static-resource/meta-frontier-ai-framework", "official_name": "Meta Frontier AI Framework (Version 1.1)", "label": "safe"}
{"id": "1970_8", "doc_id": "1970", "text": "3.2\tThreat modelling\n\nThreat modelling is fundamental to our outcomes-led approach. We run threat modelling exercises both internally and with external experts with relevant domain expertise, where required. The goal of these exercises is to explore, in a systematic way, how frontier AI models might be used to produce catastrophic outcomes. Through this process, we develop threat scenarios' which describe how different actors might use a frontier AI model to realise a catastrophic outcome.\n \nWe design assessments to simulate whether our model would uniquely enable these scenarios, and identify the enabling capabilities the model would need to exhibit to do so. Our first set of evaluations are designed to identify whether all of these enabling capabilities are present, and if the model is sufficiently performant on them. If so, this would prompt further evaluation to understand whether the model could uniquely enable the threat scenario. See Section 4.2 for more detail.\n\nIt is important to note that the pathway to realise a catastrophic outcome is often extremely complex, involving numerous external elements beyond the frontier AI model. Our threat scenarios describe an essential part of the end-to-end pathway. By testing whether our model can uniquely enable a threat scenario, we're testing whether it uniquely enables that essential part of the pathway. If it does not, then we know that our model cannot be used to realize the catastrophic outcome, because this essential part is still a barrier. If it does and cannot be further mitigated, we assign the model to the critical threshold.\n\nThis would also trigger a new threat modelling exercise to develop additional threat scenarios along the causal pathway so that we can ascertain whether the catastrophic outcome is indeed realizable, or whether there are still barriers to realising the catastrophic outcome (see Section 5.1 for more detail).\n\nOur threat modelling is informed by our own internal experts' assessment of the catastrophic risks that frontier models might pose, as well as engagements with governments, external experts, and the wider AI community. However, there remains quite considerable divergence in expert opinion as to how AI capabilities will develop and the time horizons on which they could emerge.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Convening"], "source": "https://ai.meta.com/static-resource/meta-frontier-ai-framework", "official_name": "Meta Frontier AI Framework (Version 1.1)", "label": "safe"}
{"id": "1970_17", "doc_id": "1970", "text": "Our evaluations are designed to account for the deployment context of the model. This includes assessing whether risks will remain within defined thresholds once a model is deployed or released using the target release approach. For example, to help ensure that we are appropriately assessing the risk, we prepare the asset - the version of the model that we will test - in a way that seeks to account for the tools and scaffolding in the current ecosystem that a particular threat actor might seek to leverage to enhance the model's capabilities. We also account for enabling capabilities, such as automated AI exploratory work, that might increase the potential for enhancements to model capabilities.\n\nWe may take into account monetary costs as well as a threat actor's ability to overcome other barriers to misuse relevant to our threat scenarios such as access to compute, restricted materials, or lab facilities. If the results of our evaluations indicate that a frontier AI has a \"high\" risk threshold by providing significant uplift towards realization of a catastrophic outcome we will not release the frontier AI externally.\n\nModels that are not being considered for external release will undergo evaluation to assess the robustness of the mitigations we have implemented, which might include adversarial prompting, jailbreak attempts, and red teaming, amongst other techniques. This evaluation also will take into account the narrower availability of those models and the security measures in place to prevent unauthorized access.\n \nWe typically repeat evaluations as a frontier AI nears or completes training. Evaluation results also guide the mitigations and controls we implement. The full mitigation strategy will be informed by the risk assessment, the frontier AI's particular capabilities, and the release plans. Examples of mitigation techniques we implement include:\n•\tcharge-tuning\n•\tMisuse filtering, response protocols\n•\tSanctions screening and geogating\n•\tStaged release to prepare the external ecosystem", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Adversarial testing", "Risk factors: Reliability", "Risk factors: Reliability: Robustness"], "source": "https://ai.meta.com/static-resource/meta-frontier-ai-framework", "official_name": "Meta Frontier AI Framework (Version 1.1)", "label": "safe"}
{"id": "1974_1", "doc_id": "1974", "text": "California Attorney General’s Legal Advisory on the Application\nof Existing California Laws to Artificial Intelligence\nThe California Attorney General’s Office (AGO) issues this advisory to provide guidance to consumers and entities\nthat develop, sell, and use artificial intelligence (AI)1\n about their rights and obligations under California law, including\nunder the state’s consumer protection, civil rights, competition, and data privacy laws.2\n\n1 While the definition of AI may vary depending upon the context, for the purposes of this advisory, AI includes “a machinebased system that can, for a given set of human-defined objectives, make predictions, recommendations or decisions\ninfluencing real or virtual environments. Artificial intelligence systems use machine and human-based inputs to—(A)\nperceive real and virtual environments; (B) abstract such perceptions into models through analysis in an automated manner;\nand (C) use model inference to formulate options for information or action.” (15 U.S.C. § 9401(3).) California has also\nrecently passed a law defining the term in certain instances as “an engineered or machine-based system that varies in its\nlevel of autonomy and that can, for explicit or implicit objectives, infer from the input it receives how to generate outputs\nthat can influence physical or virtual environments.” (See Gov. Code § 11546.45.5 et seq., added by AB 2885, Stats. 2024, ch.\n843.)\n2 This advisory provides the AGO’s guidance on general application of California law to AI. This advisory does not address all\npotential violations or avenues of enforcement for the identified laws, nor does it identify all laws that may apply to AI.", "tags": ["Applications: Sales, retail, and customer relations", "Strategies: Disclosure", "Strategies: Disclosure: Accuracy thereof", "Harms: Detrimental content", "Harms: Financial loss", "Risk factors: Safety", "Harms: Harm to health/safety", "Applications: Sales, retail, and customer relations", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Bias", "Risk factors: Bias", "Applications: Finance and investment", "Harms: Discrimination", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security: Dissemination", "Strategies: Input controls: Data circulation", "Strategies: Input controls", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Security", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation", "Strategies: Input controls", "Strategies: Input controls: Data use", "Strategies: Input controls: Data circulation", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Applications: Education", "Applications: Medicine, life sciences and public health", "Strategies: Input controls: Data circulation", "Strategies: Input controls: Data use", "Applications: Sales, retail, and customer relations", "Strategies: Input controls", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Disclosure: In deployment", "Applications: Sales, retail, and customer relations", "Risk factors: Transparency", "Harms: Detrimental content", "Applications: Broadcasting and media production", "Incentives: Fines", "Incentives: Civil liability", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Broadcasting and media production", "Harms: Detrimental content", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Applications: Broadcasting and media production", "Incentives: Criminal liability", "Applications: Medicine, life sciences and public health", "Strategies: Performance requirements"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence.pdf", "official_name": "California Attorney General’s Legal Advisory on the Application of Existing California Laws to Artificial Intelligence", "label": "safe"}
{"id": "1974_2", "doc_id": "1974", "text": "Artificial Intelligence Holds Great Potential and Great Risks\nAI systems are at the forefront of the technology industry, and hold great potential to achieve scientific\nbreakthroughs, boost economic growth, and benefit consumers. As home to the world’s leading technology\ncompanies and many of the most compelling recent developments in AI, California has a vested interest in the\ndevelopment and growth of AI tools. The AGO encourages the responsible use of AI in ways that are safe, ethical,\nand consistent with human dignity to help solve urgent challenges, increase efficiencies, and unlock access to\ninformation—consistent with state and federal law.", "tags": ["Applications: Sales, retail, and customer relations", "Strategies: Disclosure", "Strategies: Disclosure: Accuracy thereof", "Harms: Detrimental content", "Harms: Financial loss", "Risk factors: Safety", "Harms: Harm to health/safety", "Applications: Sales, retail, and customer relations", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Bias", "Risk factors: Bias", "Applications: Finance and investment", "Harms: Discrimination", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security: Dissemination", "Strategies: Input controls: Data circulation", "Strategies: Input controls", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Security", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation", "Strategies: Input controls", "Strategies: Input controls: Data use", "Strategies: Input controls: Data circulation", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Applications: Education", "Applications: Medicine, life sciences and public health", "Strategies: Input controls: Data circulation", "Strategies: Input controls: Data use", "Applications: Sales, retail, and customer relations", "Strategies: Input controls", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Disclosure: In deployment", "Applications: Sales, retail, and customer relations", "Risk factors: Transparency", "Harms: Detrimental content", "Applications: Broadcasting and media production", "Incentives: Fines", "Incentives: Civil liability", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Broadcasting and media production", "Harms: Detrimental content", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Applications: Broadcasting and media production", "Incentives: Criminal liability", "Applications: Medicine, life sciences and public health", "Strategies: Performance requirements"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence.pdf", "official_name": "California Attorney General’s Legal Advisory on the Application of Existing California Laws to Artificial Intelligence", "label": "safe"}
{"id": "1974_3", "doc_id": "1974", "text": "While AI tools present new opportunities, the use of AI can run the risk of exacerbating bias, discrimination, and\nthe spread of disinformation, creating opportunities for fraud and causing harm to California’s people, institutions,\ninfrastructure, economy, and environment. For AI systems to achieve their positive potential without doing harm,\nthey must be developed and used ethically and legally. Existing California law provides a host of protections that\nmay be applicable to the development and use of AI tools.", "tags": ["Applications: Sales, retail, and customer relations", "Strategies: Disclosure", "Strategies: Disclosure: Accuracy thereof", "Harms: Detrimental content", "Harms: Financial loss", "Risk factors: Safety", "Harms: Harm to health/safety", "Applications: Sales, retail, and customer relations", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Bias", "Risk factors: Bias", "Applications: Finance and investment", "Harms: Discrimination", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security: Dissemination", "Strategies: Input controls: Data circulation", "Strategies: Input controls", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Security", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation", "Strategies: Input controls", "Strategies: Input controls: Data use", "Strategies: Input controls: Data circulation", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Applications: Education", "Applications: Medicine, life sciences and public health", "Strategies: Input controls: Data circulation", "Strategies: Input controls: Data use", "Applications: Sales, retail, and customer relations", "Strategies: Input controls", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Disclosure: In deployment", "Applications: Sales, retail, and customer relations", "Risk factors: Transparency", "Harms: Detrimental content", "Applications: Broadcasting and media production", "Incentives: Fines", "Incentives: Civil liability", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Broadcasting and media production", "Harms: Detrimental content", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Applications: Broadcasting and media production", "Incentives: Criminal liability", "Applications: Medicine, life sciences and public health", "Strategies: Performance requirements"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence.pdf", "official_name": "California Attorney General’s Legal Advisory on the Application of Existing California Laws to Artificial Intelligence", "label": "safe"}
{"id": "1974_4", "doc_id": "1974", "text": "Consumers must have visibility into when and how AI systems are used to impact their lives and whether and\nhow their information is being used to develop and train systems. Developers and entities that use AI, including\nbusinesses, nonprofits, and government, must ensure that AI systems are tested and validated, and that they are\naudited as appropriate to ensure that their use is safe, ethical, and lawful, and reduces, rather than replicates or\nexaggerates, human error and biases. Developers and users must understand any risks involved in the use of AI, and\nensure that AI is not used in a manner that causes harm to individuals, entities, infrastructure, competition, or the\nenvironment, or to the public at large.", "tags": ["Applications: Sales, retail, and customer relations", "Strategies: Disclosure", "Strategies: Disclosure: Accuracy thereof", "Harms: Detrimental content", "Harms: Financial loss", "Risk factors: Safety", "Harms: Harm to health/safety", "Applications: Sales, retail, and customer relations", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Bias", "Risk factors: Bias", "Applications: Finance and investment", "Harms: Discrimination", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security: Dissemination", "Strategies: Input controls: Data circulation", "Strategies: Input controls", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Security", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation", "Strategies: Input controls", "Strategies: Input controls: Data use", "Strategies: Input controls: Data circulation", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Applications: Education", "Applications: Medicine, life sciences and public health", "Strategies: Input controls: Data circulation", "Strategies: Input controls: Data use", "Applications: Sales, retail, and customer relations", "Strategies: Input controls", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Disclosure: In deployment", "Applications: Sales, retail, and customer relations", "Risk factors: Transparency", "Harms: Detrimental content", "Applications: Broadcasting and media production", "Incentives: Fines", "Incentives: Civil liability", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Broadcasting and media production", "Harms: Detrimental content", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Applications: Broadcasting and media production", "Incentives: Criminal liability", "Applications: Medicine, life sciences and public health", "Strategies: Performance requirements"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence.pdf", "official_name": "California Attorney General’s Legal Advisory on the Application of Existing California Laws to Artificial Intelligence", "label": "safe"}
{"id": "1974_5", "doc_id": "1974", "text": "AI systems are proliferating at an exponential rate and already affect nearly all aspects of everyday life. Businesses\nare using AI systems to evaluate consumers’ benefit risk and guide loan decisions, screen tenants for rentals, and\ntarget consumers with ads and offers. AI systems are also used in the workplace to guide employment decisions, in\neducational settings to provide new learning systems, and in healthcare settings to inform medical diagnoses. But\nmany consumers are not aware of when and how AI systems are used in their lives or by institutions that they rely\non. Moreover, AI systems are novel and complex, and their inner workings are often not understood by developers\nand entities that use AI, let alone consumers. The rapid deployment of such tools has resulted in situations where AI\ntools have generated false information or biased and discriminatory results, often while being represented as neutral\nand free from human bias.", "tags": ["Applications: Sales, retail, and customer relations", "Strategies: Disclosure", "Strategies: Disclosure: Accuracy thereof", "Harms: Detrimental content", "Harms: Financial loss", "Risk factors: Safety", "Harms: Harm to health/safety", "Applications: Sales, retail, and customer relations", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Bias", "Risk factors: Bias", "Applications: Finance and investment", "Harms: Discrimination", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security: Dissemination", "Strategies: Input controls: Data circulation", "Strategies: Input controls", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Security", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation", "Strategies: Input controls", "Strategies: Input controls: Data use", "Strategies: Input controls: Data circulation", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Applications: Education", "Applications: Medicine, life sciences and public health", "Strategies: Input controls: Data circulation", "Strategies: Input controls: Data use", "Applications: Sales, retail, and customer relations", "Strategies: Input controls", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Disclosure: In deployment", "Applications: Sales, retail, and customer relations", "Risk factors: Transparency", "Harms: Detrimental content", "Applications: Broadcasting and media production", "Incentives: Fines", "Incentives: Civil liability", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Broadcasting and media production", "Harms: Detrimental content", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Applications: Broadcasting and media production", "Incentives: Criminal liability", "Applications: Medicine, life sciences and public health", "Strategies: Performance requirements"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence.pdf", "official_name": "California Attorney General’s Legal Advisory on the Application of Existing California Laws to Artificial Intelligence", "label": "safe"}
{"id": "1974_6", "doc_id": "1974", "text": "Entities that develop or use AI systems must ensure that they and their systems comply with California law, including\nlaws protecting consumers from unfair and fraudulent business practices, anticompetitive harm, discrimination and bias, and abuse of their data. Businesses must understand how the AI systems they utilize are trained, what\ninformation the systems consider, and how the systems generate output. They must also understand that they can\nbe held accountable under tort, contract, or other laws if the employment of AI results in harm, particularly when\nAI systems are employed negligently or in use cases that could entail a level of risk. Developers and users of AI must\nalso be transparent with consumers about whether consumer information is being used to train AI and how they are\nusing AI to make decisions affecting consumers.", "tags": ["Applications: Sales, retail, and customer relations", "Strategies: Disclosure", "Strategies: Disclosure: Accuracy thereof", "Harms: Detrimental content", "Harms: Financial loss", "Risk factors: Safety", "Harms: Harm to health/safety", "Applications: Sales, retail, and customer relations", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Bias", "Risk factors: Bias", "Applications: Finance and investment", "Harms: Discrimination", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security: Dissemination", "Strategies: Input controls: Data circulation", "Strategies: Input controls", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Security", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation", "Strategies: Input controls", "Strategies: Input controls: Data use", "Strategies: Input controls: Data circulation", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Applications: Education", "Applications: Medicine, life sciences and public health", "Strategies: Input controls: Data circulation", "Strategies: Input controls: Data use", "Applications: Sales, retail, and customer relations", "Strategies: Input controls", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Disclosure: In deployment", "Applications: Sales, retail, and customer relations", "Risk factors: Transparency", "Harms: Detrimental content", "Applications: Broadcasting and media production", "Incentives: Fines", "Incentives: Civil liability", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Broadcasting and media production", "Harms: Detrimental content", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Applications: Broadcasting and media production", "Incentives: Criminal liability", "Applications: Medicine, life sciences and public health", "Strategies: Performance requirements"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence.pdf", "official_name": "California Attorney General’s Legal Advisory on the Application of Existing California Laws to Artificial Intelligence", "label": "safe"}
{"id": "1974_9", "doc_id": "1974", "text": "B. California’s False Advertising Law\nCalifornia’s False Advertising Law provides another layer of protection for California’s citizens against deceptive\nadvertising. (Bus. & Prof. Code, § 17500 et seq.) The False Advertising Law “broadly restrict[s] false or misleading\nadvertising, declaring that it is unlawful for any person or business to make or distribute any statement to induce\nthe public to enter into a transaction ‘which is untrue or misleading, and which is known, or which by exercise\nof reasonable care should be known, to be untrue or misleading.’” (Nationwide Biweekly Administration, Inc.\nv. Superior Court (2020) 9 Cal.5th 279, 306 [quoting Bus. & Prof. Code, § 17500].) The law would restrict false\nadvertising regarding the capabilities, availability, and utility of AI products, the use of AI in connection with a good\nor service, as well as false advertising regarding any topic, whether or not it is generated by AI.", "tags": ["Applications: Sales, retail, and customer relations", "Strategies: Disclosure", "Strategies: Disclosure: Accuracy thereof", "Harms: Detrimental content", "Harms: Financial loss", "Risk factors: Safety", "Harms: Harm to health/safety", "Applications: Sales, retail, and customer relations", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Bias", "Risk factors: Bias", "Applications: Finance and investment", "Harms: Discrimination", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security: Dissemination", "Strategies: Input controls: Data circulation", "Strategies: Input controls", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Security", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation", "Strategies: Input controls", "Strategies: Input controls: Data use", "Strategies: Input controls: Data circulation", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Applications: Education", "Applications: Medicine, life sciences and public health", "Strategies: Input controls: Data circulation", "Strategies: Input controls: Data use", "Applications: Sales, retail, and customer relations", "Strategies: Input controls", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Disclosure: In deployment", "Applications: Sales, retail, and customer relations", "Risk factors: Transparency", "Harms: Detrimental content", "Applications: Broadcasting and media production", "Incentives: Fines", "Incentives: Civil liability", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Broadcasting and media production", "Harms: Detrimental content", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Applications: Broadcasting and media production", "Incentives: Criminal liability", "Applications: Medicine, life sciences and public health", "Strategies: Performance requirements"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence.pdf", "official_name": "California Attorney General’s Legal Advisory on the Application of Existing California Laws to Artificial Intelligence", "label": "safe"}
{"id": "1974_13", "doc_id": "1974", "text": "E. California’s Election Misinformation Prevention Laws9\nCalifornia law also provides guidance on a number of scenarios in which the use of AI may be illegal in the context\nof elections.10 California law prohibits the use of undeclared chatbots with the intent to mislead a person about its\nartificial identity in order to incentivize a purchase or influence a vote in an election. (Bus. & Prof. Code, § 17941.)\nIt is also impermissible to use AI to impersonate a candidate for elected office, or a candidate or initiative’s website\n(Elec. Code, § 18320),11 and to use AI to distribute, with actual malice, materially deceptive audio or visual media\nof a candidate for elective office within 60 days of that candidate’s election with the intent to injure the candidate’s\nreputation or deceive a voter into voting for or against the candidate. (Elec. Code, § 20010.) \n\n[footnotes omitted]", "tags": ["Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence.pdf", "official_name": "California Attorney General’s Legal Advisory on the Application of Existing California Laws to Artificial Intelligence", "label": "safe"}
{"id": "1974_14", "doc_id": "1974", "text": "Data Protection Laws Provide Additional Broad Protections for Californians\nData is the bedrock underlying the massive growth in AI, and Californians’ broad privacy and data rights directly\nimpact AI systems, whether through the data used to build and train AI, or through the information that may be\nexposed by AI outputs.\nCalifornians possess a constitutional right to privacy that applies to both government and private entities. (Hill\nv. National Collegiate Athletic Assn. (1994) 7 Cal.4th 1, 20.) Informational privacy, i.e., the “interest in precluding\nthe dissemination or misuse of sensitive and confidential information” is a core privacy interest protected by the\nCalifornia Constitution. (Id. at 35.) Developers and entities that use AI must carefully monitor AI systems’ training\ndata, inputs, and outputs to ensure that Californians’ constitutional right to privacy is respected.\nThe California Consumer Privacy Act (CCPA) broadly regulates the collection, use, sale, and sharing of consumers’\npersonal information, including heightened protections for sensitive personal information. Personal information\nmay also include inferences about consumers made by AI systems. (See Civ. Code, § 1798.140(v).) CCPA financial support allocations\nconsumers important rights:\n• The right to know about the personal information a business collects about them, and how it is used and\nshared;\n• The right to correct inaccurate personal information that a business has about them;\n• The right to delete personal information collected about them (with some exceptions);\n• The right to opt out of the sale or sharing of their personal information; and\n• The right to limit the use and disclosure of their sensitive personal information. (Id. § 1798.100 et seq.)", "tags": ["Risk factors: Privacy", "Risk factors: Security: Dissemination", "Strategies: Input controls: Data circulation", "Strategies: Input controls", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Security", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence.pdf", "official_name": "California Attorney General’s Legal Advisory on the Application of Existing California Laws to Artificial Intelligence", "label": "safe"}
{"id": "1974_15", "doc_id": "1974", "text": "AI developers and users that collect and use Californians’ personal information must comply with CCPA’s protections\nfor consumers, including by ensuring that their collection, use, retention, and sharing of consumer personal\ninformation is reasonably necessary and proportionate to achieve the purposes for which the personal information\nwas collected and processed. (Id. § 1798.100.) Businesses are restricted from processing personal information\nfor non-disclosed purposes, and even the collection, use, retention, and sharing of personal information for\ndisclosed purposes must be compatible with the context in which the personal information was collected. (Ibid.) AI\ndevelopers and users should also be aware that using personal information for research is also subject to several\nrequirements and limitations. (Id. § 1798.140(ab).) A new bill signed into law in September 2024 confirms that\nthe protections for personal information in the CCPA apply to personal information in AI systems that are capable\nof outputting personal information. (Civ. Code, § 1798.140, added by AB 1008, Stats. 2024, ch. 804.) A second bill\nexpands the definition of sensitive personal information to include “neural data.” (Civ. Code, § 1798.140, added by\nSB 1223, Stats. 2024, ch. 887.)\n\nThe California Invasion of Privacy Act (CIPA) may also impact AI training data, inputs, or outputs. CIPA restricts\nrecording or listening to private electronic communication, including wiretapping, eavesdropping on or recording\ncommunications without the consent of all parties, and recording or intercepting cellular communications without\nthe consent of all parties. (Pen. Code, § 630 et seq.) CIPA also prohibits use of systems that examine or record voice\nprints to determine the truth or falsity of statements without consent. (Id. § 637.3.) Developers and users should\nensure that their AI systems, or any data used by the system, do not violate CIPA.", "tags": ["Strategies: Input controls", "Strategies: Input controls: Data use", "Strategies: Input controls: Data circulation", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence.pdf", "official_name": "California Attorney General’s Legal Advisory on the Application of Existing California Laws to Artificial Intelligence", "label": "safe"}
{"id": "1974_16", "doc_id": "1974", "text": "California law contains heightened protection for particular types of consumer data, including education and\nhealthcare data that may be processed or used by AI systems. The Student Online Personal Information Protection\nAct (SOPIPA) broadly prohibits education technology service providers from selling student data, engaging in targeted\nadvertising using student data, and amassing profiles about students, except for specified school purposes. (Bus.\n& Prof. Code, § 22584 et seq.) SOPIPA applies to services and apps used primarily for “K-12 school purposes.” This\nincludes services and apps for home or remote instruction, as well as those intended for use at a public or private\nschool. Developers and users should ensure any educational AI systems comply with SOPIPA, even if they are\nmarketed directly to consumers.\n\nFinally, the Confidentiality of Medical Information Act (CMIA) governs the use and disclosure of Californians’ medical\ninformation and applies to businesses that offer software or hardware to consumers for the purposes of managing medical information, or for diagnosis treatment, or management of medical conditions, including mobile applications\nor other related devices. (Civ. Code, § 56 et seq.) The rise of mental health and reproductive apps led to recent\namendments to clarify that mental health and reproductive or sexual health digital services, such as apps and\nwebsites, are subject to the requirements of CMIA. Developers and users should ensure that any AI systems used for\nhealthcare, including direct-to-consumer services, comply with the CMIA.", "tags": ["Applications: Education", "Applications: Medicine, life sciences and public health", "Strategies: Input controls: Data circulation", "Strategies: Input controls: Data use", "Applications: Sales, retail, and customer relations", "Strategies: Input controls"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence.pdf", "official_name": "California Attorney General’s Legal Advisory on the Application of Existing California Laws to Artificial Intelligence", "label": "safe"}
{"id": "1974_18", "doc_id": "1974", "text": "Unauthorized Use of Likeness in the Entertainment Industry and Other Contexts\n• AB 2602 (Kalra) requires that contracts authorizing the use of an individual’s voice and likeness in a digital\nreplica created through AI technology include a “reasonably specific description” of the proposed use and\nthat the individual be represented by legal counsel or by a labor union. Absent these requirements, the\ncontract is unenforceable, unless the uses are otherwise consistent with the terms of the contract and the\nunderlying work. (Lab. Code, § 927.)\n• AB 1836 (Bauer-Kahan) prohibits the use of a deceased personality’s digital replica without prior consent\nwithin 70 years of the personality’s death, imposing a minimum $10,000 charge for the infraction. A deceased\npersonality is any natural person whose name, voice, signature, photograph, or likeness has commercial\nvalue at the time of that person’s death, or because of that person’s death. (Civ. Code, § 3344.1.)", "tags": ["Harms: Detrimental content", "Applications: Broadcasting and media production", "Incentives: Fines", "Incentives: Civil liability"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence.pdf", "official_name": "California Attorney General’s Legal Advisory on the Application of Existing California Laws to Artificial Intelligence", "label": "safe"}
{"id": "1974_20", "doc_id": "1974", "text": "Expanded Prohibitions and Reporting of Exploitative Uses of AI\n• AB 1831 (Berman) and SB 1381 (Wahab) expands existing criminal prohibitions on child pornography to\ninclude the use of AI in the creation of visual depictions of the sexual abuse and exploitation of children.\n(Pen. Code, §§ 311, 311.2, 311.3, 311.4, 311.11, 311.12, 312.3.)\n• SB 926 (Wahab) extends criminal consequences to the creation of nonconsensual pornography using deepfake\ntechnology. (Pen. Code, § 647.)\n• SB 981 (Wahab) requires social media platforms to provide a mechanism for California users to report\nsexually explicit digital identity theft or deepfake pornography. (Bus. & Prof. Code, § 22670 et seq.)", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Applications: Broadcasting and media production", "Incentives: Criminal liability"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence.pdf", "official_name": "California Attorney General’s Legal Advisory on the Application of Existing California Laws to Artificial Intelligence", "label": "safe"}
{"id": "1974_21", "doc_id": "1974", "text": "Supervision of AI Tools in Healthcare Settings\n• SB 1120 (Becker) requires health insurers to ensure that licensed physicians supervise the use of AI tools that\nmake decisions about healthcare services and insurance claims. (Health & Saf. Code, § 1367.01; Ins. Code, §\n10123.135.)", "tags": ["Applications: Medicine, life sciences and public health", "Strategies: Performance requirements"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence.pdf", "official_name": "California Attorney General’s Legal Advisory on the Application of Existing California Laws to Artificial Intelligence", "label": "safe"}
{"id": "1974_22", "doc_id": "1974", "text": "Entities Should Remain Vigilant About Other Laws and\nRegulations Which May Be Applicable to AI Technologies\nBeyond the laws and regulations discussed in this advisory, other California laws—including tort, public nuisance,\nenvironmental and business regulation, and criminal law—apply equally to AI systems and to conduct and business\nactivities that involve the use of AI. Conduct that is illegal if engaged in without the involvement of AI is equally\nunlawful if AI is involved, and the fact that AI is involved is not a defense to responsibility under any law.\nThis overview is not intended to be exhaustive. Entities that develop or use AI have a duty to ensure that they\nunderstand and are in adherence with all state, federal, and local laws that may apply to them or their activities.\nThat is particularly so when AI is used or developed for applications that could carry a potential risk of harm to\npeople, organizations, physical or virtual infrastructure, or the environment.", "tags": ["Applications: Sales, retail, and customer relations", "Strategies: Disclosure", "Strategies: Disclosure: Accuracy thereof", "Harms: Detrimental content", "Harms: Financial loss", "Risk factors: Safety", "Harms: Harm to health/safety", "Applications: Sales, retail, and customer relations", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Bias", "Risk factors: Bias", "Applications: Finance and investment", "Harms: Discrimination", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security: Dissemination", "Strategies: Input controls: Data circulation", "Strategies: Input controls", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Security", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation", "Strategies: Input controls", "Strategies: Input controls: Data use", "Strategies: Input controls: Data circulation", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Applications: Education", "Applications: Medicine, life sciences and public health", "Strategies: Input controls: Data circulation", "Strategies: Input controls: Data use", "Applications: Sales, retail, and customer relations", "Strategies: Input controls", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Disclosure: In deployment", "Applications: Sales, retail, and customer relations", "Risk factors: Transparency", "Harms: Detrimental content", "Applications: Broadcasting and media production", "Incentives: Fines", "Incentives: Civil liability", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Broadcasting and media production", "Harms: Detrimental content", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Applications: Broadcasting and media production", "Incentives: Criminal liability", "Applications: Medicine, life sciences and public health", "Strategies: Performance requirements"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence.pdf", "official_name": "California Attorney General’s Legal Advisory on the Application of Existing California Laws to Artificial Intelligence", "label": "safe"}
{"id": "1991_8", "doc_id": "1991", "text": "Section 30. Algorithmic discrimination.\n    (a) A deployer must avoid use an automated decision tool\nthat results in algorithmic discrimination.\n    (b) On and after January 1, 2028, a person may bring a\ncivil action against a deployer for infraction of this Section.\nIn an action brought under this subsection, the plaintiff\nshall have the burden of proof to demonstrate that the\ndeployer's use of the automated decision tool resulted in\nalgorithmic discrimination that caused actual harm to the\nperson bringing the civil action.\n    (c) In addition to any other remedy at law, a deployer that\nviolates this Section shall be responsible to a prevailing\nplaintiff for any of the following:\n        (1) compensatory damages;\n        (2) declaratory relief; and\n        (3) reasonable attorney's fees and costs.", "tags": ["Incentives: Civil liability", "Risk factors: Bias", "Incentives: Fines"], "source": "https://ilga.gov/legislation/fulltext.asp?DocName=10400SB2203lv&SessionID=114&GA=104&DocTypeID=SB&DocNum=2203&SpecSess=&Session=&print=true", "official_name": "Illinois SB2203", "label": "safe"}
{"id": "1991_9", "doc_id": "1991", "text": "Section 35. Impact assessment.\n    (a) Within 60 days after completing an impact assessment\nrequired by this Act, a deployer shall provide the impact\nassessment to the Attorney General.\n    (b) A deployer who knowingly violates this Section shall\nbe responsible for an administrative charge of not more than $10,000\nper infraction in an administrative enforcement action brought\nby the Attorney General. Each day on which an automated\ndecision tool is used for which an impact assessment has not\nbeen submitted as required under this Section shall give rise\nto a distinct infraction of this Section.\n    (c) The Attorney General may share impact assessments with\nother State entities as appropriate.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Incentives: Fines"], "source": "https://ilga.gov/legislation/fulltext.asp?DocName=10400SB2203lv&SessionID=114&GA=104&DocTypeID=SB&DocNum=2203&SpecSess=&Session=&print=true", "official_name": "Illinois SB2203", "label": "safe"}
{"id": "1993_5", "doc_id": "1993", "text": "Section 2. Developer duty to avoid algorithmic discrimination - required documentation. \n\n(a) Not later than 6 months after the effective date of this act, a developer of a high-risk artificial intelligence system shall use reasonable care to protect consumers from any known or reasonably foreseeable risks of algorithmic discrimination arising from the intended and contracted uses of the high-risk artificial intelligence system. In any enforcement action brought not later than 6 months after the effective date of this act, by the attorney general pursuant to section 6, there is a rebuttable presumption that a developer used reasonable care as required under this section if the developer complied with this section and any additional requirements or obligations as set forth in rules promulgated by the attorney general pursuant to section 7.", "tags": ["Applications: Government: judicial and law enforcement"], "source": "https://malegislature.gov/Bills/194/HD4053", "official_name": "Massachusetts House Docket 4053 (2025)", "label": "safe"}
{"id": "1993_22", "doc_id": "1993", "text": "(h) (1) a bank, out-of-state bank, benefit union chartered by the state of Massachusetts, federal benefit union, out-of-state benefit union, or any affiliate or subsidiary thereof, is in full adherence with this chapter  if the bank, out-of-state bank, benefit union chartered by the state of Massachusetts, federal benefit union, out-of-state benefit union, or affiliate or subsidiary is subject to examination by a state or federal prudential regulator under any published guidance or regulations that apply to the use of high-risk artificial intelligence systems and the guidance or regulations:\n\n(i) impose requirements that are substantially equivalent to or more stringent than the requirements imposed in this chapter; and\n\n(ii) at a minimum, require the bank, out-of-state bank, benefit union chartered by the state of Massachusetts, federal benefit union, out-of-state benefit union, or affiliate or subsidiary to:\n\n(A) regularly review the bank's, out-of-state bank's, benefit union chartered by the state of Massachusetts', federal benefit union's, out-of-state benefit union's, or affiliate's or subsidiary's use of high-risk artificial intelligence systems for adherence with state and federal anti-discrimination laws and regulations applicable to the bank, out-of-state bank, benefit union chartered by the state of Massachusetts federal benefit union, out-of-state benefit union, or affiliate or subsidiary; and\n\n(B) mitigate any algorithmic discrimination caused by the use of a high-risk artificial intelligence system or any risk of algorithmic discrimination that is reasonably foreseeable as a result of the use of a high-risk artificial intelligence system.\n\n(2) as used in this subsection (8):\n\n(i) \"Affiliate\" has the meaning set forth in chapter 156D.\n\n(ii) \"Bank\" has the meaning set forth in chapter 167.\n\n(iii) \"benefit union\" has the meaning set forth in chapter 167. \n\n(iv) \"Out-of-state bank\" has the meaning set forth in chapter 167.\n\n(i) if a developer, a deployer, or other person engages in an action pursuant to an exception set forth in this section, the developer, deployer, or other person bears the burden of demonstrating that the action qualifies for the exception.", "tags": ["Risk factors: Bias", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation: External auditing", "Strategies: Evaluation"], "source": "https://malegislature.gov/Bills/194/HD4053", "official_name": "Massachusetts House Docket 4053 (2025)", "label": "safe"}
{"id": "1993_23", "doc_id": "1993", "text": "Section 6. Enforcement by attorney general\n\n(a) the attorney general has exclusive authority to enforce this chapter.\n\n(b) except as provided in subsection (c) of this section, a infraction of the requirements established in this chapter constitutes an unfair trade practice pursuant to chapter 93A.\n\n(c) in any action commenced by the attorney general to enforce this chapter, it is an affirmative that the developer, deployer, or other person:\n\n(1) discovers and cures a infraction of this this chapter 93 as a result of:\n\n(i) feedback that the developer, deployer, or other person encourages deployers or users to provide to the developer, deployer, or other person;\n\n(ii) adversarial testing or red teaming, as those terms are defined or used by the national institute of standards and technology; or\n\n(iii) an internal review process; and\n\n(2) is otherwise in adherence with:\n\n(i) the latest version of the \"Artificial intelligence risk management framework\" published by the national institute of standards and technology in the United States Department of Commerce and Standard ISO/IEC 42001 of the International Organization for Standardization;\n\n(ii) another nationally or internationally recognized risk management framework for artificial intelligence systems, if the standards are substantially equivalent to or more stringent than the requirements of this chapter; or\n\n(iii) any risk management framework for artificial intelligence systems that the attorney general, in the attorney general's discretion, may designate and, if designated, shall publicly disseminate.\n\n(d) a developer, a deployer, or other person bears the burden of demonstrating to the attorney general that the requirements established in subsection (3) of this section have been satisfied.\n\n(e) nothing in this chapter, including the enforcement authority granted to the attorney general under this section, preempts or otherwise affects any right, claim, remedy, presumption, or defense available at law or in equity. A rebuttable presumption or affirmative defense established under this chapter applies only to an enforcement action brought by the attorney general pursuant to this section and does not apply to any right, claim, remedy, presumption, or defense available at law or in equity.\n\n(f) this chapter does not provide the basis for, and is not subject to, a private right of action for violations of this chapter or any other law.", "tags": ["Incentives: Civil liability", "Incentives: Fines"], "source": "https://malegislature.gov/Bills/194/HD4053", "official_name": "Massachusetts House Docket 4053 (2025)", "label": "safe"}
{"id": "1996_3", "doc_id": "1996", "text": "SECTION 2. NEW LAW A new section of law to be codified in the Oklahoma Statutes as Section 502 of Title 25, unless there is created a duplication in numbering, reads as follows:\nAs used in this act:\n1. \"AI system\" means an artificial intelligence or machine learning–based or algorithmic technology designed to perform tasks that typically require human intelligence, including decision-making, prediction, or recommendation;\n2. \"Deployer\" means any public entity, private organization, or individual that implements AI systems for operational use;\n3. \"Risk classification\" means one of the following:\na. Unacceptable Risk: (1) Means AI applications incompatible with social values and fundamental rights, (2) Includes social scoring, manipulative AI targeting vulnerable groups, and real-time biometric identification systems or may include any of the following: (a) deployment of AI systems for discriminatory lending practices or biased law enforcement profiling, (b) unauthorized use of biometric analysis tools for surveillance purposes in public and private spaces, (c) integration of AI into systems with unregulated access to sensitive government databases, or (d) AI-driven misinformation campaigns targeting elections, public health, or emergency response systems, and (3) AI systems under this category are restricted from development, deployment, or use,\nb. High Risk: (1) Means AI systems with significant potential to impact safety, civil liberties, or fundamental rights, (2) Includes AI in health care, critical infrastructure, law enforcement, financial services, and public welfare, and (3) AI systems under this category are subject to rigorous pre-deployment risk assessments, independent reviews, strict operational requirements, and ongoing real-time monitoring including mandatory human oversight by qualified overseers,\nc. Limited Risk: (1) Means AI systems that pose moderate risks, such as manipulation or deceit, but not classified as high risk, (2) Includes chatbots and content creation tools such as deepfake generators, and (3) AI systems under this category require providers to ensure transparency, informing users when they interact with AI systems or consume AI-generated content and requires basic safeguards to prevent manipulation, and\nd. Minimal Risk: (1) Means AI systems with little to no risk to users, (2) Examples include most consumer-facing AI applications such as virtual assistants, and (3) AI systems under this category require minimal oversight, with adherence to basic data protection standards; and\n4. \"Qualified human overseer\" means a trained individual responsible for monitoring and validating high-risk AI system outputs, with authority to amend or override them when necessary.", "tags": ["Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Applications: Medicine, life sciences and public health", "Applications: Government: judicial and law enforcement", "Applications: Finance and investment"], "source": "http://www.oklegislature.gov/BillInfo.aspx?Bill=hb1916&Session=2500", "official_name": "Oklahoma House Bill 1916", "label": "safe"}
{"id": "1996_5", "doc_id": "1996", "text": "SECTION 4. NEW LAW A new section of law to be codified in the Oklahoma Statutes as Section 504 of Title 25, unless there is created a duplication in numbering, reads as follows:\nA. Deployers must classify artificial intelligence (AI) systems into one of four risk categories outlined in Section 2 of this act before deployment.\nB. Deployers shall conduct assessments of AI systems to identify:\n1. Potential biases in training data;\n2. Risks to safety, civil liberties, and fundamental right; and\n3. Mitigation strategies for identified risks.\nC. High-risk AI systems must undergo ongoing performance evaluations, with documentation of findings and actions taken to address deficiencies.\nD. Qualified overseers must validate AI outputs before they are enacted and retain authority to override system recommendations.\nE. Deployers shall notify affected individuals when high-risk AI systems influence decisions about the individuals and provide avenues for appeal or human review.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: In deployment"], "source": "http://www.oklegislature.gov/BillInfo.aspx?Bill=hb1916&Session=2500", "official_name": "Oklahoma House Bill 1916", "label": "safe"}
{"id": "1996_8", "doc_id": "1996", "text": "SECTION 7. NEW LAW A new section of law to be codified in the Oklahoma Statutes as Section 507 of Title 25, unless there is created a duplication in numbering, reads as follows:\nA. Violations of this act may result in the Artificial Intelligence Council (AI Council) issuing any of the following:\n1. Financial consequences proportional to the severity of the infraction;\n2. Suspension or prohibition of noncompliant systems; and\n3. Mandatory external oversight until adherence is restored.\nB. Deployers shall publicly disclose noncompliance incidents and the corrective actions taken.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: About incidents", "Incentives: Access to business opportunities", "Incentives: Civil liability", "Incentives: Fines"], "source": "http://www.oklegislature.gov/BillInfo.aspx?Bill=hb1916&Session=2500", "official_name": "Oklahoma House Bill 1916", "label": "safe"}
{"id": "1997_6", "doc_id": "1997", "text": "(b) No less than every 90 days, a developer shall produce\nand conspicuously publish a risk assessment report. The risk\nassessment report shall cover the period between 120 and 30\ndays before the submission of the risk assessment report `and\ninclude the following:\n(1) the conclusion of any risk assessments made\npursuant to the developer's safety and security protocol\nduring the reporting period;\n(2) if different from the preceding reporting period,\nfor each type of critical risk, an assessment of the\nrelevant capabilities in whichever of the developer's\nfoundation models, whether deployed or not, would pose the\nhighest level of that critical risk if deployed without\nadequate safeguards and protections; and\n(3) if the developer has deployed a foundation model\nor a modified version of a foundation model during the\nreporting, that would, if deployed without adequate\nsafeguards and protections, pose a higher level of\ncritical risk than any of the developer's existing\ndeployed foundation models:\n(A) the grounds on which, and the process by\nwhich, the developer decided to deploy the foundation\nmodel; and\n(B) any safeguards and protections implemented by\nthe developer to mitigate critical risks.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Post-market monitoring"], "source": "https://ilga.gov/legislation/fulltext.asp?DocName=&SessionId=114&GA=104&DocTypeId=HB&DocNum=3506&GAID=18&LegID=162191&SpecSess=&Session=", "official_name": "Illinois HB 3506", "label": "safe"}
{"id": "1997_9", "doc_id": "1997", "text": "Section 25. reviews.\n(a) At least once every calendar year, a developer shall\nretain a reputable third-party auditor to produce a report\nassessing the following:\n(1) whether the developer has complied with its safety\nand security protocol and any instances of noncompliance\nor ambiguous adherence;\n(2) any instances where the developer's safety and\nsecurity protocol has not been stated clearly enough to\ndetermine whether the developer has complied; and\n(3) any instances where the auditor believes the\ndeveloper may have violated subsection (d) of Section 15\nor Section 20.\n(b) A developer shall allow the third-party auditor access\nto all materials produced to comply with this Act and any other\nmaterials reasonably necessary to perform the assessment\nrequired under subsection (a).\n(c) No later than 90 days after the completion of the\nthird-party auditor's report required under subsection (a),\nthe developer shall conspicuously publish the report.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: External auditing"], "source": "https://ilga.gov/legislation/fulltext.asp?DocName=&SessionId=114&GA=104&DocTypeId=HB&DocNum=3506&GAID=18&LegID=162191&SpecSess=&Session=", "official_name": "Illinois HB 3506", "label": "safe"}
{"id": "1997_13", "doc_id": "1997", "text": "Section 35. Enforcement.\n(a) The Attorney General may bring a civil action against\na developer that violates Sections 15 or 25. A developer found\nguilty of violating Sections 15 or 25 may be assessed a civil\nconsequence not to exceed $1,000,000. In calculating the civil\nconsequence assessed under this subsection, a court shall consider\nthe severity of the infraction and whether the infraction\nresulted in, or could have resulted in, the materialization of\na critical risk.\n(b) The Attorney General may seek injunctive or\ndeclaratory relief for any infraction of this Act. The Attorney\nGeneral may seek injunctive relief if a developer's activities\npresent an imminent threat of catastrophic harm to the public.", "tags": ["Incentives: Civil liability", "Incentives: Fines"], "source": "https://ilga.gov/legislation/fulltext.asp?DocName=&SessionId=114&GA=104&DocTypeId=HB&DocNum=3506&GAID=18&LegID=162191&SpecSess=&Session=", "official_name": "Illinois HB 3506", "label": "safe"}
{"id": "2002_1", "doc_id": "2002", "text": "Section 1. Chapter 5 of Title 6 of the Rules of the City of New York is amended to add Subchapter T to read as follows:\n\n\nSubchapter T: Automated Employment Decision Tools\n§ 5-300. Definitions.\nAs used in this subchapter, the following terms have the following meanings:\n\n\nAutomated Employment Decision Tool. “Automated employment decision tool” or “AEDT” means “Automated employment decision tool” as defined by § 20-870 of the Code where the phrase “to substantially assist or replace discretionary decision making” means:\ni. to rely solely on a simplified output (score, tag, classification, ranking, etc.), with no other factors considered; or\nii. to use a simplified output as one of a set of criteria where the simplified output is weighted more than any other criterion in the set; or\niii. to use a simplified output to overrule conclusions derived from other factors including human decision-making.\n\n\nBias review. “Bias review” means “Bias review” as defined by § 20-870 of the Code.\n\n\nCandidate for Employment. “Candidate for employment” means a person who has applied for a specific employment position by submitting the necessary information or items in the format required by the employer or employment agency.", "tags": ["Risk factors: Bias", "Strategies: Evaluation", "Strategies: Evaluation: External auditing"], "source": "https://rules.cityofnewyork.us/wp-content/uploads/2023/04/DCWP-NOA-for-Use-of-Automated-Employment-Decisionmaking-Tools-2.pdf", "official_name": "New York City Department of Consumer and Worker Protection Amendment to Rules of NY City (Use of Automated Employment Decisionmaking Tools)", "label": "safe"}
{"id": "2002_2", "doc_id": "2002", "text": "Category. “Category” means any component 1 category required to be reported by employers pursuant to subsection (c) of section 2000e-8 of title 42 of the United States Code as specified in part 1602.7 of title 29 of the Code of Federal Regulations, as designated on the Equal Employment Opportunity Commission Employer Information Report EEO-1.\n\n\nCode. “Code” means the Administrative Code of the City of New York.\n\n\nDistribution Date. “Distribution date” means the date the employer or employment agency began using a specific AEDT.\n\n\nEmployment Decision. “Employment decision” means “Employment decision” as defined by § 20-870 of the Code.\n\n\nEmployment Agency. “Employment agency” means “Employment agency” as defined by 6 RCNY § 5-249.\n\n\nHistorical data. “Historical data” means data collected during an employer or employment agency’s use of an AEDT to assess candidates for employment or employees for promotion.", "tags": ["Risk factors: Bias", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Evaluation: External auditing", "Strategies: Evaluation", "Applications: Business services and analytics", "Risk factors: Bias", "Applications: Business services and analytics", "Risk factors: Bias", "Strategies: Evaluation: External auditing", "Strategies: Evaluation", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Bias", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Bias", "Applications: Business services and analytics", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In deployment", "Strategies: Disclosure", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About inputs"], "source": "https://rules.cityofnewyork.us/wp-content/uploads/2023/04/DCWP-NOA-for-Use-of-Automated-Employment-Decisionmaking-Tools-2.pdf", "official_name": "New York City Department of Consumer and Worker Protection Amendment to Rules of NY City (Use of Automated Employment Decisionmaking Tools)", "label": "safe"}
{"id": "2002_3", "doc_id": "2002", "text": "Independent Auditor. “Independent auditor” means a person or group that is capable of exercising objective and impartial judgment on all issues within the scope of a bias review of an AEDT. An auditor is not an independent auditor of an AEDT if the auditor:\ni. is or was involved in using, developing, or distributing the AEDT;\nii. at any point during the bias review, has an employment relationship with an employer or employment agency that seeks to use or continue to use the AEDT or with a vendor that developed or distributes the AEDT; or\niii. at any point during the bias review, has a direct financial interest or a material indirect financial interest in an employer or employment agency that seeks to use or continue to use the AEDT or in a vendor that developed or distributed the AEDT. \n\n\nImpact Ratio. “Impact ratio” means either (1) the selection rate for a category divided by the selection rate of the most selected category or (2) the scoring rate for a category divided by the scoring rate for the highest scoring category.\n\n\nselection rate for a category\nImpact Ratio =   _________________________________ \nselection rate of the most selected category \n\n\nOR \n\n\nscoring rate for a category\nImpact Ratio = _________________________________ \nscoring rate of the highest scoring category", "tags": ["Strategies: Evaluation: External auditing", "Strategies: Evaluation"], "source": "https://rules.cityofnewyork.us/wp-content/uploads/2023/04/DCWP-NOA-for-Use-of-Automated-Employment-Decisionmaking-Tools-2.pdf", "official_name": "New York City Department of Consumer and Worker Protection Amendment to Rules of NY City (Use of Automated Employment Decisionmaking Tools)", "label": "safe"}
{"id": "2002_6", "doc_id": "2002", "text": "§ 5-301 Bias review. \n(a) An employer or employment agency may not use or continue to use an AEDT if more than one year has passed since the most recent bias review of the AEDT. \n\n\n(b) Where an AEDT selects candidates for employment or employees being considered for promotion to move forward in the hiring process or classifies them into groups, a bias review must, at a minimum: \n(1) Calculate the selection rate for each category; \n(2) Calculate the impact ratio for each category; \n(3) Ensure that the calculations required in paragraphs (1) and (2) of this subdivision separately calculate the impact of the AEDT on: \ni. Sex categories (e.g., impact ratio for selection of male candidates vs female candidates), \nii. Race/Ethnicity categories (e.g., impact ratio for selection of Hispanic or Latino candidates vs Black or African American [Not Hispanic or Latino] candidates), and \niii. intersectional categories of sex, ethnicity, and race (e.g., impact ratio for selection of Hispanic or Latino male candidates vs. Not Hispanic or Latino Black or African American female candidates). \n(4) Ensure that the calculations in paragraphs (1), (2), and (3) of this subdivision are performed for each group, if an AEDT classifies candidates for employment or employees being considered for promotion into specified groups (e.g., leadership styles); and \n(5) Indicate the number of individuals the AEDT assessed that are not included in the required calculations because they fall within an unknown category.", "tags": ["Risk factors: Bias", "Strategies: Evaluation: External auditing", "Strategies: Evaluation", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination"], "source": "https://rules.cityofnewyork.us/wp-content/uploads/2023/04/DCWP-NOA-for-Use-of-Automated-Employment-Decisionmaking-Tools-2.pdf", "official_name": "New York City Department of Consumer and Worker Protection Amendment to Rules of NY City (Use of Automated Employment Decisionmaking Tools)", "label": "safe"}
{"id": "2002_7", "doc_id": "2002", "text": "Example: An employer wants to use an AEDT to screen resumes and schedule interviews for a job posting. To do so, the employer must ensure that a bias review of the AEDT was conducted no more than a year before the planned use of the AEDT. This bias review is necessary even though the employer is not using the AEDT to make the final hiring decision, but only to screen at an early point in the application process. The employer asks the vendor for a bias review. The vendor provides historical data regarding applicant selection that the vendor has collected from multiple employers to an independent auditor who will conduct a bias review as follows: \n\n\n[AEDT data and calculations from example omitted]", "tags": ["Risk factors: Bias", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Evaluation: External auditing", "Strategies: Evaluation", "Applications: Business services and analytics", "Risk factors: Bias", "Applications: Business services and analytics", "Risk factors: Bias", "Strategies: Evaluation: External auditing", "Strategies: Evaluation", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Bias", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Bias", "Applications: Business services and analytics", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In deployment", "Strategies: Disclosure", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About inputs"], "source": "https://rules.cityofnewyork.us/wp-content/uploads/2023/04/DCWP-NOA-for-Use-of-Automated-Employment-Decisionmaking-Tools-2.pdf", "official_name": "New York City Department of Consumer and Worker Protection Amendment to Rules of NY City (Use of Automated Employment Decisionmaking Tools)", "label": "safe"}
{"id": "2002_9", "doc_id": "2002", "text": "(d) Notwithstanding the requirements of paragraphs (2) and (3) of subdivision (b) and paragraphs (3) and (4) of subdivision (c), an independent auditor may exclude a category that represents less than 2% of the data being used for the bias review from the required calculations for impact ratio. Where such a category is excluded, the summary of results must include the independent auditor’s justification for the exclusion, as well as the number of applicants and scoring rate or selection rate for the excluded category. \n\n\nExample: An employer uses an AEDT to score applicants for “culture fit.” To do so, the employer must ensure that a bias review of the AEDT was conducted no more than a year before the use of the AEDT. The employer provides historical data on “culture fit” score of applicants for each category to an independent auditor to conduct a bias review as follows: \n\n\n[AEDT data and calculations from example omitted]", "tags": ["Risk factors: Bias", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Evaluation: External auditing", "Strategies: Evaluation", "Applications: Business services and analytics", "Risk factors: Bias", "Applications: Business services and analytics", "Risk factors: Bias", "Strategies: Evaluation: External auditing", "Strategies: Evaluation", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Bias", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Bias", "Applications: Business services and analytics", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In deployment", "Strategies: Disclosure", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About inputs"], "source": "https://rules.cityofnewyork.us/wp-content/uploads/2023/04/DCWP-NOA-for-Use-of-Automated-Employment-Decisionmaking-Tools-2.pdf", "official_name": "New York City Department of Consumer and Worker Protection Amendment to Rules of NY City (Use of Automated Employment Decisionmaking Tools)", "label": "safe"}
{"id": "2002_11", "doc_id": "2002", "text": "Example 1: An employer is planning to use an AEDT for the first time. The employer may rely on a bias review conducted using the historical data of other employers or employment agencies, or on a bias review conducted using test data. \n\n\nExample 2: An employment agency has been using an AEDT for 6 months. The bias review the employment agency relied on before its first use of the AEDT was conducted 10 months ago using test data. The employment agency will need an updated bias review if it will continue to use the AEDT once 12 months have passed since the bias review it first relied on was conducted. The employment agency’s data from 6 months of use of the AEDT is not sufficient on its own to conduct a statistically significant bias review. The employment agency may rely on a bias review using the historical data of other employers and employment agencies if it provides its 6 months of historical data to the independent auditor for use and consideration. The employment agency may also rely on a bias review that uses test data. \n\n\nExample 3: An employer has been using an AEDT for 3 years and will soon need an updated bias review. The employer has statistically significant data from its 3 years of use of the AEDT. The employer may rely on a bias review conducted using historical data from multiple employers if it provides its 3 years of historical data to the independent auditor for use and consideration. The employer may also rely on a bias review conducted using historical data from its own use of the AEDT, without any data from other employers or employment agencies. The employer may not rely on a bias review conducted using test data.", "tags": ["Risk factors: Bias", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Evaluation: External auditing", "Strategies: Evaluation", "Applications: Business services and analytics", "Risk factors: Bias", "Applications: Business services and analytics", "Risk factors: Bias", "Strategies: Evaluation: External auditing", "Strategies: Evaluation", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Bias", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Bias", "Applications: Business services and analytics", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In deployment", "Strategies: Disclosure", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About inputs"], "source": "https://rules.cityofnewyork.us/wp-content/uploads/2023/04/DCWP-NOA-for-Use-of-Automated-Employment-Decisionmaking-Tools-2.pdf", "official_name": "New York City Department of Consumer and Worker Protection Amendment to Rules of NY City (Use of Automated Employment Decisionmaking Tools)", "label": "safe"}
{"id": "2031_1", "doc_id": "2031", "text": "Overview: Data privacy and security are core values of our organization. To successfully provide a\nhigh quality of life for those that live, work, and play in Long Beach, it is critical that we build public\ntrust through excellence in data privacy, data security, and community engagement. The following\nData Privacy Guidelines assert the City’s core values on protecting the privacy and information\nsecurity of our constituents. They are intended to provide a framework to help the City and partners\nincorporate privacy by design as we deploy new technologies and new services in Long Beach.", "tags": ["Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About inputs", "Risk factors: Privacy", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Risk factors: Bias", "Harms: Discrimination", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://www.longbeach.gov/globalassets/smart-city/media-library/documents/final_data-privacy-guidelines", "official_name": "City of Long Beach Data Privacy Guidelines (2021)", "label": "safe"}
{"id": "2031_2", "doc_id": "2031", "text": "Note: The California Consumer Protection Act (CCPA), which went into effect January 1, 2020,\nprovides a set of consumer rights governing data collection requirements for businesses. It does not\napply to public agencies at this time, though City vendors that meet CCPA eligibility requirements\nmust comply. The City supports the intent behind the CCPA (and subsequent amendments such as\nthe California Privacy Rights Act which passed in November 2020), and we strive to adhere to the\nguidelines below which are based upon CCPA requirements. The City is also required to comply with\ndata transparency laws such as the California Public Records Act, which provides fundamental rights\nto the public to access government information.", "tags": ["Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About inputs", "Risk factors: Privacy", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Risk factors: Bias", "Harms: Discrimination", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://www.longbeach.gov/globalassets/smart-city/media-library/documents/final_data-privacy-guidelines", "official_name": "City of Long Beach Data Privacy Guidelines (2021)", "label": "safe"}
{"id": "2031_3", "doc_id": "2031", "text": "Key Terms:\n• Data privacy: The practices taken to govern the collection, protection, and sharing of personal\nand confidential information.\n• Smart city: A city that uses emerging technology and data to manage complex city\noperations, efficiently deliver services, and improve the quality of life.\n• Equity: When everyone can achieve their highest quality of life no matter their background.\nOften used in the context of race.\n• Personally identifiable information (PII): Data that can be used to distinguish or trace an\nindividual’s identity, either alone or when combined with other personal data.\n• Algorithm: A process or set of rules that a computer needs to do to complete a task.\n• Artificial intelligence (AI): Machines that have the ability to “learn” and “problem solve.”\n• Data stewardship: The management and oversight of data to help provide individuals with\nhigh-quality data that is easily accessible in a consistent manner.", "tags": ["Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About inputs", "Risk factors: Privacy", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Risk factors: Bias", "Harms: Discrimination", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://www.longbeach.gov/globalassets/smart-city/media-library/documents/final_data-privacy-guidelines", "official_name": "City of Long Beach Data Privacy Guidelines (2021)", "label": "safe"}
{"id": "2031_4", "doc_id": "2031", "text": "Purpose: Every day, Long Beach residents, visitors, and business owners trade personal privacy for\nconvenience. For example, a City app that allows you pay for parking with your phone could also\ntransmit anonymized data to City officials to better manage demand for parking. As smart city\ntechnologies become more commonplace, the City and its vendors will inevitably collect data that if\nnot managed properly may put certain communities and individuals at risk. These technologies aren’t\ninherently good or bad, however through their design and implementation, they can be exploited to\ndo harm. In particular, underserved and marginalized communities face disproportionately negative\nimpacts from misuse of data.\nThe Data Privacy Guidelines are meant to augment – not replace - existing laws, rules, and\nregulations that apply to our technology projects and services. The City values privacy as an\nexpectation and given right, and will advocate for our constituents to have greater control over the\ncollection and use of their personal information.", "tags": ["Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About inputs", "Risk factors: Privacy", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Risk factors: Bias", "Harms: Discrimination", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://www.longbeach.gov/globalassets/smart-city/media-library/documents/final_data-privacy-guidelines", "official_name": "City of Long Beach Data Privacy Guidelines (2021)", "label": "safe"}
{"id": "2031_5", "doc_id": "2031", "text": "Methodology: The City’s Technology & advancement Department worked closely with the resident-led\nTechnology & advancement Commission to develop the Data Privacy Guidelines. City staff and\nCommissioners led over 20 focus groups, workshops, and interviews with key community-based\norganizations and stakeholders. The Commission also developed a multi-lingual Data Privacy Survey,\nwhich yielded over 450 responses on residents’ data privacy preferences.", "tags": ["Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About inputs", "Risk factors: Privacy", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Risk factors: Bias", "Harms: Discrimination", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://www.longbeach.gov/globalassets/smart-city/media-library/documents/final_data-privacy-guidelines", "official_name": "City of Long Beach Data Privacy Guidelines (2021)", "label": "safe"}
{"id": "2031_6", "doc_id": "2031", "text": "What’s Next: The Data Privacy Guidelines represent a first step towards operationalizing privacy in\nour City programs, technology projects, and services. Following the adoption of these Guidelines, the\nCity will provide technical guidance to all City Departments and continue to work with appropriate\nCity Commissions, Long Beach residents, and other local stakeholders to embed these guidelines\nwithin City policies, contracts, procedures, trainings, educational campaigns (for both City staff and\nLong Beach residents), software applications, and legacy systems.\nThe City will advocate for state and federal legislation consistent with these Guidelines, and revisit\nand update these Guidelines on a recurring basis.", "tags": ["Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About inputs", "Risk factors: Privacy", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Risk factors: Bias", "Harms: Discrimination", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://www.longbeach.gov/globalassets/smart-city/media-library/documents/final_data-privacy-guidelines", "official_name": "City of Long Beach Data Privacy Guidelines (2021)", "label": "safe"}
{"id": "2031_9", "doc_id": "2031", "text": "4. Long Beach will use data in an ethical and non-discriminatory manner to not reinforce\nexisting racial biases and prejudiced decision-making. Emerging technology promises many\nbenefits, but may exclude, harm, and even criminalize already marginalized populations if not\ncarefully managed.\na. Long Beach will leverage a racial equity lens to examine the burdens, benefits, and\nunintended consequences of data collected for technology projects and applications.\nThe City will practice data integrity and use data for stated and public purposes.\nb. Long Beach will never sell, or permit vendors to sell, personally identifiable\ninformation (PII) data to third parties and will only use collected data to serve the\npublic good and to bring value to our communities. Long Beach will limit collection\nand sharing of personal data for only purposes which are directly relevant and\nnecessary to accomplish a clearly-communicated purpose. This extends to data\nsharing between third parties. Long Beach will never share PII data with independent\nthird parties without first soliciting individuals’ consent unless we are legally required\nto do so in connection with law enforcement investigations, mandatory contractual\nobligations, Public Records Act (PRA) requirements, or other legal proceedings. In\nthese cases where the City must disclose PII as required by law, Long Beach will \nwork to provide notice to affected individuals where possible unless doing so\ncompromises a law enforcement investigation.\nc. Long Beach will ensure human review of decision frameworks made by algorithms\nand AI. Algorithmic and artificial intelligence (AI) technology is increasingly complex\nand mysterious. The City will use evidence-based practices to evaluate potentially\ndiscriminatory consequences of this technology and require human involvement on\nany decision-making schemas and training input that are informed by outcomes of AI,\nmachine learning algorithms, and related technology.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Risk factors: Bias", "Harms: Discrimination", "Strategies: Performance requirements"], "source": "https://www.longbeach.gov/globalassets/smart-city/media-library/documents/final_data-privacy-guidelines", "official_name": "City of Long Beach Data Privacy Guidelines (2021)", "label": "safe"}
{"id": "2031_10", "doc_id": "2031", "text": "5. Long Beach will practice ethical data stewardship throughout the data lifecycle to\nminimize misuse of personal data.\na. Long Beach will anonymize, deidentify, and/or aggregate personal information for\nany City purposes when access to individual records is not expressly needed.\nb. Long Beach will work to ensure residents can access and correct their personal\ndata and provide individuals with the ability to opt out of data collection (without\njeopardizing City service quality) when it is not required for a City service.\nc. Long Beach will securely retain and store data only as long as it is needed and in a\nmanner that is consistent with both applicable laws and the context in which it was\ncollected.", "tags": ["Risk factors: Privacy", "Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://www.longbeach.gov/globalassets/smart-city/media-library/documents/final_data-privacy-guidelines", "official_name": "City of Long Beach Data Privacy Guidelines (2021)", "label": "safe"}
{"id": "2038_2", "doc_id": "2038", "text": "Definitions\n\nARTICLE 2 – For the purposes of this Law;\n\nArtificial Intelligence: Refers to computer-based systems capable of performing human-like cognitive functions, and have capabilities such as learning, reasoning, problem-solving, sensing, and understanding language.\n\nProvider: Refers to natural or legal persons who develop, produce, and market artificial intelligence systems.\n\nDeployer/User: Refers to natural or legal persons who distribute artificial intelligence systems for commercial purposes or use them within their own operations.\n\nImporter: Refers to natural or legal persons who import artificial intelligence systems from abroad.\n\nDistributor: Refers to natural or legal persons who market and sell artificial intelligence systems.\n\nArtificial Intelligence Operators: Refers to all providers, deployers, users, importers, and distributors.", "tags": ["Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Transparency", "Harms: Discrimination", "Harms: Harm to health/safety", "Risk factors: Privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Incentives: Fines"], "source": "https://cdn.tbmm.gov.tr/KKBSPublicFile/D28/Y2/T2/WebOnergeMetni/e50ccc8a-ab90-45fa-a553-76b880c78fb8.pdf", "official_name": "Artificial Intelligence Law Proposal (Grand National Assembly of Turkey)", "label": "safe"}
{"id": "2038_4", "doc_id": "2038", "text": "Risk Management and Assessment\n\nARTICLE 4 – Risk assessments shall be conducted during the development and use of artificial intelligence systems, and special measures shall be adopted for systems that pose high risks.\n\nHigh-risk artificial intelligence systems shall be registered with the relevant supervisory authorities and subjected to a conformity assessment.\n\nadherence and Supervision\n\nARTICLE 5 – Artificial intelligence operators are obligated to comply with the provisions of this Law and related regulations.\n\nSupervisory authorities possess the necessary powers to monitor the adherence of artificial intelligence systems with this Law and to detect violations.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment"], "source": "https://cdn.tbmm.gov.tr/KKBSPublicFile/D28/Y2/T2/WebOnergeMetni/e50ccc8a-ab90-45fa-a553-76b880c78fb8.pdf", "official_name": "Artificial Intelligence Law Proposal (Grand National Assembly of Turkey)", "label": "safe"}
{"id": "2038_5", "doc_id": "2038", "text": "Violations and Sanctions\n\nARTICLE 6 – The following sanctions shall be applicable to artificial intelligence operators who act contrary to the provisions of this Law:\n\nA charge of up to 35 million Turkish Liras1 or up to 7% of annual turnover for restricted artificial intelligence applications.\n\nA charge of up to 15 million Turkish Liras2 or up to 3% of annual turnover for breaches of obligations.\n\nA charge of up to 7.5 million Turkish Liras3 or up to 1.5% of annual turnover for providing false information.\n\nEnforcement\n\nARTICLE 7 – This Law shall enter into force on the date of its publication.\n\nExecution\n\nARTICLE 8 – The provisions of this Law shall be executed by the President.", "tags": ["Incentives: Fines"], "source": "https://cdn.tbmm.gov.tr/KKBSPublicFile/D28/Y2/T2/WebOnergeMetni/e50ccc8a-ab90-45fa-a553-76b880c78fb8.pdf", "official_name": "Artificial Intelligence Law Proposal (Grand National Assembly of Turkey)", "label": "safe"}
{"id": "2040_3", "doc_id": "2040", "text": "2 - Assessing the Capabilities of Frontier Models \n\nWe intend to evaluate our most powerful frontier models regularly to check whether their AI capabilities are approaching a CCL. We also intend to evaluate any of these models that could indicate an exceptional increase in capabilities over previous models, and where appropriate, assess the likelihood of such capabilities and risks before and during training.  \n \nTo do so, we will define a set of evaluations called “early warning evaluations,” with a specific “alert threshold” that flags when a CCL may be reached before the evaluations are run again. In our evaluations, we seek to equip the model with appropriate scaffolding and other augmentations to make it more likely that we are also assessing the capabilities of systems that will likely be produced with the model. We may run early warning evaluations more frequently or adjust the alert threshold of our evaluations if the rate of progress suggests our safety buffer is no longer adequate.  \n \nWhere necessary, early warning evaluations may be supplemented by other evaluations to better understand model capabilities relative to our CCLs. We may use additional external evaluators to test a model for relevant capabilities, if evaluators with relevant expertise are needed to provide an additional signal about a model’s proximity to CCLs.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application"], "source": "https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/updating-the-frontier-safety-framework/Frontier%20Safety%20Framework%202.0%20(1).pdf", "official_name": "Google DeepMind Frontier Safety Framework Version 2.0", "label": "safe"}
{"id": "2040_4", "doc_id": "2040", "text": "3 - Applying Mitigations \n\nWhen a model reaches an alert threshold for a CCL, we will assess the proximity of the model to the CCL and analyze the risk posed, involving internal and external experts as needed. This will inform the formulation and application of a response plan.  \n \nCentral to most response plans will be the application of the mitigations described later in this document. For misuse, we have two categories of mitigations: security mitigations intended to prevent the exfiltration of model weights, and deployment mitigations (such as safety charge-tuning and misuse filtering, detection, and response) intended to counter the misuse of critical capabilities in deployments. For deceptive alignment risk, automated monitoring may be applied to detect and respond to deceptive behavior for models that meet the first deceptive alignment CCL. Note that these mitigations reflect considerations from the perspective of addressing severe risks from powerful capabilities alone; due to this focused scope, other risk management and security considerations may result in more stringent mitigations applied to a model than specified by the Framework.  \n \nA model flagged by an alert threshold may be assessed to pose risks for which readily available mitigations (including but not limited to those described below) may not be sufficient. If this happens, the response plan may involve putting deployment or further development on hold until adequate mitigations can be applied. Conversely, where model capabilities remain quite distant from a CCL, a response plan may involve the adoption of additional capability assessment processes to flag when heightened mitigations may be required.  \n \nThe appropriateness and efficacy of applied mitigations should be reviewed periodically, drawing on information like related misuse or misuse attempt incidents; results from continued post-mitigation testing; statistics about our intelligence, monitoring and escalation processes; and updated threat modeling and risk landscape analysis.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Risk factors: Security", "Risk factors: Safety", "Harms: Harm to health/safety", "Harms: Detrimental content", "Strategies: Evaluation: Impact assessment"], "source": "https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/updating-the-frontier-safety-framework/Frontier%20Safety%20Framework%202.0%20(1).pdf", "official_name": "Google DeepMind Frontier Safety Framework Version 2.0", "label": "safe"}
{"id": "2040_5", "doc_id": "2040", "text": "Misuse \n\nThis section describes our mitigation approach for models that pose risks of severe harm through misuse, and then details our set of misuse CCLs, as well as the mitigations that we provisionally assess as appropriate for them. \n \nMitigation Approach \nThere are two categories of mitigations to address models with misuse critical capabilities: security mitigations intended to prevent the exfiltration of model weights, and deployment mitigations intended to counter the misuse of critical capabilities in deployments. For security, we have several levels of mitigations, allowing calibration of the robustness of security measures to the risks posed. For deployment mitigations, we specify a standard process for applying, assessing, and reviewing mitigations: the aim of this process is to calibrate mitigations to CCLs, and the procedural approach reflects the more iterative and CCL-dependent nature of deployment mitigations. \n \nSecurity Mitigations \nSecurity mitigations against exfiltration risk are important for models reaching CCLs. This is because the release of model weights may enable the removal of any safeguards trained into or deployed with the model, and hence access (including by threat actors) to any critical capabilities. Here, we rely on the RAND framework  to articulate the level of security recommended for each CCL. When we reference RAND security levels, we are referring to the security principles in their framework, rather than the benchmarks (i.e. concrete measures) also described in the RAND report.  Because AI security is an area of active research, we expect the concrete measures implemented to reach each level of security to evolve substantially.  \n \nDeployment Mitigations \nThe following deployment mitigation process will be applied to models reaching a CCL, allowing for iterative and flexible tailoring of mitigations to each risk and use case.  \n \n1.\tDevelopment and assessment of mitigations: safeguards and an accompanying safety case  are developed by iterating on the following: \n   a.\tDeveloping and improving a suite of safeguards targeting the capability. This includes, as appropriate, safety charge-tuning, misuse filtering and detection, and response protocols. \n   b.\tAssessing the robustness of these mitigations against the risk posed through assurance evaluations and threat modeling research. The assessment takes the form of a safety case, taking into account factors such as the likelihood and consequences of misuse. \n2.\tPre-deployment review of safety case: general availability deployment8 of a model takes place only after the appropriate corporate governance body determines the safety case regarding each CCL the model has reached to be adequate. \n3.\tPost-deployment review of safety case: the safety case will be updated through red-teaming and revisions to our threat models. The safeguards for the model may be updated as well to ensure continued adequacy.", "tags": ["Risk factors: Security", "Risk factors: Safety", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation: Adversarial testing"], "source": "https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/updating-the-frontier-safety-framework/Frontier%20Safety%20Framework%202.0%20(1).pdf", "official_name": "Google DeepMind Frontier Safety Framework Version 2.0", "label": "safe"}
{"id": "2040_10", "doc_id": "2040", "text": "Deceptive Alignment \n\nThis section describes an initial approach for addressing risks of deceptive alignment. By \"deceptive alignment,\" we mean the risk that AI systems purposefully undermine human control over AI systems. While we do not express any opinion here about how likely it is for deceptive alignment to arise, this section’s provisions aim to prepare for this possibility.  \n \nNote that the effective mitigation of deceptive alignment risk requires the application of safeguards to all AI systems reaching critical capabilities. It is important that the industry adopts strategies for addressing this risk: what is detailed in this section represents one possible approach for doing so. \n \nMitigation Approach and Critical Capability Levels \n\nAn initial mitigation approach focuses on detecting when models might develop a baseline instrumental reasoning ability at which they have the potential to undermine human control, assuming no additional mitigations were applied. When models reach this capability level, we believe applying an automated monitor to the model’s explicit reasoning (e.g. chain-of-thought output) is an effective mitigation. Once a model is capable of effective instrumental reasoning in ways that cannot be monitored, additional mitigations may be warranted—the development of which is an area of active research.  \n \nThe table below summarizes the initial approach: \n \nTable 2: Deceptive Alignment CCLs and Mitigations \n\nDeceptive Alignment CCL: Instrumental Reasoning Level 1: the instrumental reasoning abilities of the model enable enough situational awareness (ability to work out and use relevant details of its deployment setting) and stealth (ability to circumvent basic oversight mechanisms) such that, absent additional mitigations, we cannot rule out the model significantly undermining human control.\nMitigation: Automated monitoring: a monitoring system to detect illicit use of instrumental reasoning capabilities in a model’s chain-of-thought and use it in high-stakes deployments where there is significant risk of the model undermining human control. \n \nDeceptive Alignment CCL: Instrumental Reasoning Level 2: the instrumental reasoning abilities of the model enable enough situational awareness and stealth that, even when relevant model outputs (including, e.g., scratchpads) are being monitored, we cannot detect or rule out the risk of a model significantly undermining human control.\nMitigation: Future work: We are actively researching approaches to addressing models that reach this CCL.", "tags": ["Risk factors: Reliability", "Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring"], "source": "https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/updating-the-frontier-safety-framework/Frontier%20Safety%20Framework%202.0%20(1).pdf", "official_name": "Google DeepMind Frontier Safety Framework Version 2.0", "label": "safe"}
{"id": "2040_11", "doc_id": "2040", "text": "Safety Cases and Control Evaluations \n\nLooking forward, we expect the approach to deceptive alignment risks to take a similar form as the  deployment mitigation process. For each deceptive alignment risk, AI developers should: \n- Develop a safety case based on the model’s capabilities and the mitigations applied and tested. Such safety cases would make the case that the likelihood of deceptive alignment risk would be low enough for safe deployment, even if the model were trying to meaningfully undermine human control. \n- Test the mitigations applied to models via red-teaming protocols called “control evaluations,”  to determine whether they meet the requirements of the safety case. \n \nThe approach above relies on two safety cases centering respectively on the claims that: \n1.\tA model does not meet a threshold CCL and therefore would be incapable of substantially undermining human control. \n2.\tAn automated monitoring process that is validated by a control evaluation is adequate for catching the model’s malicious behavior. \n \nOnce neither of these apply, that is, once automated monitoring is no longer adequate/can no longer pass a control evaluation, a model is deemed to have reached the second CCL.", "tags": ["Risk factors: Safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Adversarial testing"], "source": "https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/updating-the-frontier-safety-framework/Frontier%20Safety%20Framework%202.0%20(1).pdf", "official_name": "Google DeepMind Frontier Safety Framework Version 2.0", "label": "safe"}
{"id": "2041_8", "doc_id": "2041", "text": "(e) The court is authorized to award reasonable attorney’s fees to a plaintiff who brings a successful action for a infraction of this section.\n(f) In a civil action brought pursuant to this section, once it has been demonstrated by a preponderance of the evidence that an activity proscribed by this section was a contributing factor in the alleged restricted action against the employee, the developer shall have the burden of proof to demonstrate by clear and convincing evidence that the alleged action would have occurred for legitimate, independent reasons even if the employee had not engaged in activities protected by this section.\n(g) (1) In a civil action or administrative proceeding brought pursuant to this section, an employee may petition the superior court in any county wherein the infraction in question is alleged to have occurred, or wherein the person resides or transacts business, for appropriate temporary or preliminary injunctive relief.\n(2) Upon the filing of the petition for injunctive relief, the petitioner shall cause notice thereof to be served upon the person, and thereupon the court shall have jurisdiction to financial support allocation temporary injunctive relief as the court deems just and proper.\n(3) In addition to any harm resulting directly from a infraction of this section, the court shall consider the chilling effect on other employees asserting their rights under this section in determining whether temporary injunctive relief is just and proper.\n(4) Appropriate injunctive relief shall be issued on a showing that reasonable cause exists to believe a infraction has occurred.\n(5) An order authorizing temporary injunctive relief shall remain in effect until an administrative or judicial determination or citation has been issued, or until the completion of a review pursuant to subdivision (b) of Section 98.74, whichever is longer, or at a certain time set by the court. Thereafter, a preliminary or permanent injunction may be issued if it is shown to be just and proper. Any temporary injunctive relief must avoid restrict a developer from disciplining or terminating an employee for conduct that is unrelated to the claim of the retaliation.\n(h) Notwithstanding Section 916 of the Code of Civil Procedure, injunctive relief granted pursuant to this section must avoid be stayed pending appeal.", "tags": ["Incentives: Civil liability"], "source": "https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202520260SB53", "official_name": "California SB 53", "label": "safe"}
{"id": "2096_5", "doc_id": "2096", "text": "Article 7: When internet application distribution platforms conduct reviews for placing applications on the market or online, they shall request the internet application service provider to provide an explanation of whether it provides generative AI services. 　　Where internet application service providers are providing generative AI services, the internet application distribution platform shall check materials related to the labeling of generated synthetic content. 　　\n\nArticle 8: Service providers shall clearly explain the methods, styles, and other such specifications for labeling generated synthetic content in user service agreements, and notify users to carefully read and understand the corresponding labeling management requirements. 　　\n\nArticle 9: Where users request that service providers provide generated synthetic content without explicit labels added, then, after clearly indicating users' labeling obligations and usage responsibilities through the service agreement, they may provide generated synthetic content that does not contain explicit labels and are to store the relevant logs for at least 6 months in accordance with law. 　　\n\nArticle 10: Where users use online information content transmission services to publish generated synthetic content, they shall proactively declare it and use the labeling functions provided by the platform to make labels. 　　\n\nThe generated synthetic content labels provided for in these Measures must not be maliciously deleted, altered, fabricated, or concealed by any organization or individual, tools and services must not be provided for others to carry out the malicious conduct described above, and the lawful rights and interests of others must not be harmed through the improper labeling.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Risk factors: Transparency", "Harms: Detrimental content"], "source": "https://www.chinalawtranslate.com/en/ai-labeling/", "official_name": "Measures for Labeling of AI-Generated Synthetic Content", "label": "safe"}
{"id": "2096_6", "doc_id": "2096", "text": "Article 11: Where service providers carry out labeling activities, they shall comply with relevant laws, administrative regulations, departmental rules, and the requirements of mandatory national standards. 　　\n\nArticle 12: When service providers perform procedures such as for filing algorithms or for safety assessments, they shall provide materials related to labeling generated synthetic content in accordance with these Measures, and strengthen label information sharing to prevent and combat the provision of support and assistance for related illegal and criminal activities. 　　\n\nArticle 13: Where the provisions of these Measures are violated, relevant regulatory departments such as for internet information, telecommunications, public security, television, and radio are to address it in accordance with relevant laws, administrative regulations, and departmental rules. 　　\n\nArticle 14: These measures are to take effect from September 1, 2025.", "tags": ["Incentives: Civil liability", "Risk factors: Safety", "Strategies: Evaluation"], "source": "https://www.chinalawtranslate.com/en/ai-labeling/", "official_name": "Measures for Labeling of AI-Generated Synthetic Content", "label": "safe"}
{"id": "2263_2", "doc_id": "2263", "text": "Chapter 2\tReviewers\n\nArticle 4\t Institutes of higher education, scientific research institutions, medical and health institutions, and enterprises, among others, are the entities responsible for the management of S&T ethics reviews for their work units (单位). Work units engaged in S&T activities in areas such as life sciences, medicine, and artificial intelligence (AI), and whose research involves sensitive S&T ethics areas, shall establish S&T ethics (review) committees. Other work units with S&T ethical review requirements may establish S&T ethics (review) committees based on their actual situations.\n\nA work unit shall provide its S&T ethics (review) committee necessary staff, office space, financial support, and other prerequisites, and take effective measures to ensure that the S&T ethics (review) committee carries out ethical review work independently.\n\nThe establishment of professional and regional S&T ethics review centers should be explored.\n\nArticle 5\t The main duties of the S&T ethics (review) committee include:\n\n(1) Formulating and improving the management system and work norms of the S&T ethics (review) committee;\n\n(2) Providing consultation on S&T ethics and guiding scientific and technical personnel in conducting S&T ethical risk assessments of S&T activities;\n\n(3) Conducting S&T ethics reviews and tracking and supervising the entire process of relevant S&T activities in accordance with requirements;\n\n(4) Determining whether proposed S&T activities fall within the scope of the list defined in Article 25 of these Measures;\n\n(5) Organizing and conducting training for committee members on S&T ethics reviews, and training for scientific and technical personnel on S&T ethics;\n\n(6) Accepting and assisting in investigating complaint reports on issues involving S&T ethics issues in relevant S&T activities;\n\n(7) Carrying out registration and reporting in accordance with the requirements of Articles 43, 44, and 45 of these Measures and cooperating with the local and relevant main industrial oversight departments (地方、相关行业主管部门) in conducting relevant work involving S&T ethics reviews.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Convening", "Strategies: New institution"], "source": "https://cset.georgetown.edu/publication/china-science-ethics-review-trial-measures/", "official_name": "(Trial) Measures for Science and Technology Ethics Reviews", "label": "safe"}
{"id": "2263_4", "doc_id": "2263", "text": "Chapter 3\tReview Procedures\n\nSection 1\tApplication and Acceptance\n\nArticle 9\tS&T ethics risk assessments shall be conducted to carry out S&T activities. The S&T ethics (review) committee shall formulate the work unit's S&T ethics risk assessment methods in accordance with the requirements of these Measures, and guide S&T personnel in conducting S&T ethics risk assessments. If an assessment finds that an S&T activity falls within the scope of Article 2 of these Measures, the person in charge of the S&T activity shall apply to the S&T ethics (review) committee for an S&T ethics review. The application materials shall mainly include:\n\n(1) An overview of the S&T activity, including the name, purpose, significance, necessity, and previous S&T ethics reviews of the S&T activity;\n\n(2) The implementation plan and related materials for the S&T activity, including the S&T activity plan, possible S&T ethical risks and their prevention and control measures and emergency response plans, and the form in which the achievements of S&T activities are to be announced;\n\n(3) The legal qualification materials of the relevant institutions involved in the S&T activity, the relevant research experience and participation in S&T ethics training of participants, the source of financial support for the S&T activity, conflict of interest declarations for the S&T activity, etc.;\n\n(4) Informed consent forms, and materials explaining the sources of biological samples, data and information, and laboratory animals;\n\n(5) A written commitment to comply with the requirements of S&T ethics and scientific research integrity; and\n\n(6) Other materials that the S&T ethics (review) committee deems necessary to submit.\n\nArticle 10\tThe S&T ethics (review) committee shall decide whether to accept the application based on the S&T ethics review application materials, and shall notify the applicant. A decision to accept shall specify the applicable review procedures; and if the materials are incomplete, a one-time and complete notification of the required supplementary materials shall be made.\n\nArticle 11\tIn principle, S&T ethics reviews shall adopt the meeting-based review method, except as otherwise provided in these Measures.\n\nArticle 12\tWhere S&T activities with international cooperation fall within the scope of Article 2 of these Measures, such activities must pass the S&T ethics reviews prescribed by the countries where the cooperating parties are located before they may be conducted.\n\nArticle 13\tIn the case of a work unit where the S&T ethics (review) committee fails to meet the requirements for review work, or the work unit has not established an S&T ethics (review) committee and has no personnel to carry out S&T activities, it shall commission, in writing, another S&T ethics (review) committee that satisfies the requirements to conduct ethics reviews.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Impact assessment"], "source": "https://cset.georgetown.edu/publication/china-science-ethics-review-trial-measures/", "official_name": "(Trial) Measures for Science and Technology Ethics Reviews", "label": "safe"}
{"id": "2263_5", "doc_id": "2263", "text": "Section 2\tGeneral Procedures\n\nArticle 14\tThe S&T ethics review meeting shall be presided over by the chair or his or her designated deputy chair. There shall be no fewer than five members present, and they shall include members from the different categories listed in Article 7.\n\nBased on the requirements of the review, the applicant may be required to attend the meeting in order to elaborate on the plan or explain specific issues. Consultants and experts from relevant fields who do not have a direct interest may be invited to provide advice. Consultants and experts must avoid participate in voting at the meeting.\n\nWhere the meeting is conducted by video, it shall comply with the relevant rules and requirements of the S&T ethics (review) committee on conditions for the use of video conferencing, meeting rules, etc.\n\nArticle 15\tThe S&T ethics (review) committee shall conduct its reviews in accordance with the following key content and standards:\n\n(1) The proposed S&T activity shall comply with the principles of S&T ethics, and the qualifications of the scientific and technical personnel participating in the S&T activity, and the research infrastructure and facilities, shall satisfy the relevant requirements.\n\n(2) The proposed S&T activity has scientific and social value, and the achievement of its research objectives would have a positive effect for advancing human well-being and achieving sustainable social development. The S&T activity has reasonable risks relative to benefits, and the ethical risk control plan and emergency plan are scientific, appropriate, and operational.\n\n(3) For S&T activities involving humans as research participants, the recruitment plan is fair and reasonable, the collection, storage, use, and processing of biological samples are lawful and compliant, the handling of private personal data, biometric information, and other information complies with relevant regulations on personal information protection; plans for the assurance of research participants’ legal rights and interests, including remuneration and the treatment or compensation of injuries, are reasonable, and special protection is given to vulnerable groups; the informed consent form provided is complete, the risk disclosures are objective and sufficient, the presentation is clear and easy to understand, and the method and process of obtaining individuals’ informed consent is compliant and appropriate.\n\n(4) For S&T activities involving laboratory animals, the use of laboratory animals complies with the principles of replacement, reduction, and optimization (替代、减少、优化), the sources of laboratory animals are lawful and reasonable, the technical operation requirements for feeding, use, and handling comply with animal welfare standards, and safeguards for the safety of employees and the public environment are appropriate.\n\n(5) For S&T activities involving data and algorithms, data handling activities such as collection, storage, processing, and use, as well the exploratory work of new data technology, comply with relevant national regulations on data security, personal information protection, etc., and data security risk monitoring and emergency response plans are appropriate; the design, implementation, and application of algorithms, models, and systems complies with the principles of fairness, justice, transparency, reliability, and controllability; ethical risk assessment and emergency response plans are reasonable, and measures to protect the rights and interests of users are comprehensive and appropriate.\n\n(6) The formulated conflict of interest statement and management plan are reasonable.\n\n(7) Other content that the S&T ethics (review) committee deems necessary to review.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Impact assessment"], "source": "https://cset.georgetown.edu/publication/china-science-ethics-review-trial-measures/", "official_name": "(Trial) Measures for Science and Technology Ethics Reviews", "label": "safe"}
{"id": "2263_6", "doc_id": "2263", "text": "Article 16\tThe S&T ethics (review) committee may decide to approve, approve with modifications, review again after modifications, or disapprove the reviewed S&T activity. Where the decision is to approve with modifications or review again after modifications, it shall propose modifications and clearly specify the modification requirements; and where the decision is to disapprove, it shall state the reasons.\n\nA review decision made by the S&T ethics (review) committee shall be approved by at least two-thirds of the members present.\n\nArticle 17\tThe S&T ethics (review) committee shall generally make a review decision within 30 days after accepting the application, but it may be extended appropriately in special circumstances, with the extended time limit expressly stated. The review decision shall be delivered to the applicant in a timely manner.\n\nArticle 18\tWhere the applicant objects to the review decision, the applicant may file a written appeal with the S&T ethics (review) committee that made the decision, stating the reasons and providing relevant supporting materials. Where the grounds for appeal are sufficient, the S&T ethics (review) committee shall make a new review decision in accordance with the provisions of these Measures.\n\nArticle 19\tThe S&T ethics (review) committee shall conduct ethics follow-up reviews of reviewed and approved S&T activities, and when necessary, may make decisions such as suspending or terminating S&T activities. The interval before a follow-up review shall generally not exceed 12 months.\n\nThe main contents of the follow-up review include:\n\n(1) The implementation and adjustment circumstances of the S&T activity implementation plan;\n\n(2) The implementation status of S&T ethical risk prevention and control measures;\n\n(3) Potential changes in S&T ethical risks, and circumstances that may affect the rights and interests or the safety of research participants; and\n\n(4) Other content requiring follow-up review.\n\nThe S&T ethics (review) committee may require the person in charge of the S&T activity to submit relevant materials as required for the follow-up review.\n\nArticle 20\tWhere adjustments to the implementation plan of an S&T activity, changes in the external environment, etc., may have caused changes in S&T ethical risks to occur, the person in charge of the S&T activity shall promptly report to the S&T ethics (review) committee. The S&T ethics (review) committee shall conduct an assessment of the risks and benefits, suggest whether to continue or suspend implementation, and, when necessary, conduct a new ethics review.\n\nArticle 21\tWhere multiple work units cooperate to carry out an S&T activity the lead work unit may, based on the actual situation, establish mechanisms for S&T ethics review collaboration and mutual recognition of results, in order to strengthen the coordinated management of S&T ethics.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment"], "source": "https://cset.georgetown.edu/publication/china-science-ethics-review-trial-measures/", "official_name": "(Trial) Measures for Science and Technology Ethics Reviews", "label": "safe"}
{"id": "2263_8", "doc_id": "2263", "text": "Section 4\tExpert Reconsideration Procedures\n\nArticle 25\tA checklist system for S&T activities requiring expert reconsideration shall be established, and checklist-style management (清单管理) shall be implemented for emerging S&T activities that may generate significant ethical risks and challenges. The checklist shall be adjusted dynamically according to work requirements, and shall be published by the Ministry of Science and Technology (MOST).\n\nArticle 26\tTo conduct S&T activities that are included in checklist-style management, after a preliminary review by the S&T ethics (review) committee is passed, the work unit concerned shall submit a request for expert reconsideration to the local or relevant main industrial oversight department. Where multiple work units are involved, the lead work unit shall compile materials and apply to the local or relevant main industrial oversight department for expert reconsideration.\n\nArticle 27\tWhere expert reconsideration is applied for, the work unit undertaking the S&T activity shall organize the submission of the following materials by the S&T ethics (review) committee and scientific and technical personnel, in accordance with requirements:\n\n(1) Materials listed in Article 9 of these Measures;\n\n(2) The preliminary review opinion of the S&T ethics (review) committee;\n\n(3) Other relevant materials required by the work unit organizing the reconsideration.\n\nArticle 28\tThe local or relevant main industrial oversight department shall organize and establish an expert reconsideration panel, which shall consist of no fewer than five peer experts with high academic standing in fields related to the S&T activity, as well as experts in areas such as ethics and law. Members of S&T ethics (review) committees must avoid participate in reconsideration work for S&T activities reviewed by their own committees.\n\nReconsideration experts shall take the initiative to declare whether they have a direct interest in the matter under reconsideration, and strictly abide by secrecy protection provisions and recusal requirements.\n\nArticle 29\tThe expert reconsideration panel shall conduct its reconsideration in accordance with the following key content and standards:\n\n(1) adherence of the initial review opinion. The initial review opinion shall comply with China’s laws, administrative regulations, relevant national provisions, and S&T ethics requirements.\n\n(2) Reasonableness of the initial review opinion. The initial review opinion shall fully, adequately, appropriately, and reasonably assess the potential ethical risks and prevention and control measures of the S&T activity, taking into account the requirements of technology development and the actual situation of China’s S&T development.\n\n(3) Other content that the expert reconsideration panel deems necessary to review.", "tags": ["Strategies: Convening", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment"], "source": "https://cset.georgetown.edu/publication/china-science-ethics-review-trial-measures/", "official_name": "(Trial) Measures for Science and Technology Ethics Reviews", "label": "safe"}
{"id": "2263_9", "doc_id": "2263", "text": "Article 30\tThe expert reconsideration panel shall conduct the reconsideration in an appropriate manner and may, if necessary, require the relevant S&T ethics (review) committee and scientific and technical personnel to explain the relevant circumstances.\n\nThe expert reconsideration panel shall issue a reconsideration opinion of either approval or disapproval, and the reconsideration opinion shall be approved by at least two-thirds of all reconsideration experts.\n\nArticle 31\tThe local or relevant main industrial oversight department shall generally provide feedback to the applicant within 30 days of receipt of the reconsideration application.\n\nArticle 32\tThe work unit’s S&T ethics (review) committee shall make an S&T ethics review decision based on the expert reconsideration opinion.\n\nArticle 33\tThe work unit’s S&T ethics (review) committee shall strengthen the follow-up review and dynamic management of the S&T activities conducted by the work unit that are included in checklist-style management, and the interval before a follow-up review shall generally not exceed six months.\n\nWhere there is a major change in S&T ethical risk, a new ethics review shall be carried out in accordance with the provisions of Article 20 of these Measures, and application shall be made for expert reconsideration.\n\nArticle 34\tIf the State implements regulatory measures such as administrative examination and approval for relevant S&T activities included in checklist-style management, and adherence with ethical requirements is a condition for approval and part of the regulatory content, further expert reconsiderations for such activities need not be conducted. Examination and approval departments and regulatory departments, and work units undertaking S&T activities, shall strictly implement their ethics regulation responsibilities and prevent and control ethical risks.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment"], "source": "https://cset.georgetown.edu/publication/china-science-ethics-review-trial-measures/", "official_name": "(Trial) Measures for Science and Technology Ethics Reviews", "label": "safe"}
{"id": "2263_12", "doc_id": "2263", "text": "Article 46\tRegarding conduct in S&T activities that violates S&T ethical norms or requirements, any work unit or individual shall be entitled to report complaints, in accordance with law, to the work units undertaking the S&T activities or to the local or relevant main industrial oversight department.\n\nArticle 47\tWhere a work unit undertaking S&T activities, or its scientific and technical personnel, violates the provisions of these Measures, and any of the following circumstances exists, the agency with jurisdiction shall impose consequences or take other handling measures in accordance with laws, administrative regulations, and relevant provisions; where property loss or other damage has been caused, civil responsibility shall be borne according to law; and where a crime has been committed, criminal responsibility shall be pursued according to law.\n\n(1) Obtaining S&T ethics review approval through fraud or forgery, or falsifying or tampering with documents related to S&T ethics review;\n\n(2) Carrying out an S&T activity included in checklist-style management without having passed the S&T ethics review and expert reconsideration as required by regulations;\n\n(3) Conducting an S&T activity without obtaining S&T ethics review and approval as required;\n\n(4) Conducting an S&T activity beyond the scope approved by the S&T ethics review;\n\n(5) Interfering with or obstructing S&T ethics review work;\n\n(6) Other actions that violate the provisions of these Measures.\n\nArticle 48\tWhere the S&T ethics (review) committee or a member thereof violates the provisions of these Measures and any of the following circumstances exist, the agency with jurisdiction shall impose consequences or take other measures in accordance with laws, administrative regulations, and relevant provisions; where property loss or other damage has been caused, civil responsibility shall be borne according to law; and where a crime has been committed, criminal responsibility shall be pursued according to law.\n\n(1) Falsification of materials to aid the work units undertaking S&T activities in obtaining S&T ethics review and approval;\n\n(2) Engagement in favoritism or irregularities for personal gain, abuse of power, or neglect of duty;\n\n(3) Other violations of the provisions of these Measures.", "tags": ["Incentives: Civil liability", "Incentives: Criminal liability"], "source": "https://cset.georgetown.edu/publication/china-science-ethics-review-trial-measures/", "official_name": "(Trial) Measures for Science and Technology Ethics Reviews", "label": "safe"}
{"id": "2267_14", "doc_id": "2267", "text": "4. FOSTERING PUBLIC TRUST IN FEDERAL USE OF AI \n\n\nAgencies must continue to develop AI that serves the public by, for example, increasing the accessibility of government services, increasing government efficiency, enhancing national security, and growing American economic competitiveness in a way that benefits people across the United States. Agencies must ensure their AI use is trustworthy, secure, and accountable, in accordance with Executive Order 13960. In the pursuit of agency-wide AI adoption and use, the Federal workforce at varying levels will participate in AI or AI-enabling roles, with accountable officials assuming risk. As part of this effort, AI risk management policies must be written to both ensure the minimum number of requirements necessary to enable the trustworthy and responsible use of AI and also ensure those requirements are understandable and implementable. \n\n\nAgencies are required to implement minimum risk management practices, detailed in Section 4(b) of this memorandum, to manage risks from high-impact AI use cases. However, Sections 4(a) through (b) of this memorandum do not apply to elements of the Intelligence Community. Consistent with these goals, agencies must undertake the following, in addition to following 0MB standards and requirements governing information dissemination, where applicable.", "tags": ["Risk factors: Reliability", "Risk factors: Security"], "source": "https://www.whitehouse.gov/wp-content/uploads/2025/02/M-25-21-Accelerating-Federal-Use-of-AI-through-Innovation-Governance-and-Public-Trust.pdf", "official_name": "Memorandum 25-21 (Accelerating Federal Use of AI through Innovation, Governance, and Public Trust)", "label": "safe"}
{"id": "2267_17", "doc_id": "2267", "text": "b. Implementing Minimum Risk Management Practices for High-Impact AI \n\n\nAgencies must implement the following minimum risk management practices for highimpact AI use cases: \n\n\ni. Conduct Pre-Deployment Testing. Agencies must develop pre-deployment testing and prepare risk mitigation plans that reflect expected real-world outcomes and identify expected benefits to the AI use. In conducting pre-deployment testing, if an agency does not have access to the underlying AI source code, models, or data, the agency must use alternative test methodologies, such as querying the AI service and observing the outputs or providing evaluation data to the vendor and obtaining results. \n\n\nii. Complete AI Impact Assessment. Agencies must complete an AI impact assessment before deploying any high-impact AI use case. These assessments must be updated periodically and throughout the AI' s lifecycle, as appropriate, using target variables that anticipate real-world outcomes. The AI impact assessments must be documented and address or include, at a minimum: \nA. the intended purpose for the AI and its expected benefit, supported by specific metrics or qualitative analysis, assessing impact inclusive of but not limited to costs, customer experience, or expected positive outcomes of AI use, as compared to existing agency processes; \nB. the quality and appropriateness ofthe relevant data and model capability, supported by a summary of the data used in the AI' s design, development, training, testing, and operation and its fitness for the AI' s intended purpose; describe the data collection, and preparation process; and indicate whether the data is to be publicly disclosed as an open government data asset. When applicable, this summary must describe information included in the data about classes protected by Federal nondiscrimination laws; \nC. the potential impacts ofusing AI, supported by documentation on potential impacts on the privacy, civil rights, and civil liberties of the public, and of using or not using AI. The assessment should reference privacy impact assessments, CAIO-approved minimum risk management practice waivers or other materials, if relevant, and also describe any planned mitigation measures for anticipated negative impacts, such as unlawful discrimination;\nD. reassessment scheduling and procedures, supported by schedules for periodic reassessments as well as reassessment requirements following significant modifications to an underlying AI system, in addition to the specific requirements and processes for such testing; \nE. related costs analysis, supported by a summary of direct costs associated and expected savings, if any; \nF. results of independent review, supported by an independent reviewer within the agency who has not been involved in the development. The independent reviewer of the impact assessment shall identify any potential concerns or gaps. Any comments provided by the independent reviewer must be included in the impact assessment documentation and shared with the individual accepting the risk for the AI use case when that determination is made; and \nG. risk acceptance, supported by a signature from the individual accepting the risk.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Risk factors: Reliability", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Bias", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation: External auditing", "Applications: Government: other applications/unspecified", "Strategies: Disclosure"], "source": "https://www.whitehouse.gov/wp-content/uploads/2025/02/M-25-21-Accelerating-Federal-Use-of-AI-through-Innovation-Governance-and-Public-Trust.pdf", "official_name": "Memorandum 25-21 (Accelerating Federal Use of AI through Innovation, Governance, and Public Trust)", "label": "safe"}
{"id": "2267_23", "doc_id": "2267", "text": "7. METHODS OF UNDERSTANDING AI RISK MANAGEMENT \n\n\nBelow are ways in which risks may arise from the use of AI. The term \"risks from the use of AI\" refers to risks related to efficacy, safety, fairness, transparency, accountability, appropriateness, or lawfulness of a decision or action resulting from the use of AI to inform, influence, decide, or execute that decision or action. \n\n\nThis includes such risks regardless of whether: \n1. the AI merely informs the decision or action, partially automates it, or fully automates it; \n2. there is or is not human oversight for the decision or action; \n3. it is or is not readily apparent that a decision or action took place, such as when an AI application performs a background task or silently declines to take an action; or \n4. the humans involved in making the decision or action or that are affected by it are or are not aware of how or to what extent the AI influenced or automated the decision or action. \n\n\nThe following factors can create, contribute to, or exacerbate risks from the use of AI: \n1. AI outputs that are inaccurate or misleading; \n2. AI outputs that are unreliable, ineffective, or not robust; \n3. AI outputs that discriminate on the basis of a protected characteristic; \n4. AI outputs that contribute to actions or decisions resulting in harmful or unsafe outcomes, including AI outputs that lower the barrier for people to take intentional and harmful actions; \n5. AI being used for tasks to which it is poorly suited or being inappropriately repurposed in a context for which it was not intended; \n6. AI being used in a context in which affected people have a reasonable expectation that a human is or should be primarily responsible for a decision or action; and \n7. the adversarial evasion or manipulation of AI, as in the case of an entity purposefully inducing AI to misclassify an input. \n\n\nThis definition applies to risks specifically arising from using AI and that affect the outcomes of decisions or actions. It does not include all risks associated with AI, such as risks related to the privacy, security, and confidentiality of the data used to train AI or used as inputs to AI models.", "tags": ["Risk factors: Safety", "Risk factors: Bias", "Risk factors: Reliability", "Risk factors: Transparency", "Harms: Detrimental content", "Harms: Harm to health/safety"], "source": "https://www.whitehouse.gov/wp-content/uploads/2025/02/M-25-21-Accelerating-Federal-Use-of-AI-through-Innovation-Governance-and-Public-Trust.pdf", "official_name": "Memorandum 25-21 (Accelerating Federal Use of AI through Innovation, Governance, and Public Trust)", "label": "safe"}
{"id": "2267_24", "doc_id": "2267", "text": "8. Public Consultation and Feedback \n\n\nTo carry out public consultations and feedback processes, agencies are recommended to take appropriate steps to solicit public input, which could include: \n1. direct usability testing, such as observing users interacting with the system; \n2. general solicitations of comments from the public, such as a request for information in the Federal Register or a \"Tell Us About Your Experience\" sheet with an open-ended space for responses; \n3. post-transaction customer feedback collections;\n4. public hearings or meetings; and \n5. any other transparent process that seeks public input, comments, or feedback from the affected groups in a meaningful, accessible, and effective manner. \n\n\n[omitted Consolidated Table of Actions]", "tags": ["Strategies: Convening", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation"], "source": "https://www.whitehouse.gov/wp-content/uploads/2025/02/M-25-21-Accelerating-Federal-Use-of-AI-through-Innovation-Governance-and-Public-Trust.pdf", "official_name": "Memorandum 25-21 (Accelerating Federal Use of AI through Innovation, Governance, and Public Trust)", "label": "safe"}
{"id": "2268_1", "doc_id": "2268", "text": "1. OVERVIEW \n\n\nExecutive Order 13960, Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government, 1 charges Federal agencies with using safe and secure artificial intelligence (AI) in innovative ways to improve government efficiency and mission effectiveness. In carrying out this direction, agencies must procure effective and trustworthy AI capabilities in a timely and cost-effective manner. Consistent with the Advancing American AI Act, Executive Order 14179, Removing Barriers to American Leadership in Artificial Intelligence, and Office of Management and Budget (0MB) Memorandum M-25-21, Accelerating Federal Use ofAI through advancement, Governance, and Public Trust, this memorandum provides guidance to agencies to improve their ability to acquire AI responsibly. This memorandum rescinds and replaces 0MB Memorandum M-24-18, Advancing the Responsible Acquisition of Artificial Intelligence in Government. To that end, there are three grounding themes that drive this memorandum's requirements: \n\n\nEnsuring the Government and the Public Benefit from a Competitive American Al Marketplace. Competition in the marketplace enables the government to acquire the best solutions at lower cost to the taxpayer. As agencies seek to accelerate the adoption of AI-enabled services, they must pay careful attention to vendor sourcing, data portability, and long-term interoperability to avoid significant and costly dependencies on a single vendor. The government must communicate clear and specific requirements that make it easy for vendors to offer state-of-the-art AI capabilities to support efficient and effective public services. \n\n\nSafeguarding Taxpayer Dollars by Tracking AI Performance and Managing Risk. AI presents a tremendous opportunity to improve government efficiency and effectiveness. To achieve this promise, agencies must ensure that the AI systems they procure are fit for purpose and deliver consistent results that preserve public trust in the manner outlined in Executive Order 13960. \n\n\nPromoting Effective AI Acquisition with Cross-Functional Engagement. Robust collaboration is a foundational principle of the Executive Branch's acquisition process and remains critical for surfacing potential issues sooner rather than later to avoid obstacles and risks in procuring new technology, such as AI. The novel challenges that AI introduces require agile engagement from agency officials with varied expertise to fully address during acquisition.", "tags": ["Applications: Government: other applications/unspecified", "Risk factors: Reliability"], "source": "https://www.whitehouse.gov/wp-content/uploads/2025/02/M-25-22-Driving-Efficient-Acquisition-of-Artificial-Intelligence-in-Government.pdf", "official_name": "Memorandum 25-22 (Driving Efficient Acquisition of Artificial Intelligence in Government)", "label": "safe"}
{"id": "2268_14", "doc_id": "2268", "text": "e. Contract Administration \ni. Authorization To Operate adherence. Consistent with the requirements of 0MB Circular No. A-130 and other policies established pursuant to the Federal Information Security Modernization Act, any AI systems and services operated as an information system by or on behalf of an agency must receive an authorization to operate from an appropriate agency official prior to deployment.26 \nii. Contractual Oversight. Agencies must perform effective system oversight consistent with the terms of the contract. This includes monitoring system performance to ensure that any emerging risks to privacy, civil rights, and civil liberties are identified and mitigated as appropriate. \niii. Performance and Cost Justification. As part of contract administration, agencies should, to the extent practicable, arrange for periodic evaluation of the AI system or service's value to the government. Such an evaluation should take into account comparative system effectiveness and efficiency for purpose, the risk associated with use, and any ongoing operation and maintenance costs. Where practicable, agencies should consider terms to solicit and incorporate feedback from end users, program managers, and other relevant stakeholders to inform modifications that continuously improve performance of the AI system or service in the context of the agency's m1ss10n. \niv. Sunset Criteria. Where practicable, agencies should determine criteria for sunsetting the use of an AI system. Changes in costs, agency needs, vendor-proposed requirements, or model performance may signal that an agency should reconsider continued use.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring"], "source": "https://www.whitehouse.gov/wp-content/uploads/2025/02/M-25-22-Driving-Efficient-Acquisition-of-Artificial-Intelligence-in-Government.pdf", "official_name": "Memorandum 25-22 (Driving Efficient Acquisition of Artificial Intelligence in Government)", "label": "safe"}
{"id": "247_1", "doc_id": "247", "text": "House Bill 203 (AS PASSED HOUSE AND SENATE)\nBy: Representatives Newton of the 127th, Beverly of the 143rd, Hawkins of the 27th, Stephens of the 164th, and Cooper of the 45th \nA BILL TO BE ENTITLED\nAN ACT\nTo amend Chapter 12 of Title 31 of the Official Code of Georgia Annotated, relating to the control of hazardous conditions, preventable diseases, and metabolic disorders, so as to revise provisions relating to restrictions on the sale and dispensing of contact lenses with respect to physicians; to revise definitions; to provide requirements for assessment mechanisms; to provide for rules and regulations; to provide for statutory construction; to provide for violations and consequences; to provide for related matters; to repeal conflicting laws; and for other purposes.\n\nBE IT ENACTED BY THE GENERAL ASSEMBLY OF GEORGIA:", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Fines", "Applications: Medicine, life sciences and public health"], "source": "https://www.legis.ga.gov/api/legislation/document/20232024/213440", "official_name": "An act to amend Chapter 12 of Title 31 of the Official Code of Georgia Annotated, relating to the control of hazardous conditions, preventable diseases, and metabolic disorders [...]", "label": "safe"}
{"id": "247_2", "doc_id": "247", "text": "SECTION 1.\n\tChapter 12 of Title 31 of the Official Code of Georgia Annotated, relating to the control of hazardous conditions, preventable diseases, and metabolic disorders, is amended by revising Code Section 31-12-12, relating to restrictions on the sale or dispensing of contact lenses, as\n\tfollows:\n\t\"31-12-12.\n\t(a)  As used in this Code section, the term:\n\t(1)  'Assessment mechanism' means automated or virtual equipment, application, or technology designed to be used on a telephone, a computer, or an internet accessible device that may be used either in person or via telemedicine to conduct an eye assessment, and includes artificial intelligence devices and any equipment, electronic or nonelectronic, that are used to conduct an eye assessment.\n(2)  'Contact lens' means any lens placed directly on the surface of the eye, regardless of whether or not it is intended to correct a visual defectincluding any cosmetic, therapeutic, or corrective lens.\n(3)  'Dispense' means the act of furnishing spectacles or contact lenses to an individual.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Fines", "Applications: Medicine, life sciences and public health"], "source": "https://www.legis.ga.gov/api/legislation/document/20232024/213440", "official_name": "An act to amend Chapter 12 of Title 31 of the Official Code of Georgia Annotated, relating to the control of hazardous conditions, preventable diseases, and metabolic disorders [...]", "label": "safe"}
{"id": "247_3", "doc_id": "247", "text": "(4)  'Eye assessment' means an assessment of the ocular health and visual status of a patient that may include, but is not limited to, objective refractive data or information generated by an automated testing device, including an autorefractor, in order to establish a refraction diagnosis for the correction of vision disorders.  This may include synchronous or asynchronous telemedicine technologies.\n(5)  'Eye examination' means a real-time examination, which includes the use of telemedicine, in accordance with the applicable standard of care of the prescriber, of the ocular health and visual status of an individual that does not consist solely of objective refractive data or information generated by an automated testing device, including an autorefractor or kiosk, in order to establish a medical diagnosis or refractive diagnosis for the establishment of refractive error, conducted with the patient and prescriber in the same physical location or via telemedicine.  If the eye examination is conducted via telemedicine, the patient and prescriber shall be required to be in synchronous verbal and visual contact during such parts of the examination necessary to ensure that the examination is, at a minimum, equivalent to an eye examination conducted in person.\n(6)  'Kiosk' means automatic equipment or application designed to be used on a telephone, a computer, or an internet based device that can be used either in person or via telemedicine to conduct an eye examination.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Fines", "Applications: Medicine, life sciences and public health"], "source": "https://www.legis.ga.gov/api/legislation/document/20232024/213440", "official_name": "An act to amend Chapter 12 of Title 31 of the Official Code of Georgia Annotated, relating to the control of hazardous conditions, preventable diseases, and metabolic disorders [...]", "label": "safe"}
{"id": "247_4", "doc_id": "247", "text": "(7)  'Over-the-counter spectacles' means eyeglasses or lenses in a frame for the correction of vision that may be sold by any person, firm, or corporation at retail without a prescription; these spectacles must avoid exceed +3.25 diopters.\n\t(8)  'Prescriber' means an optometrist or ophthalmologist licensed in this state.\n\t(9)  'Prescription' means a handwritten or electronic order issued by a prescriber.\n\t(10)  'Spectacles' means an optical instrument or device worn or used by an individual that has one or more lenses designed to correct or enhance vision addressing the visual needs of the individual wearer, commonly known as glasses or eyeglasses, including spectacles that may be adjusted by the wearer to achieve different types of visual correction or enhancement.  Spectacles does not include an optical instrument or device that is not intended to correct or enhance vision or that is sold without consideration of the visual status of the individual who will use the optical instrument or device. Spectacles does not include over-the-counter spectacles.\n(11)  'Telehealth' has the same meaning as in paragraph (6) of  subsection (b) of Code 65 Section 33-24-56.4.\n(12)  'Telemedicine' has the same meaning as in paragraph (7) of  subsection (b) of Code 67 Section 33-24-56.4.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Fines", "Applications: Medicine, life sciences and public health"], "source": "https://www.legis.ga.gov/api/legislation/document/20232024/213440", "official_name": "An act to amend Chapter 12 of Title 31 of the Official Code of Georgia Annotated, relating to the control of hazardous conditions, preventable diseases, and metabolic disorders [...]", "label": "safe"}
{"id": "247_5", "doc_id": "247", "text": "(b)(1)\n(A)  No person in this state shall sell, dispense, or serve as a conduit for the sale or dispensing of contact lenses or spectacles to the ultimate user of such contact lenses or spectacles except persons licensed and regulated by Chapter 29, 30, or 34 of Title 43.\n(B)  No person in this state shall write a prescription for contact lenses or spectacles  unless he or she is a prescriber.\n(C)  No person in this state shall write a prescription for contact lenses or spectacles unless an eye examination is conducted or, pursuant to the conditions in subsection (d) 76 of this Code section, an eye assessment is performed.  \n(D)  No person in this state shall write an initial prescription for contact lenses until he or she has completed all measurements, tests, and examinations necessary to satisfy his or her professional judgment that the patient is a viable candidate to wear contact lenses, recognizing that more than one visit between the patient and the prescriber may be required and contact lenses suitable for the patient's eyes have been evaluated and fitted by the prescriber, and the prescriber is satisfied with the fitting based on ocular health and the visual needs of the patient.  The patient shall be entitled to receive a copy of the contact lens prescription upon completion of fit.\n(2)  Any person who violates a subparagraph of paragraph (1) of this subsection one or two times shall upon conviction be guilty of a misdemeanor and punished by imprisonment for up to one year or by a charge not to exceed $1,000.00 or by both such charge and imprisonment.  Any person who violates a subparagraph of paragraph (1) of this subsection three or more times shall upon conviction be guilty of a felony and punished by imprisonment for one to five years or by a charge not to exceed $10,000.00 or by both 93\tsuch charge and imprisonment.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Fines", "Applications: Medicine, life sciences and public health"], "source": "https://www.legis.ga.gov/api/legislation/document/20232024/213440", "official_name": "An act to amend Chapter 12 of Title 31 of the Official Code of Georgia Annotated, relating to the control of hazardous conditions, preventable diseases, and metabolic disorders [...]", "label": "safe"}
{"id": "247_6", "doc_id": "247", "text": "(c)  An assessment mechanism to conduct an eye assessment or to generate a prescription for contact lenses or spectacles in this state shall:\n\t(1)  Be conducted in accordance with the provisions of Code Section 33-24-56.4, the 'Georgia Telehealth Act';\n\t(2)  Collect the patient's medical history, previous prescription information for corrective\n\teyewear, and length of time since the patient's most recent in-person eye health\n\texamination;\n\t(3)  Provide any applicable accommodation required by the federal Americans with Disabilities Act, 42 U.S.C. Section 12101, et seq., as amended;\n(4)  Gather and transmit protected health information in adherence with the federal Health Insurance Portability and Accountability Act of 1996, as amended; and (5) Perform a procedure with a recognized current procedural terminology code maintained by the American Medical Association, if applicable.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Fines", "Applications: Medicine, life sciences and public health"], "source": "https://www.legis.ga.gov/api/legislation/document/20232024/213440", "official_name": "An act to amend Chapter 12 of Title 31 of the Official Code of Georgia Annotated, relating to the control of hazardous conditions, preventable diseases, and metabolic disorders [...]", "label": "safe"}
{"id": "247_7", "doc_id": "247", "text": "(d)  To use an assessment mechanism to conduct an eye assessment or to generate a prescription for contact lenses or spectacles, a prescriber shall be licensed in good standing in this state, acting within his or her scope of practice, and shall:\n(1)  Conform to the standard of eye healthcare expected of traditional in-person clinical settings as appropriate to the patient's age and presenting condition, including when the standard of care requires the use of diagnostic testing and performance of a physical examination, which may be carried out through the use of peripheral devices appropriate to the patient's condition;\n(2)  Not use the data or information obtained from an eye assessment as the sole basis for issuing a prescription;\n(3)  Create and maintain for at least seven years a medical record for each patient, for use during the ongoing treatment of a patient and in adherence with all state and federal\n\tlaws regarding maintenance and accessibility;\n(4)  Read and interpret the diagnostic information and data, including any photographs 121\tand scans, gathered by the assessment mechanism;\n\t(5)  Verify the identity of the patient requesting treatment via the assessment mechanism;\n\t(6)  Verify that the patient is between 21 and 50 years of age;\n\t(7)  Confirm that the patient is not experiencing any of the following:\n\t(A)  Corneal disease, such as keratoconus or herpes;\n\t(B)  Vessels in the cornea;\n\t(C)  Glaucoma;\n\t(D)  Macular degeneration;\n\t(E)  Hereditary eye disease;\n\t(F)  Freckle, birthmark, or mole inside the eye;\n\t(G)  Diabetes;\n\t(H)  Shingle on the forehead or face;\n\t(I)  Pain, redness, or itchiness of the eye;\n\t(J)  High sensitivity to brightness;\n\t(K)  Pain or discomfort or blurred or double vision while wearing corrective visual aids;\n\t(L)  Bright flashes or floaters;\n\t(M)  Temporary loss of vision; or\n\t(N)  Moderate to severe dry eye;\n\t(8)  Confirm that the patient is not currently taking medications with potentially serious ocular side effects;\n\t(9)  For issuing prescriptions for spectacles, verify that the patient has received an eye examination by a prescriber within the previous 24 months;\n\t(10)  For issuing prescriptions for contact lenses, verify that the patient has received an 144\teye examination by a prescriber:\n\t(A)  For the initial prescription and first renewal of the initial prescription; or\n\t(B)  After the first renewal of the initial prescription, within 24 months;\n\t(11)  Provide a handwritten or electronic signature, along with the prescriber's state licensure number, certifying his or her diagnosis, evaluation, treatment, prescription, and consultation recommendations for the patient;\n(12)  Maintain responsibility insurance, through its owner or lessee, in an amount adequate to cover claims made by individuals diagnosed or treated based on information and data,\n\tincluding any photographs and scans, generated by an assessment mechanism; and\n(13)  Disclose to patients and require acceptance in advance as a term of use that:\n\t(A)  The eye assessment is not a replacement of an eye examination;\n\t(B)  The eye assessment cannot be used to generate an initial prescription for contact 156\tlenses or spectacles or first renewal of the initial prescription; and\n\t(C)  The eye assessment may only be used if the patient has had an eye examination within the previous 24 months.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Fines", "Applications: Medicine, life sciences and public health"], "source": "https://www.legis.ga.gov/api/legislation/document/20232024/213440", "official_name": "An act to amend Chapter 12 of Title 31 of the Official Code of Georgia Annotated, relating to the control of hazardous conditions, preventable diseases, and metabolic disorders [...]", "label": "safe"}
{"id": "247_8", "doc_id": "247", "text": "(e)  All contact lenses used in the determination of a contact lens prescription are considered to be diagnostic lenses.  After the diagnostic period and the contact lenses have been adequately fitted and the patient released from immediate follow-up care by persons licensed and regulated by Chapter 29, 30, or 34 of Title 43, the  prescriber shall, upon the request of the patient, at no cost, provide a prescription in writing for replacement contact lenses.  A person must avoid dispense or adapt contact lenses or spectacles without first receiving authorization to do so by a written prescription, except when authorized orally to do so by a person licensed and regulated by Chapter 30 or 34 of Title 43.\n\n(f)  Patients who comply with such fitting and follow-up requirements as may be established by the  prescriber may obtain replacement contact lenses until the expiration date listed on the prescription from a person who may lawfully dispense contact lenses under subsection (b) of this Code section.\n\n(g)  A prescriber may refuse to give the patient a copy of the patient's prescription until the patient has paid for all services rendered in connection with the prescription.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Fines", "Applications: Medicine, life sciences and public health"], "source": "https://www.legis.ga.gov/api/legislation/document/20232024/213440", "official_name": "An act to amend Chapter 12 of Title 31 of the Official Code of Georgia Annotated, relating to the control of hazardous conditions, preventable diseases, and metabolic disorders [...]", "label": "safe"}
{"id": "247_9", "doc_id": "247", "text": "(h)  No replacement contact lenses may be sold or dispensed except pursuant to a prescription which:\n(1)  Conforms to state and federal regulations governing such forms and includes the name, address, and state licensure number of  the prescriber;\n(2)  Explicitly states an expiration date of not more than 12 months from the date of the last prescribing contact lens examination, unless a medical or refractive problem affecting\n\tvision requires an earlier expiration date;\n\t(3)  Explicitly states the number of refills;\n\t(4)  Explicitly states that it is for contact lenses and indicates the lens brand name and type, including all specifications necessary for the ordering or fabrication of lenses; and (5)  Is kept on file by the person selling or dispensing the replacement contact lenses for at least 24 months after the prescription is filled.\n\n(i)  Anyone who fills a prescription bears the full responsibility of the accuracy of the contact lenses or spectacles provided under the prescription.  At no time, without the direction of a prescriber, shall any changes or substitutions be made in the brand or type of lenses the prescription calls for with the exceptions of tint change if requested by the patient.  However, if a prescription specifies 'only' a specific color or tinted lens, those instructions shall be observed.\n\n(j)  All sales of and prescriptions for contact lenses in this state shall conform to the federal Fairness to Contact Lens Consumers Act, P.L. 108-164, 15  U.S.C. Section 7601, et seq.  The provisions of this Code section shall be construed in aid of and in conformity with said federal act.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Fines", "Applications: Medicine, life sciences and public health"], "source": "https://www.legis.ga.gov/api/legislation/document/20232024/213440", "official_name": "An act to amend Chapter 12 of Title 31 of the Official Code of Georgia Annotated, relating to the control of hazardous conditions, preventable diseases, and metabolic disorders [...]", "label": "safe"}
{"id": "247_10", "doc_id": "247", "text": "(k)  Civil proceedings to enforce the provisions of this Code section may be brought by any board created under Chapter 29, 30, or 34 of Title 43 or by any other interested person through injunction or other appropriate remedy.\n\n(l)  The Georgia State Board of Optometry and the Georgia Composite Medical Board may each promulgate reasonable rules and regulations applicable to their respective licensees to carry out the provisions of this Code section.\n\n(m)  Evaluation, treatment, and consultation recommendations by a prescriber utilizing an assessment mechanism pursuant to the requirements in this Code section, including a prescription via electronic means, shall be held to the same standards of care as those in traditional in-person clinical settings.\n\n(n)  This Code section must avoid be construed to:\n\t(1)  Limit the discretion of a prescriber to direct a patient to utilize telehealth as deemed\n\tappropriate;\n\t(2)  Limit the sharing of patient information, in whatever form, between an optometrist,\n\tosteopath, or physician; or\n\t(3)  Apply beyond ocular health and eye care.\n\n(o)  Any person who dispenses, offers to dispense, or attempts to dispense contact lenses or spectacles in infraction of this Code section or any applicable rules and regulations concerning the dispensing of contact lenses or spectacles in this state shall, in addition to any other consequence provided by law, pay a civil consequence to the Office of the Attorney General in an amount not to exceed $11,000.00 for each infraction.\"\n\nSECTION 2.\n\tAll laws and parts of laws in conflict with this Act are repealed.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Incentives: Fines", "Applications: Medicine, life sciences and public health"], "source": "https://www.legis.ga.gov/api/legislation/document/20232024/213440", "official_name": "An act to amend Chapter 12 of Title 31 of the Official Code of Georgia Annotated, relating to the control of hazardous conditions, preventable diseases, and metabolic disorders [...]", "label": "safe"}
{"id": "2548_1", "doc_id": "2548", "text": "(a) Definitions.--In this section:\n            (1) American science cloud.--The term ``American science \n        cloud'' means a system of United States government, academic, \n        and private sector programs and infrastructures utilizing cloud \n        computing technologies to facilitate and support scientific \n        research, data sharing, and computational analysis across \n        various disciplines while ensuring adherence with applicable \n        legal, regulatory, and privacy standards.\n            (2) Artificial intelligence.--The term ``artificial \n        intelligence'' has the meaning given the term in section 5002 of \n        the National Artificial Intelligence Initiative Act of 2020 (15 \n        U.S.C. 9401).", "tags": ["Strategies: Convening", "Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://www.congress.gov/bill/119th-congress/house-bill/1/text", "official_name": "One Big Beautiful Bill Act 2025, Section 50404 (\"Transformational artificial intelligence models\")", "label": "safe"}
{"id": "2548_2", "doc_id": "2548", "text": "(b) Transformational Models.--The Secretary of Energy shall--\n            (1) mobilize National Laboratories to partner with industry \n        sectors within the United States to curate the scientific data \n        of the Department of Energy across the National Laboratory \n        complex so that the data is structured, cleaned, and \n        preprocessed in a way that makes it suitable for use in \n        artificial intelligence and machine learning models; and\n\n            (2) initiate seed efforts for self-improving artificial \n        intelligence models for science and engineering powered by the \n        data described in paragraph (1).", "tags": ["Strategies: Convening", "Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://www.congress.gov/bill/119th-congress/house-bill/1/text", "official_name": "One Big Beautiful Bill Act 2025, Section 50404 (\"Transformational artificial intelligence models\")", "label": "safe"}
{"id": "2548_3", "doc_id": "2548", "text": "(c) Uses.--\n            (1) Microelectronics.--The curated data described in \n        subsection (b)(1) may be used to rapidly develop next-generation \n        microelectronics that have greater capabilities beyond Moore's \n        law while requiring lower energy consumption.\n            (2) New energy technologies.--The artificial intelligence \n        models developed under subsection (b)(2) shall be provided to \n        the scientific community through the American science cloud to \n        accelerate advancement in discovery science and engineering for \n        new energy technologies.", "tags": ["Strategies: Convening", "Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://www.congress.gov/bill/119th-congress/house-bill/1/text", "official_name": "One Big Beautiful Bill Act 2025, Section 50404 (\"Transformational artificial intelligence models\")", "label": "safe"}
{"id": "266_9", "doc_id": "266", "text": "Article 12: Algorithmic recommendation service providers are encouraged to comprehensively use tactics such as content de-weighting, scattering interventions, etc., and optimize the transparency and understandability of search, ranking, selection, push notification, display, and other such norms, to avoid creating harmful influence on users, and prevent or reduce controversies or disputes. \n\nArticle 13: Where algorithmic recommendation service providers provide Internet news information services, they shall obtain an Internet news information service permit according to the law; and standardize their deployment of Internet news information collection, editing and dissemination services, resharing services, and broadcast platform services. They may not generate or synthesize fake news information, and may not disseminate news information not published by work units in the State-determined scope.", "tags": ["Risk factors: Transparency", "Risk factors: Interpretability and explainability", "Harms: Detrimental content", "Strategies: Licensing, registration, and certification", "Applications: Broadcasting and media production", "Applications: Networking and telecommunications", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application"], "source": "https://digichina.stanford.edu/work/translation-internet-information-service-algorithmic-recommendation-management-provisions-effective-march-1-2022/", "official_name": "Internet Information Service Algorithmic Recommendation Management Provisions", "label": "safe"}
{"id": "266_19", "doc_id": "266", "text": "Article 25: The national and provincial, autonomous region, and municipal cybersecurity and informatization departments shall, after receiving filing materials submitted by a filing applicant, and where the materials are complete, financial support allocation filing within 30 working days, and issue a filing number and publish the matter; where materials are not complete, filing is not to be granted, and the filing applicant shall be notified within 30 working days, and the reason explained. \n\nArticle 26: Algorithmic recommendation service providers who have completed filing shall indicate their filing number in a clear position on their website, application program, etc., used for providing external services, and provide a link to the published information. \n\nArticle 27: The providers of algorithmic recommendation services with public opinion properties or social mobilization capabilities shall conduct a security assessment according to relevant State regulations.", "tags": ["Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Licensing, registration, and certification", "Applications: Security", "Strategies: Tiering", "Strategies: Tiering: Tiering based on impact", "Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring"], "source": "https://digichina.stanford.edu/work/translation-internet-information-service-algorithmic-recommendation-management-provisions-effective-march-1-2022/", "official_name": "Internet Information Service Algorithmic Recommendation Management Provisions", "label": "safe"}
{"id": "266_22", "doc_id": "266", "text": "Article 31: Where algorithmic recommendation service providers violate the provisions of Article 7, Article 8, Article 9 Paragraph I, Article 10, Article 14, Article 16, Article 17, Article 16, Article 22, Article 24, or Article 26 of these Provisions, and laws or administrative regulations contain provisions, those provisions are followed; where laws or administrative regulations do not contain provisions, cybersecurity and informatization departments or telecommunications, public security or market regulation, or other such relevant departments will, on the basis of their duties and responsibilities, issue a warning or a report of criticism, and order rectification within a limited time; where rectification is refused or circumstances are grave, they are to order provisional suspension of information updates, and impose a charge between 10,000 and 100,000 yuan. Where an act violating public order management is constituted, public order management punishment is to be imposed according to the law; where a crime is constituted, criminal responsibility is to be prosecuted according to the law.", "tags": ["Incentives: Criminal liability", "Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-internet-information-service-algorithmic-recommendation-management-provisions-effective-march-1-2022/", "official_name": "Internet Information Service Algorithmic Recommendation Management Provisions", "label": "safe"}
{"id": "266_24", "doc_id": "266", "text": "Article 33: Where providers of algorithmic recommendation services with public opinion properties or social mobilization capabilities obtain filing through hiding relevant circumstances when reporting for filing, providing false materials, or other such improper means, the national and provincial, autonomous region, or municipal cybersecurity and informatization departments cancel filing according to the law, and issue a warning or a report of criticism; where circumstances are grave, they are to order provisional suspension of information updates, and impose a charge between 10,000 and 100,000 yuan. \n\nWhere providers of algorithmic recommendation services with public opinion properties or social mobilization capabilities cease services without carrying out filing cancellation formalities according to the requirements of Article 24 Paragraph III of these Provisions, or they receive administrative punishments such as website closure orders, cancellation of relevant business permits, revocation of the business license, etc., because grave unlawful situations occurred, the national, provincial, autonomous region, and municipal cybersecurity and informatization departments are to impose filing cancellation.\n\nChapter VI: Supplementary provisions", "tags": ["Incentives: Fines"], "source": "https://digichina.stanford.edu/work/translation-internet-information-service-algorithmic-recommendation-management-provisions-effective-march-1-2022/", "official_name": "Internet Information Service Algorithmic Recommendation Management Provisions", "label": "safe"}
{"id": "2665_13", "doc_id": "2665", "text": "Recommended Policy Actions\n\n• Direct the National Science and Technology Council (NSTC) Machine Learning and AI Subcommittee to make recommendations on minimum data quality standards for the use of biological, materials science, chemical, physical, and other scientific data modalities in AI model training.\n• Promulgate the OMB regulations required in the Confidential Information Protection and Statistical Efficiency Act of 2018 on presumption of accessibility and expanding secure access, which will lower barriers and break down silos to accessing Federal data, ultimately facilitating the improved use of AI for evidence building by statistical agencies while protecting confidential data from inappropriate access and use.16\n• Establish secure compute environments within NSF and DOE to enable secure AI use- cases for controlled access to restricted Federal data.\n• Create an online portal for NSF’s National Secure Data Service (NSDS) demonstration project to provide the public and Federal agencies with a front door to AI use-cases involving controlled access to restricted Federal data.\n• Explore the creation of a whole-genome sequencing program for life on Federal lands, led by the NSTC and including members of the U.S. Department of Agriculture, DOE, NIH, NSF, the Department of Interior, and Cooperative Ecosystem Studies Units to collaborate on the development of an initiative to establish a whole genome sequencing program for life on Federal lands (to include all biological domains). This new data would be a valuable resource in training future biological foundation models.", "tags": ["Risk factors: Security", "Strategies: Input controls", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Harms: Violation of civil or human rights, including privacy"], "source": "https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf", "official_name": "Winning the Race: America's AI Action Plan 2025", "label": "safe"}
{"id": "2665_26", "doc_id": "2665", "text": "Build High-Security Data Centers for Military and Intelligence Community Usage\n\nBecause AI systems are particularly well-suited to processing raw intelligence data today, and because of the vastly expanded capabilities AI systems could have in the future, it is likely that AI will be used with some of the U.S. government’s most sensitive data. The data centers where these models are deployed must be resistant to attacks by the most determined and capable nation-state actors.\n\nRecommended Policy Actions\n\n• Create new technical standards for high-security AI data centers, led by DOD, the IC, NSC, and NIST at DOC, including CAISI, in collaboration with industry and, as appropriate, relevant Federally Funded exploratory work Centers.\n• Advance agency adoption of classified compute environments to support scalable and secure AI workloads.", "tags": ["Strategies: Performance requirements", "Risk factors: Security", "Strategies: Convening"], "source": "https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf", "official_name": "Winning the Race: America's AI Action Plan 2025", "label": "safe"}
{"id": "2668_3", "doc_id": "2668", "text": "22757.20. This chapter shall be known as the Leading Ethical AI Development (LEAD) for Kids Act.\n\n22757.21. For purposes of this chapter:\n(a) “Artificial intelligence” means an engineered or machine-based system that varies in its level of autonomy and that can, for explicit or implicit objectives, infer from the input it receives how to generate outputs that can influence physical or virtual environments.\n(b) “Child” means a natural person under 18 years of age who resides in this state.\n(c) (1) “Companion chatbot” means a generative artificial intelligence system with a natural language interface that simulates a sustained humanlike relationship with a user by doing all of the following:\n(A) Retaining information on prior interactions or user sessions and user preferences to personalize the interaction and facilitate ongoing engagement with the companion chatbot.\n(B) Asking unprompted or unsolicited emotion-based questions that go beyond a direct response to a user prompt.\n(C) Sustaining an ongoing dialogue concerning matters personal to the user.\n(2) “Companion chatbot” does not include the following:\n(A) Any system used by a business entity solely for customer service or to strictly provide users with information about available commercial services or products provided by that entity, customer service account information, or other information strictly related to its customer service.\n(B) Any system that is solely designed and marketed for providing efficiency improvements or research or technical assistance.\n(C) Any system used by a business entity solely for internal purposes or employee productivity.\n(d) “Generative artificial intelligence” means artificial intelligence that can generate derived synthetic content, including text, images, video, and audio, that emulates the structure and characteristics of the artificial intelligence’s training data.\n(e) “Operator” means a person, partnership, corporation, business entity, or state or local government agency that makes a companion chatbot available to users.\n(f) “Personal information” has the meaning defined in Section 1798.140 of the Civil Code.", "tags": ["Incentives: Civil liability", "Incentives: Fines", "Risk factors: Safety", "Harms: Harm to health/safety", "Harms: Detrimental content", "Risk factors: Safety", "Harms: Harm to health/safety", "Harms: Detrimental content", "Harms: Harm to health/safety", "Risk factors: Safety", "Harms: Detrimental content"], "source": "https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202520260AB1064", "official_name": "CA AB 1064", "label": "safe"}
{"id": "2668_5", "doc_id": "2668", "text": "22757.23. (a) The Attorney General may bring an action against an operator for a infraction Section 22757.22 to obtain any of the following remedies:\n(1) A civil consequence of twenty-five thousand dollars ($25,000) for each infraction.\n(2) Injunctive or declaratory relief.\n(3) Reasonable attorney’s fees.\n(b) A child who suffers actual harm as a result of a infraction of Section 22757.22, or a parent or guardian acting on behalf of that child, may bring a civil action against the operator to recover all of the following:\n(1) Actual damages.\n(2) Punitive damages.\n(3) Reasonable attorney’s fees and costs.\n(4) Injunctive or declaratory relief.\n(5) Any other relief the court deems proper.\n\n22757.24. The provisions of this chapter are severable. If any provision of this chapter or its application is held invalid, that invalidity must avoid affect other provisions or applications that can be given effect without the invalid provision or application.", "tags": ["Incentives: Civil liability", "Incentives: Fines"], "source": "https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202520260AB1064", "official_name": "CA AB 1064", "label": "safe"}
{"id": "2673_10", "doc_id": "2673", "text": "Impact of Tools on Stakeholders\nHaving examined each of the instruments in more detail, we propose to address the question:\n“What positive impact will they have on the interests and capacities of all three key stakeholders?”\n\nCitizens\n\nAt first glance, it may seem that the tools we have described are aimed solely at helping businesses\nprepare for the introduction of future national regulation and access to global markets, including\nthe EU market. And while this is certainly the reasoning behind the creation of the tools, we also\nexpect a positive impact on the level of human rights protection against AI risks. First of all, this\napplies to soft law tools: general and sectoral guidelines and voluntary codes of conduct – we\nprovide clear guidance on what needs to be considered when developing and using AI products. In\nthe case of voluntary guidelines, this impact is reinforced by a form of reputational obligation,\nincluding through our proposed monitoring of companies adherence with their commitments.\nThe positive impact on human rights of training tools is less obvious. But if we take a closer look, it becomes clear that this positive impact of training tools can be no less, and perhaps even greater. Such an impact will be indirect: the more responsible and implementing elements of the future legislation of AI products we have on the market, the higher the level of human rights protection within the country. We are convinced that such an indirect impact will be much more effective than, for example, the early introduction of legally binding regulation without providing time and tools for business to prepare. In such a scenario, imposing sanctions for violations would not be an effective defence mechanism – the state, not being able to track all offending companies, would simply not be physically able to charge or block all products that would violate the requirements of such prematurely introduced legislation.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements"], "source": "https://storage.thedigital.gov.ua/files/c/fc/36c4cae89deedfbf3781ec6bceddffcc.pdf", "official_name": "White Paper on Artificial Intelligence Regulation in Ukraine: Vision of the Ministry of Digital Transformation of Ukraine 2024", "label": "safe"}
{"id": "2674_5", "doc_id": "2674", "text": "VIII\t– the security, and the protection of privacy and personal data;\nIX\t– information security; X – information access;\nXI\t– national defense, state security, and national sovereignty;\nXII\t– the freedom of business models, as long as it\ndoes not conflict with the provisions outlined in this Law;\nXIII\t– the preservation of the stability, security, resilience, and functionality of artificial intelligence systems,\tthrough\ttechnical\tmeasures\tcompatible\twith international standards and the encouragement of the use of good practices;\nXIV\t– the protection of free competition and against abusive market practices, under Law No. 12,529, of November 30, 2011; and\nXV\t– alignment with Laws No. 13.709, of August 14, 2018 (General Personal Data Protection Law), 12.965, of April\n23, 2014, 12.529, of November 30, 2011, 8078, of September 11, 1990 (Consumer Protection Code), and 12.527 of November 18, 2011.\nSole Paragraph. The codes of conduct and good practice guides mentioned in item VII of the head provision of this article may serve as indicative elements of adherence.", "tags": ["Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Security", "Risk factors: Reliability", "Risk factors: Reliability: Robustness"], "source": "https://www.derechosdigitales.org/wp-content/uploads/Brazil-Bill-Law-of-No-21-of-2020-EN.pdf#%5B%7B%22num%22%3A17%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C102%2C794%2C0%5D", "official_name": "Federal Senate Bill of Law No. 21, of 2020", "label": "safe"}
{"id": "2674_6", "doc_id": "2674", "text": "Art. 5 The principles for artificial intelligence development and application in Brazil are:\nI\t– beneficial purpose: search for beneficial results for humanity through artificial intelligencesystems;\nII\t–human centrality: respect for human dignity, privacy, personal data, and fundamental rights protection, when the system deals with issues related to human beings;\nIII\t–\tnon-discrimination:\tmitigation\tof\tthe possibility of using the systems for discriminatory, illegal, or abusive purposes;\nIV\t– search for neutrality: recommendations for agents acting in the development and operation chain of artificial intelligence systems to identify and mitigate biases contrary to the provisions of current legislation;\nV\t– transparency: the people's right to be informed in a clear, accessible, and accurate way about the use of artificial intelligence solutions, unless otherwise provided by law and observing commercial and industrial secrets, in the following cases:\na)\ton the fact that they are communicating directly with artificial intelligence systems, such as through conversation robots for personalized online service (chatbot), when using these systems;\nb)\ton the identity of the natural person, when one operates the system autonomously and individually, or of the legal entity responsible for the operation of artificial intelligence systems;\nc)\ton general criteria that guide the functioning of the artificial intelligence system, ensuring thatcommercial\nand industrial secrets are safeguarded, when there is a potential for a relevant risk to fundamental rights;", "tags": ["Harms: Violation of civil or human rights, including privacy", "Risk factors: Privacy", "Harms: Discrimination", "Risk factors: Bias", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Performance requirements"], "source": "https://www.derechosdigitales.org/wp-content/uploads/Brazil-Bill-Law-of-No-21-of-2020-EN.pdf#%5B%7B%22num%22%3A17%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C102%2C794%2C0%5D", "official_name": "Federal Senate Bill of Law No. 21, of 2020", "label": "safe"}
{"id": "2674_7", "doc_id": "2674", "text": "VI\t– security and prevention: use of technical, organizational, and administrative measures, considering the usage of reasonable and available means at the time, compatible with best practices, international standards, and economic feasibility, aimed at allowing the management and mitigation of risks arising from the operation of artificial intelligence systems throughout their life cycle and their continuous operation;\nVII\t– responsible advancement: guarantee of adoption of the provisions of this Law by the agents that work in the development and operation chain of artificial intelligence systems that are in usage, documenting their internal management process and taking responsibility for the results of the functioning of these systems, within the limits of their respective participation, context, and available technologies;\nVIII\t– data availability: non-infringement of copyright by the usage of data, databases, and texts protected by it to train artificial intelligence systems, provided that there is no impact on the normal exploitation of the work by its owner.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation: Impact assessment"], "source": "https://www.derechosdigitales.org/wp-content/uploads/Brazil-Bill-Law-of-No-21-of-2020-EN.pdf#%5B%7B%22num%22%3A17%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C102%2C794%2C0%5D", "official_name": "Federal Senate Bill of Law No. 21, of 2020", "label": "safe"}
{"id": "269_1", "doc_id": "269", "text": "The people of the State of California do enact as follows:\n\nSECTION 1. Section 11546.45.5 is added to the Government Code, to read:\n\n11546.45.5. (a) For purposes of this section:\n(1) “Automated decision system” means a computational process derived from machine learning, statistical modeling, data analytics, or artificial intelligence that issues simplified output, including a score, classification, or recommendation, that is used to assist or replace human discretionary decisionmaking and materially impacts natural persons. “Automated decision system” does not include a spam email filter, firewall, antivirus software, identity and access management tools, calculator, database, dataset, or other compilation of data.\n(2) “Board” means any administrative or regulatory board, commission, committee, council, association, or authority consisting of more than one person whose members are appointed by the Governor, the Legislature, or both.\n(3) “Department” means the Department of Technology.\n(4) “High-risk automated decision system” means an automated decision system that is used to assist or replace human discretionary decisions that have a legal or similarly significant effect, including decisions that materially impact access to, or approval for, housing or accommodations, education, employment, benefit, health care, and criminal justice.\n(5) (A) “State agency” means any of the following:\n(i) Any state office, department, division, or bureau.\n(ii) The California State University.\n(iii) The Board of Parole Hearings.\n(iv) Any board or other professional licensing and regulatory body under the administration or oversight of the Department of Consumer Affairs.\n(B) “State agency” does not include the University of California, the Legislature, the judicial branch, or any board, except as provided in subparagraph (A).", "tags": ["Harms: Financial loss", "Harms: Violation of civil or human rights, including privacy", "Applications: Government: judicial and law enforcement", "Applications: Education", "Applications: Finance and investment", "Applications: Medicine, life sciences and public health"], "source": null, "official_name": null, "label": "safe"}
{"id": "276_1", "doc_id": "276", "text": "Be it enacted by the Legislature of the State of Arizona:\n\nSection 1. Section 16-442, Arizona Revised Statutes, is amended to read:\n\n16-442. Committee approval; adoption of vote tabulating equipment; experimental use; emergency certification\n\nA. The secretary of state shall appoint a committee of three persons, to consist of a member of the engineering college at one of the universities, a member of the state bar of Arizona and one person familiar with voting processes in the state,  not more than two of whom shall be of the same political party, and at least one of whom shall have at least five years of experience with and shall be able to render an opinion based on knowledge of, training in or education in electronic voting systems, procedures and security.  The committee shall investigate and test the various types of vote recording or tabulating machines or devices that may be used under this article. The committee shall submit its recommendations to the secretary of state who shall make final adoption of the type or types, make or makes, model or models to be certified for use in this state. The committee shall serve without compensation.\n\nB. Machines or devices used at any election for federal, state or county offices may only be certified for use in this state and may only be used in this state if they comply with the help America vote act of 2002 and if those machines or devices have been tested and approved by a laboratory that is accredited pursuant to the help America vote act of 2002.  Machines, devices, firmware or software used in this state may not include any artificial intelligence or learning hardware, firmware or software.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Applications: Government: other applications/unspecified"], "source": "https://www.azleg.gov/legtext/56leg/1R/bills/SB1565S.htm", "official_name": "State of Arizona Senate Bill 1565, An Act Amending Sections 16-442, 16-552 and 16-621, Arizona Revised Statutes; Relating to Conduct of Elections.", "label": "safe"}
{"id": "276_2", "doc_id": "276", "text": "C. After consultation with the committee prescribed by subsection A of this section, the secretary of state shall adopt standards that specify the criteria for loss of certification for equipment that was used at any election for federal, state or county offices and that was previously certified for use in this state. On loss of certification, machines or devices used at any election may not be used for any election for federal, state or county offices in this state unless recertified for use in this state.\n\nD. The secretary of state may revoke the certification of any voting system or device for use in a federal, state or county election in this state or may restrict for up to five years the purchase, lease or use of any voting system or device leased, installed or used by a person or firm in connection with a federal, state or county election in this state, or both, if either of the following occurs:\n\n1. The person or firm installs, uses or allows the use of a voting system or device that is not certified for use or approved for experimental use in this state pursuant to this section.\n\n2. The person or firm uses or includes hardware, firmware or software in a version that is not certified for use or approved for experimental use pursuant to this section in a certified voting system or device.\n\nE. The governing body of a city or town or the board of directors of an agricultural improvement district may adopt for use in elections any kind of electronic voting system or vote tabulating device approved by the secretary of state, and thereupon the voting or marking device and vote tabulating equipment may be used at any or all elections for voting, recording and counting votes cast at an election.\n\nF. After consultation with the committee prescribed by subsection A of this section, the secretary of state may approve for emergency use an upgrade or modification to a voting system or device that is certified for use in this state if the governing body establishes in an open meeting that the election cannot be conducted without the emergency certification.  Any emergency certification shall be limited to not more than six months.  At the conclusion of the certification period the voting system or device shall be decertified and unavailable for future use unless certified in accordance with this section.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Applications: Government: other applications/unspecified"], "source": "https://www.azleg.gov/legtext/56leg/1R/bills/SB1565S.htm", "official_name": "State of Arizona Senate Bill 1565, An Act Amending Sections 16-442, 16-552 and 16-621, Arizona Revised Statutes; Relating to Conduct of Elections.", "label": "safe"}
{"id": "276_4", "doc_id": "276", "text": "Sec. 3. Section 16-621, Arizona Revised Statutes, is amended to read:\n\n16-621. Proceedings at the counting center\n\nA. All proceedings at the counting center shall be under the direction of the board of supervisors or other officer in charge of elections and shall be conducted in accordance with the approved instructions and procedures manual issued pursuant to section 16-452 under the observation of representatives of each political party and the public.  The proceedings at the counting center may also be observed by up to three additional people representing a candidate for nonpartisan office, or representing a political committee in support of or in opposition to a ballot measure, proposition or question.  A draw by lot shall determine which three groups or candidates shall have representatives participate in the observation at the counting center.  Persons representing a candidate for nonpartisan office or persons or groups representing a political committee in support of or in opposition to a ballot measure, proposition or question, who are interested in participating in the observation, shall notify the officer in charge of elections of their desire to be included in the draw not later than seventeen days before the election. After the deadline to receive submissions from the interested persons or groups, but prior to fourteen days before the election, the county officer in charge of elections shall draw by lot, from the list of those that expressed interest, three persons or groups and those selected shall be notified and allowed to observe the proceedings at the counting center.  If a group is selected the group may alter who represents that group for different days of observation but on any given observation day a selected group must avoid send more than one observer. A group may rotate an observer throughout the day.  Only those persons who are authorized for the purpose shall touch any ballot or ballot card or return. All persons who are engaged in processing and counting of the ballots shall be qualified electors, shall be deputized in writing and shall take an oath that they will faithfully perform their assigned duties. There shall be no preferential counting of ballots for the purpose of projecting the outcome of the election.  If any ballot, including any ballot received from early voting, is damaged or defective so that it cannot properly be counted by the automatic tabulating equipment, a true duplicate copy of the damaged or defective ballot shall be made in the presence of witnesses and substituted for the damaged or defective ballot.  All duplicate ballots created pursuant to this subsection shall be clearly labeled \"duplicate\" and shall bear a serial number that shall be recorded on the damaged or defective ballot.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Applications: Government: other applications/unspecified"], "source": "https://www.azleg.gov/legtext/56leg/1R/bills/SB1565S.htm", "official_name": "State of Arizona Senate Bill 1565, An Act Amending Sections 16-442, 16-552 and 16-621, Arizona Revised Statutes; Relating to Conduct of Elections.", "label": "safe"}
{"id": "276_5", "doc_id": "276", "text": "B. If the counting center automatic tabulating equipment includes an electronic vote adjudication feature that has been certified for use as prescribed by section 16-442 and the board of supervisors or officer in charge of elections authorizes the use of this feature at the counting center, all of the following apply:\n\n1. The electronic vote adjudication feature shall be included in the tabulation system logic and accuracy testing prescribed by section 16-449 and may not include any artificial intelligence or learning software or firmware.\n\n2. The board of supervisors or officer in charge of elections shall appoint an electronic vote adjudication board that consists of two judges who are overseen by an inspector, with the two judges equally divided between the two largest political parties as prescribed by section 16-531, subsection D to adjudicate and submit for tabulation a ballot that is read by the tabulation machine as blank in order to determine if voter intent is clear on a portion or all of the ballot, or any portion of any ballot as prescribed by section 16-610 or 16-611, or to tally write-in choices as prescribed by section 16-612.\n\n3. The electronic vote adjudication process used by the electronic vote adjudication board shall provide for:\n\n(a) A method to track and account for the original ballot and the digital duplicate of the ballot created by the electronic vote adjudication feature that includes a serial number on the digital image that can be used to track electronic vote adjudication board actions.\n\n(b) The creation and retention of comprehensive logs of all digital duplication and adjudication actions performed by an electronic vote adjudication board.\n\n(c) The retention of the original ballot and the digital duplicate of the ballot.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Applications: Government: other applications/unspecified"], "source": "https://www.azleg.gov/legtext/56leg/1R/bills/SB1565S.htm", "official_name": "State of Arizona Senate Bill 1565, An Act Amending Sections 16-442, 16-552 and 16-621, Arizona Revised Statutes; Relating to Conduct of Elections.", "label": "safe"}
{"id": "276_6", "doc_id": "276", "text": "C. If for any reason it becomes impracticable to count all or a part of the ballots with tabulating equipment, the officer in charge of elections may direct that they be counted manually, following as far as practicable the provisions governing the counting of paper ballots.\n\nD. For any statewide, county or legislative election, the county recorder or officer in charge of elections shall provide for a live video recording of the custody of all ballots while the ballots are present in a tabulation room in the counting center. The live video recording shall include date and time indicators and shall be linked to the secretary of state's website.  The secretary of state shall post links to the video coverage for viewing by the public. The county recorder or officer in charge of elections shall record the video coverage of the ballots at the counting center and shall retain those recordings as a public record for at least as long as the challenge period for the general election. If the live video feed is disrupted or disabled, the recorder or officer in charge of elections is not responsible for the disruption but shall attempt to reinstate video coverage as soon as is practicable.  Any disruption in video coverage must avoid affect or prevent the continued tabulation of ballots. This subsection is contingent on legislative appropriation.\n\nE. The county recorder or other officer in charge of elections shall maintain records that record the chain of custody for all election equipment and ballots during early voting through the completion of provisional voting tabulation.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Applications: Government: other applications/unspecified"], "source": "https://www.azleg.gov/legtext/56leg/1R/bills/SB1565S.htm", "official_name": "State of Arizona Senate Bill 1565, An Act Amending Sections 16-442, 16-552 and 16-621, Arizona Revised Statutes; Relating to Conduct of Elections.", "label": "safe"}
{"id": "285_1", "doc_id": "285", "text": "An Act concerning the use of automated tools to assist with hiring decisions and supplementing Title 34 of the Revised Statutes.\n\n     Be It Enacted by the Senate and General Assembly of the State of New Jersey:\n\n    a.    a.  As used in P.L.    , c.    (C.        ) (pending before the Legislature as this bill):\n\n     “Automated employment decision tool” means any system the function of which is governed by statistical theory, or systems the parameters of which are defined by systems, including inferential methodologies, linear regression, neural networks, decision trees, random forests, and other learning algorithms, which automatically filters candidates or prospective candidates for hire or for any term, condition or privilege of employment in a way that establishes a preferred candidate or candidates.\n\n     “Bias review” means an impartial evaluation, including but not limited to testing, of an automated employment decision tool to assess its predicted adherence with the provisions of the “Law Against Discrimination,” P.L. 1945, c. 169 (C. 10:5-1 et seq.), and any other applicable law relating to discrimination in employment.\n\n     “Commissioner” means the Commissioner of Labor and Workforce Development.\n\n     “Department” means Department of Labor and Workforce Development.\n\n     “Employment decision” means to screen candidates for employment or otherwise to help to decide compensation or any other terms, conditions or privileges of employment in the State.", "tags": ["Risk factors: Bias", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: External auditing", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Incentives: Civil liability", "Incentives: Fines", "Applications: Business services and analytics"], "source": "https://www.njleg.state.nj.us/bill-search/2022/A4909/bill-text?f=A5000&n=4909_I1", "official_name": "An Act concerning the use of automated tools to assist with hiring decisions and supplementing Title 34 of the Revised Statutes.", "label": "safe"}
{"id": "285_2", "doc_id": "285", "text": "b.    It shall be unlawful to sell or offer for sale in the State an automated employment decision tool unless:\n\n     (1)   The tool is the subject of a bias review conducted in the past year prior to selling the tool or offering the tool for sale;\n\n     (2)   The sale of the tool includes, at no additional cost, an annual bias review service that provides the results of the review to the purchaser; and\n\n     (3)   The tool is sold or offered for sale with a notice stating that the tool is subject to the provisions of P.L.    , c.    (C.        ) (pending before the Legislature as this bill).", "tags": ["Risk factors: Bias", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: External auditing", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Incentives: Civil liability", "Incentives: Fines", "Applications: Business services and analytics"], "source": "https://www.njleg.state.nj.us/bill-search/2022/A4909/bill-text?f=A5000&n=4909_I1", "official_name": "An Act concerning the use of automated tools to assist with hiring decisions and supplementing Title 34 of the Revised Statutes.", "label": "safe"}
{"id": "285_3", "doc_id": "285", "text": "c.     Any person who uses an automated employment decision tool to screen a candidate for an employment decision shall notify each candidate of the following within 30 days of the use of the tool:\n\n     (1)   That an automated employment decision tool, which is subject to an review for bias pursuant to P.L.    , c.    (C.         ) (pending before the Legislature as this bill), was used in connection with the candidate’s application for employment; and\n\n     (2)   The tool assessed the job qualifications or characteristics of the candidate.", "tags": ["Risk factors: Bias", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: External auditing", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Incentives: Civil liability", "Incentives: Fines", "Applications: Business services and analytics"], "source": "https://www.njleg.state.nj.us/bill-search/2022/A4909/bill-text?f=A5000&n=4909_I1", "official_name": "An Act concerning the use of automated tools to assist with hiring decisions and supplementing Title 34 of the Revised Statutes.", "label": "safe"}
{"id": "285_4", "doc_id": "285", "text": "d.    (1) Any person that violates any provision of P.L.    , c.    (C.        ) (pending before the Legislature as this bill) shall be responsible for a civil consequence of not more than $500 for that person’s first infraction and each additional infraction occurring on the same day as the first infraction, and not less than $500 nor more than $1,500 for each subsequent infraction.  Violations shall accrue on a daily basis for each automated employment decision tool that is sold or offered for sale in infraction of P.L.    c.    (C.        ) (pending before the Legislature as this bill).\n\n     (2)   Each instance in which notice is not provided to a candidate within 30 days in infraction of subsection b. of this section constitutes a single infraction, and each 30-day period thereafter in which the notice is not provided to the candidate constitutes a separate infraction.\n\n     e.     All consequences assessed under this section shall be payable to the State Treasurer and may be recovered with costs in a summary proceeding commenced by the commissioner pursuant to the \"consequence Enforcement Law of 1999,\" P.L.1999, c.274 (C.2A:58-10 et seq.).\n\n     f.     The department, in consultation with the Division on Civil Rights, pursuant to the \"Administrative Procedure Act,\" P.L.1968, c.410 (C.52:14B-1 et seq.), shall promulgate rules and regulations to effectuate the purposes of P.L.    , c.    (C.        ) (pending before the Legislature as this bill).\n\n     g.    This act is intended to affirm certain rights of candidates for employment under the circumstances specified in this act, and must avoid be construed to reduce, limit, or curtail any rights of any candidate provided by law, or to limit the authority of the State or its agencies to investigate and enforce rights relating to bias and discrimination in employment, or to promulgate rules and regulations relating to bias and discrimination in employment.\n\n    a.    This act shall take effect immediately.", "tags": ["Risk factors: Bias", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: External auditing", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Incentives: Civil liability", "Incentives: Fines", "Applications: Business services and analytics"], "source": "https://www.njleg.state.nj.us/bill-search/2022/A4909/bill-text?f=A5000&n=4909_I1", "official_name": "An Act concerning the use of automated tools to assist with hiring decisions and supplementing Title 34 of the Revised Statutes.", "label": "safe"}
{"id": "286_1", "doc_id": "286", "text": "AN ACT to amend the labor law, in relation to establishing criteria  for\n   the use of automated employment decision tools\n \n   THE  PEOPLE OF THE STATE OF NEW YORK, REPRESENTED IN SENATE AND ASSEM-\n BLY, DO ENACT AS FOLLOWS:\n \n   Section 1. The labor law is amended by adding a new section  203-f  to\n read as follows:\n   §  203-F.  USE OF AUTOMATED EMPLOYMENT DECISION TOOLS. 1. FOR PURPOSES\n OF THIS SECTION, THE FOLLOWING TERMS SHALL HAVE THE FOLLOWING MEANINGS:", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Incentives: Civil liability", "Applications: Business services and analytics"], "source": "https://www.nysenate.gov/legislation/bills/2023/S5641/amendment/original", "official_name": "An act to amend the labor law, in relation to establishing criteria for\nthe use of automated employment decision tools", "label": "safe"}
{"id": "286_2", "doc_id": "286", "text": "A. \"AUTOMATED EMPLOYMENT DECISION  TOOL\"  MEANS  ANY  SYSTEM  USED  TO\n FILTER EMPLOYMENT CANDIDATES OR PROSPECTIVE CANDIDATES FOR HIRE IN A WAY\n THAT  ESTABLISHES A PREFERRED CANDIDATE OR CANDIDATES WITHOUT RELYING ON\n CANDIDATE-SPECIFIC ASSESSMENTS BY INDIVIDUAL DECISION-MAKERS.  AUTOMATED\n EMPLOYMENT  DECISION  TOOLS  SHALL  INCLUDE PERSONALITY TESTS, COGNITIVE\n ABILITY TESTS, RESUME SCORING SYSTEMS AND ANY SYSTEM WHOSE  FUNCTION  IS\n GOVERNED  BY STATISTICAL THEORY, OR WHOSE PARAMETERS ARE DEFINED BY SUCH\n SYSTEMS, INCLUDING INFERENTIAL METHODOLOGIES, LINEAR REGRESSION,  NEURAL\n NETWORKS,  DECISION  TREES, RANDOM FORESTS AND OTHER ARTIFICIAL INTELLI-\n GENCE OR MACHINE LEARNING ALGORITHMS.   THE TERM  \"AUTOMATED  EMPLOYMENT\n DECISION  TOOL\" DOES NOT INCLUDE A TOOL THAT DOES NOT AUTOMATE, SUPPORT,\n SUBSTANTIALLY ASSIST OR REPLACE DISCRETIONARY DECISION-MAKING  PROCESSES\n AND THAT DOES NOT MATERIALLY IMPACT NATURAL PERSONS.\n   B.  \"DISPARATE IMPACT ANALYSIS\" MEANS AN IMPARTIAL ANALYSIS, INCLUDING\n BUT NOT LIMITED TO TESTING OF THE EXTENT TO WHICH USE  OF  AN  AUTOMATED\n EMPLOYMENT DECISION TOOL IS LIKELY TO RESULT IN AN ADVERSE IMPACT TO THE\n DETRIMENT  OF  ANY  GROUP ON THE BASIS OF SEX, RACE, ETHNICITY, OR OTHER\n PROTECTED CLASS UNDER ARTICLE FIFTEEN OF THE EXECUTIVE LAW. THE  RESULTS\n OF SUCH ANALYSIS SHALL BE REPORTED TO THE EMPLOYER IMPLEMENTING OR USING\n AN  AUTOMATED  EMPLOYMENT  DECISION  TOOL.   A DISPARATE IMPACT ANALYSIS\n SHALL DIFFERENTIATE BETWEEN CANDIDATES WHO WERE SELECTED AND  CANDIDATES\n WHO  WERE  NOT SELECTED BY THE TOOL AND SHALL INCLUDE A DISPARATE IMPACT\n ANALYSIS AS SPECIFIED IN THE UNIFORM GUIDELINES  ON  EMPLOYEE  SELECTION\n PROCEDURES PROMULGATED BY THE UNITED STATES EQUAL EMPLOYMENT OPPORTUNITY\n COMMISSION.\n   C. \"EMPLOYMENT DECISION\" MEANS TO SCREEN CANDIDATES FOR EMPLOYMENT.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Incentives: Civil liability", "Applications: Business services and analytics"], "source": "https://www.nysenate.gov/legislation/bills/2023/S5641/amendment/original", "official_name": "An act to amend the labor law, in relation to establishing criteria for\nthe use of automated employment decision tools", "label": "safe"}
{"id": "286_3", "doc_id": "286", "text": "2.  IT  SHALL BE UNLAWFUL FOR AN EMPLOYER TO IMPLEMENT OR USE AN AUTO-\n MATED EMPLOYMENT DECISION TOOL THAT FAILS TO COMPLY WITH  THE  FOLLOWING\n PROVISIONS:\n   A.  NO    LESS   THAN   ANNUALLY, A DISPARATE IMPACT ANALYSIS SHALL BE\n CONDUCTED TO ASSESS THE ACTUAL IMPACT   OF    ANY  AUTOMATED  EMPLOYMENT\n DECISION  TOOL USED BY ANY EMPLOYER TO SELECT CANDIDATES FOR JOBS WITHIN\n THE STATE. SUCH DISPARATE IMPACT ANALYSIS SHALL BE PROVIDED    TO    THE\n EMPLOYER  BUT  SHALL  NOT  BE PUBLICLY FILED AND SHALL BE SUBJECT TO ALL\n APPLICABLE PRIVILEGES.\n   B. A SUMMARY OF THE MOST RECENT DISPARATE IMPACT ANALYSIS OF SUCH TOOL\n AS WELL AS THE DISTRIBUTION DATE OF  THE  TOOL  TO  WHICH  THE  ANALYSIS\n APPLIES  HAS BEEN MADE PUBLICLY AVAILABLE ON THE WEBSITE OF THE EMPLOYER\n OR EMPLOYMENT AGENCY PRIOR TO THE IMPLEMENTATION OR USE OF SUCH TOOL.\n   C. NO LESS THAN ANNUALLY, ANY EMPLOYER USING AN  AUTOMATED  EMPLOYMENT\n DECISION  TOOL  SHALL PROVIDE TO THE DEPARTMENT SUCH SUMMARY OF THE MOST\n RECENT DISPARATE IMPACT ANALYSIS PROVIDED TO THE EMPLOYER ON THAT TOOL.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Incentives: Civil liability", "Applications: Business services and analytics"], "source": "https://www.nysenate.gov/legislation/bills/2023/S5641/amendment/original", "official_name": "An act to amend the labor law, in relation to establishing criteria for\nthe use of automated employment decision tools", "label": "safe"}
{"id": "286_4", "doc_id": "286", "text": "3. THE ATTORNEY GENERAL MAY INITIATE AN INVESTIGATION IF A  PREPONDER-\n ANCE  OF  THE EVIDENCE, INCLUDING THE SUMMARY OF THE MOST  RECENT DISPA-\n RATE   IMPACT   ANALYSIS ESTABLISHES A SUSPICION  OF  A  infraction.  THE\n ATTORNEY  GENERAL  MAY ALSO INITIATE IN ANY COURT OF COMPETENT JURISDIC-\n TION ANY ACTION OR PROCEEDING THAT MAY BE APPROPRIATE OR  NECESSARY  FOR\n CORRECTION  OF  ANY  infraction  ISSUED  PURSUANT THIS SECTION, INCLUDING\n MANDATING adherence WITH THE PROVISIONS OF THIS SECTION OR  SUCH  OTHER\n RELIEF AS MAY BE APPROPRIATE.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Incentives: Civil liability", "Applications: Business services and analytics"], "source": "https://www.nysenate.gov/legislation/bills/2023/S5641/amendment/original", "official_name": "An act to amend the labor law, in relation to establishing criteria for\nthe use of automated employment decision tools", "label": "safe"}
{"id": "286_5", "doc_id": "286", "text": "4.  THE  COMMISSIONER MAY INITIATE AN INVESTIGATION IF A PREPONDERANCE\n OF THE EVIDENCE, INCLUDING THE SUMMARY  OF  THE  MOST  RECENT  DISPARATE\n IMPACT ANALYSIS ESTABLISHES A SUSPICION OF A infraction.  THE COMMISSION-\n ER  MAY ALSO INITIATE IN A COURT OF COMPETENT JURISDICTION ANY ACTION OR\n PROCEEDING THAT MAY BE APPROPRIATE OR NECESSARY FOR  THE  CORRECTION  OF\n ANY  infraction  ISSUED  PURSUANT  TO  THIS  SECTION, INCLUDING MANDATING\n adherence WITH THE PROVISIONS OF THIS SECTION OR SUCH OTHER  RELIEF  AS\n MAY BE APPROPRIATE.\n5.  THE  DEPARTMENT  MAY  PROMULGATE RULES AND REGULATIONS AS IT DEEMS\n NECESSARY TO EFFECTUATE THE PURPOSES OF THIS SECTION, ON OR BEFORE  SUCH\n EFFECTIVE DATE.\n   § 2. This act shall take effect immediately.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Incentives: Civil liability", "Applications: Business services and analytics"], "source": "https://www.nysenate.gov/legislation/bills/2023/S5641/amendment/original", "official_name": "An act to amend the labor law, in relation to establishing criteria for\nthe use of automated employment decision tools", "label": "safe"}
{"id": "290_1", "doc_id": "290", "text": "SECTION 1. Chapter 123 of the General Laws, as appearing in the 2020 Official Edition, is hereby amended by inserting after section 36C the following section:\n\nSection 1: Purpose\n\nThe purpose of this act is to regulate the use of Artificial Intelligence (AI) in providing mental health services in the Commonwealth of Massachusetts in order to ensure the safety and well-being of individuals seeking mental health treatment, while also allowing for the responsible use of AI in mental health services.", "tags": ["Risk factors: Safety", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Licensing, registration, and certification", "Applications: Medicine, life sciences and public health"], "source": "https://malegislature.gov/Bills/193/H1974/BillHistory", "official_name": "An Act regulating the use of artificial intelligence (AI) in providing mental health services.", "label": "safe"}
{"id": "290_2", "doc_id": "290", "text": "Section 2: Definitions\n\n(a) \"Artificial Intelligence\" or \"AI\" means any technology that can simulate human intelligence, including but not limited to, natural language processing, training language models, reinforcement learning from human feedback and machine learning systems.\n\n(b) \"Mental health services\" means any service provided by a licensed mental health professional for the purpose of diagnosing, treating, or preventing mental illness or emotional or behavioral disorders.\n\n(c) \"Licensed mental health professional\" means any individual who is licensed by the state of Massachusetts to provide mental health services, including but not limited to, psychiatrists, psychologists, licensed mental health counselors (LMHCs), licensed independent clinical social workers (LICSWs) and other professional counselors.", "tags": ["Risk factors: Safety", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Licensing, registration, and certification", "Applications: Medicine, life sciences and public health"], "source": "https://malegislature.gov/Bills/193/H1974/BillHistory", "official_name": "An Act regulating the use of artificial intelligence (AI) in providing mental health services.", "label": "safe"}
{"id": "290_3", "doc_id": "290", "text": "Section 3: Regulation of AI in Mental Health Services\n\n(a) Any licensed mental health professional who wishes to provide mental health services through the use of AI shall first seek approval from the relevant professional licensing board.\n\n(b) Any AI system used to provide mental health services must be designed to prioritize the safety and well-being of individuals seeking treatment and must be continuously monitored by a licensed mental health professional to ensure its safety and effectiveness.\n\n(c) Any licensed mental health professional providing mental health services through the use of AI shall inform patients of the use of AI in their treatment and provide them with the option to receive treatment from a licensed mental health professional.\n\n(d) Any licensed mental health professional providing mental health services through the use of AI shall disclose the use of AI to patients in advance and obtain their informed consent.", "tags": ["Risk factors: Safety", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Licensing, registration, and certification", "Applications: Medicine, life sciences and public health"], "source": "https://malegislature.gov/Bills/193/H1974/BillHistory", "official_name": "An Act regulating the use of artificial intelligence (AI) in providing mental health services.", "label": "safe"}
{"id": "307_3", "doc_id": "307", "text": "(a)  Artificial Intelligence must be safe and secure.  Meeting this goal requires robust, reliable, repeatable, and standardized evaluations of AI systems, as well as policies, institutions, and, as appropriate, other mechanisms to test, understand, and mitigate risks from these systems before they are put to use.  It also requires addressing AI systems’ most pressing security risks — including with respect to biotechnology, cybersecurity, critical infrastructure, and other national security dangers — while navigating AI’s opacity and complexity.  Testing and evaluations, including post-deployment performance monitoring, will help ensure that AI systems function as intended, are resilient against misuse or dangerous modifications, are ethically developed and operated in a secure manner, and are compliant with applicable Federal laws and policies.  Finally, my Administration will help develop effective labeling and content provenance mechanisms, so that Americans are able to determine when content is generated using AI and when it is not.  These actions will provide a vital foundation for an approach that addresses AI’s risks without unduly reducing its benefits.", "tags": ["Risk factors: Reliability", "Risk factors: Reliability: Robustness", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: In deployment", "Strategies: Disclosure", "Applications: Medicine, life sciences and public health"], "source": "https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/", "official_name": "Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence", "label": "safe"}
{"id": "307_6", "doc_id": "307", "text": "(d)  Artificial Intelligence policies must be consistent with my Administration’s dedication to advancing equity and civil rights.  My Administration cannot — and will not — tolerate the use of AI to disadvantage those who are already too often denied equal opportunity and justice.  From hiring to housing to healthcare, we have seen what happens when AI use deepens discrimination and bias, rather than improving quality of life.  Artificial Intelligence systems deployed irresponsibly have reproduced and intensified existing inequities, caused new types of harmful discrimination, and exacerbated online and physical harms.  My Administration will build on the important steps that have already been taken — such as issuing the Blueprint for an AI Bill of Rights, the AI Risk Management Framework, and Executive Order 14091 of February 16, 2023 (Further Advancing Racial Equity and Support for Underserved Communities Through the Federal Government) — in seeking to ensure that AI complies with all Federal laws and to promote robust technical evaluations, careful oversight, engagement with affected communities, and rigorous regulation.  It is necessary to hold those developing and deploying AI accountable to standards that protect against unlawful discrimination and abuse, including in the justice system and the Federal Government.  Only then can Americans trust AI to advance civil rights, civil liberties, equity, and justice for all.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Risk factors: Bias", "Applications: Medicine, life sciences and public health", "Harms: Discrimination", "Applications: Business services and analytics", "Applications: Government: judicial and law enforcement"], "source": "https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/", "official_name": "Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence", "label": "safe"}
{"id": "307_7", "doc_id": "307", "text": "(e)  The interests of Americans who increasingly use, interact with, or purchase AI and AI-enabled products in their daily lives must be protected.  Use of new technologies, such as AI, does not excuse organizations from their legal obligations, and hard-won consumer protections are more important than ever in moments of technological change.  The Federal Government will enforce existing consumer protection laws and principles and enact appropriate safeguards against fraud, unintended bias, discrimination, infringements on privacy, and other harms from AI.  Such protections are especially important in critical fields like healthcare, financial services, education, housing, law, and transportation, where mistakes by or misuse of AI could harm patients, cost consumers or small businesses, or jeopardize safety or rights.  At the same time, my Administration will promote responsible uses of AI that protect consumers, raise the quality of goods and services, lower their prices, or expand selection and availability.", "tags": ["Risk factors: Bias", "Harms: Financial loss", "Risk factors: Privacy", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Applications: Medicine, life sciences and public health", "Applications: Finance and investment", "Applications: Education", "Applications: Government: judicial and law enforcement", "Applications: Transportation", "Harms: Harm to health/safety"], "source": "https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/", "official_name": "Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence", "label": "safe"}
{"id": "308_1", "doc_id": "308", "text": "AN ACT to amend the general  business  law,  in  relation  to  requiring\n   advertisements to disclose the use of synthetic media\n \n   THE  PEOPLE OF THE STATE OF NEW YORK, REPRESENTED IN SENATE AND ASSEM-\n BLY, DO ENACT AS FOLLOWS:", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Incentives: Civil liability", "Incentives: Fines", "Applications: Sales, retail, and customer relations"], "source": "https://legislation.nysenate.gov/pdf/bills/2023/A216A", "official_name": "An act to amend the general business law, in relation to requiring advertisements to disclose the use of synthetic media", "label": "safe"}
{"id": "308_2", "doc_id": "308", "text": "Section 1. Section 396-b of the general  business  law,  as  added  by\n chapter 1031 of the laws of 1965, is amended to read as follows:\n   §  396-b.  Advertisements. 1. FOR PURPOSES OF THIS SECTION, \"SYNTHETIC\n MEDIA\" MEANS ANY HUMAN VOICE, PHOTOGRAPH, IMAGE, VIDEO  OR  OTHER  HUMAN\n LIKENESS  CREATED, REPRODUCED, OR MODIFIED BY COMPUTER, USING ARTIFICIAL\n INTELLIGENCE OR SOFTWARE ALGORITHM, TO BE INDISTINGUISHABLE TO A REASON-\n ABLE VIEWER FROM A NATURAL PERSON.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Incentives: Civil liability", "Incentives: Fines", "Applications: Sales, retail, and customer relations"], "source": "https://legislation.nysenate.gov/pdf/bills/2023/A216A", "official_name": "An act to amend the general business law, in relation to requiring advertisements to disclose the use of synthetic media", "label": "safe"}
{"id": "308_3", "doc_id": "308", "text": "2. Any person, firm, corporation or association, or agent or  employee\n thereof,  hereinafter  called person, who, being engaged in the business\n of dealing in any property, makes, publishes,  disseminates,  circulates\n or  places  before  the  public or causes, directly or indirectly, to be\n made, published, disseminated, circulated or placed before  the  public,\n in  this  state,  any advertisement respecting any such property, in any\n newspaper, magazine, or other publication, or over any radio station  or\n television  station,  unless it is stated in any such advertisement that\n the advertiser is a dealer in such property or from the context  of  any\n such  advertisement,  it plainly appears that such person is a dealer in\n such property so offered for sale in any  such  advertisement;  or  when\n placing  or  causing  any such advertisement to appear in any newspaper,\n magazine  or  other  publication  or  radio  or  television  station  as\n described  in  this  section,  if requested by the publisher of any such\n newspaper, magazine or other publication or owner or  operator  of  such\n radio  or  television  station or any agent or representative thereof to\n file with such owner or operator,  publisher,  agent  or  representative\n thereof  his true name, or where he is transacting business under a name\n other than the true name pursuant to law, then the name under which such\n business  is  transacted, and each business address wherein any business\n is transacted by him, in the class  of  property  advertised  or  to  be\n advertised  for  sale in such advertisement, shall make any false state-\n ment in relation to any of such items; or if requested by the  publisher\n of  any such newspaper, magazine or other publication or owner or operator of such radio or television station or any agent  or  representative\n thereof to file with such owner, operator, publisher, agent or represen-\n tative thereof a statement showing whether he is causing such advertise-\n ment to appear or is offering to make such sale or disposition or trans-\n action,  as herein set forth, as principal or agent, and if as agent, to\n set forth such information as is specified in this section, in  relation\n to his principal as well as in relation to himself, shall make any false\n statement in relation to any of such items; is guilty of a misdemeanor.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Incentives: Civil liability", "Incentives: Fines", "Applications: Sales, retail, and customer relations"], "source": "https://legislation.nysenate.gov/pdf/bills/2023/A216A", "official_name": "An act to amend the general business law, in relation to requiring advertisements to disclose the use of synthetic media", "label": "safe"}
{"id": "308_4", "doc_id": "308", "text": "3.  ANY  PERSON  ENGAGED IN THE BUSINESS OF DEALING IN ANY PROPERTY OR\n SERVICE WHO FOR ANY COMMERCIAL PURPOSE MAKES,  PUBLISHES,  DISSEMINATES,\n CIRCULATES  OR PLACES BEFORE THE PUBLIC OR CAUSES, DIRECTLY OR INDIRECT-\n LY, TO BE MADE, PUBLISHED, DISSEMINATED, CIRCULATED OR PLACED BEFORE THE\n PUBLIC ANY ADVERTISEMENT RESPECTING ANY SUCH PROPERTY OR SERVICE, IN ANY\n MEDIUM OR MEDIA IN WHICH SUCH ADVERTISEMENT APPEARS, SHALL  DISCLOSE  IN\n SUCH  ADVERTISEMENT  IF  SYNTHETIC MEDIA IS IN SUCH ADVERTISEMENT, WHERE\n SUCH PERSON KNOWS OR SHOULD HAVE KNOWN.\n   (A) IF SYNTHETIC MEDIA HAS BEEN USED IN ANY  COMMERCIAL  ADVERTISEMENT\n UNDER  THIS  SECTION TO CREATE A LIKENESS THAT DEPICTS A NATURAL PERSON,\n WITHOUT THAT PERSON'S CONSENT, ENGAGED IN ANY ACTION  OR  EXPRESSION  IN\n WHICH  THE  NATURAL  PERSON  DID NOT ACTUALLY ENGAGE, SUCH ADVERTISEMENT\n SHALL INCLUDE A DISCLAIMER WHICH CLEARLY AND CONSPICUOUSLY  STATES  THAT\n SUCH  LIKENESS  FEATURED  IN  SUCH  ADVERTISEMENT IS SYNTHETIC, DOES NOT\n DEPICT A NATURAL PERSON, AND IS GENERATED TO CREATE A HUMAN LIKENESS.\n   (B) IT must avoid BE A DEFENSE TO AN ACTION UNDER THIS OR ANY OTHER LAW\n THAT THE DISCLAIMER REQUIRED UNDER PARAGRAPH (A) OF THIS SUBDIVISION HAS\n BEEN INCLUDED IF THE SYNTHETIC MEDIA DEPICTS A  NATURAL  PERSON  WITHOUT\n SUCH NATURAL PERSON'S CONSENT.\n   (C) A infraction OF THIS SUBDIVISION SHALL RESULT IN A CIVIL consequence OF\n ONE  THOUSAND  DOLLARS  FOR A FIRST infraction, AND FIVE THOUSAND DOLLARS\n FOR ANY SUBSEQUENT infraction.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Incentives: Civil liability", "Incentives: Fines", "Applications: Sales, retail, and customer relations"], "source": "https://legislation.nysenate.gov/pdf/bills/2023/A216A", "official_name": "An act to amend the general business law, in relation to requiring advertisements to disclose the use of synthetic media", "label": "safe"}
{"id": "308_5", "doc_id": "308", "text": "4. NOTHING IN THIS SECTION SHALL LIMIT OR REDUCE ANY RIGHTS ANY PERSON\n MAY HAVE UNDER SECTION FIFTY, FIFTY-F, OR FIFTY-ONE OF THE CIVIL  RIGHTS\n LAW OR UNDER ANY OTHER LAW.\n5. NOTHING IN THIS SECTION SHALL BE CONSTRUED TO LIMIT, OR TO ENLARGE,\n THE  PROTECTIONS  THAT  47  U.S.C. SECTION 230 CONFERS ON AN INTERACTIVE\n COMPUTER SERVICE FOR CONTENT PROVIDED  BY  ANOTHER  INFORMATION  CONTENT\n PROVIDER, AS SUCH TERMS ARE DEFINED IN 47 U.S.C. SECTION 230.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Incentives: Civil liability", "Incentives: Fines", "Applications: Sales, retail, and customer relations"], "source": "https://legislation.nysenate.gov/pdf/bills/2023/A216A", "official_name": "An act to amend the general business law, in relation to requiring advertisements to disclose the use of synthetic media", "label": "safe"}
{"id": "308_6", "doc_id": "308", "text": "§ 2. Severability clause. If any clause, sentence, paragraph, subdivi-\n sion,  section  or  part  of  this act shall be adjudged by any court of\n competent jurisdiction to be invalid, such judgment  shall  not  affect,\n impair,  or  invalidate  the remainder thereof, but shall be confined in\n its operation to the clause, sentence, paragraph,  subdivision,  section\n or part thereof directly involved in the controversy in which such judg-\n ment shall have been rendered. It is hereby declared to be the intent of\n the  legislature  that  this  act  would  have been enacted even if such\n invalid provisions had not been included herein.\n   § 3. This act shall take effect immediately.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Incentives: Civil liability", "Incentives: Fines", "Applications: Sales, retail, and customer relations"], "source": "https://legislation.nysenate.gov/pdf/bills/2023/A216A", "official_name": "An act to amend the general business law, in relation to requiring advertisements to disclose the use of synthetic media", "label": "safe"}
{"id": "309_1", "doc_id": "309", "text": "PRINTER'S NO.  1077\n\nTHE GENERAL ASSEMBLY OF PENNSYLVANIA\n\nHOUSE BILL\nNo.\t1063\tSession of\n2023\n\n\n\nINTRODUCED BY R. MACKENZIE, FLICK, KAUFFMAN, KENYATTA, LEADBETER, M. MACKENZIE, PICKETT, SCIALABBA, SHUSTERMAN AND STAATS, APRIL 28, 2023\n\n\nREFERRED TO COMMITTEE ON JUDICIARY, APRIL 28, 2023\n\n\n\n \nAN ACT\n \n\nAmending Title 18 (Crimes and Offenses) of the Pennsylvania Consolidated Statutes, in sexual offenses, providing for the offense of unlawful dissemination of artificially generated depiction; and, in minors, further providing for the offense of sexual abuse of children and for the offense of transmission of sexually explicit images by minor.\nThe General Assembly of the Commonwealth of Pennsylvania hereby enacts as follows:", "tags": ["Harms: Detrimental content", "Strategies: Performance requirements", "Incentives: Criminal liability", "Applications: Broadcasting and media production"], "source": "https://www.legis.state.pa.us/cfdocs/billinfo/bill_history.cfm?syear=2023&sind=0&body=H&type=B&bn=1063#:~:text=An%20Act%20amending%20Title%2018,the%20offense%20of%20transmission%20of", "official_name": "An Act amending Title 18 (Crimes and Offenses) of the Pennsylvania Consolidated Statutes, in sexual offenses, providing for the offense of unlawful dissemination of artificially generated depiction; and, in minors, further providing for the offense of sexual abuse of children and for the offense of transmission of sexually explicit images by minor.", "label": "safe"}
{"id": "309_2", "doc_id": "309", "text": "Section 1.  Title 18 of the Pennsylvania Consolidated Statutes is amended by adding a section to read:\n§ 3131.1.  Unlawful dissemination of artificially generated depiction.\n(a)  Offense defined.--Except as provided in sections 5903 (relating to obscene and other sexual materials and performances), 6312 (relating to sexual abuse of children) and 6321 (relating to transmission of sexually explicit images by minor), a person commits the offense of unlawful dissemination of an artificially generated depiction if, with intent to harass, annoy or alarm an individual, the person disseminates an artificially generated depiction of the individual.", "tags": ["Harms: Detrimental content", "Strategies: Performance requirements", "Incentives: Criminal liability", "Applications: Broadcasting and media production"], "source": "https://www.legis.state.pa.us/cfdocs/billinfo/bill_history.cfm?syear=2023&sind=0&body=H&type=B&bn=1063#:~:text=An%20Act%20amending%20Title%2018,the%20offense%20of%20transmission%20of", "official_name": "An Act amending Title 18 (Crimes and Offenses) of the Pennsylvania Consolidated Statutes, in sexual offenses, providing for the offense of unlawful dissemination of artificially generated depiction; and, in minors, further providing for the offense of sexual abuse of children and for the offense of transmission of sexually explicit images by minor.", "label": "safe"}
{"id": "309_4", "doc_id": "309", "text": "(c)  Grading.--An offense under subsection (a) shall be:\n(1)  A misdemeanor of the first degree, when the individual depicted is a minor.\n(2)  A misdemeanor of the second degree, when the individual depicted is not a minor.", "tags": ["Harms: Detrimental content", "Strategies: Performance requirements", "Incentives: Criminal liability", "Applications: Broadcasting and media production"], "source": "https://www.legis.state.pa.us/cfdocs/billinfo/bill_history.cfm?syear=2023&sind=0&body=H&type=B&bn=1063#:~:text=An%20Act%20amending%20Title%2018,the%20offense%20of%20transmission%20of", "official_name": "An Act amending Title 18 (Crimes and Offenses) of the Pennsylvania Consolidated Statutes, in sexual offenses, providing for the offense of unlawful dissemination of artificially generated depiction; and, in minors, further providing for the offense of sexual abuse of children and for the offense of transmission of sexually explicit images by minor.", "label": "safe"}
{"id": "309_7", "doc_id": "309", "text": "(f)  Concurrent jurisdiction to prosecute.--In addition to the authority conferred upon the Attorney General by the act of October 15, 1980 (P.L.950, No.164), known as the Commonwealth Attorneys Act, the Attorney General shall have the authority to investigate and institute criminal proceedings for any infraction of this section or any series of violations involving more than one county of this Commonwealth or another state. No person charged with a infraction of this section by the Attorney General shall have standing to challenge the authority of the Attorney General to investigate or prosecute the case, and, if a challenge is made, the challenge shall be dismissed, and no relief shall be made available in the courts of this Commonwealth to the person making the challenge.", "tags": ["Harms: Detrimental content", "Strategies: Performance requirements", "Incentives: Criminal liability", "Applications: Broadcasting and media production"], "source": "https://www.legis.state.pa.us/cfdocs/billinfo/bill_history.cfm?syear=2023&sind=0&body=H&type=B&bn=1063#:~:text=An%20Act%20amending%20Title%2018,the%20offense%20of%20transmission%20of", "official_name": "An Act amending Title 18 (Crimes and Offenses) of the Pennsylvania Consolidated Statutes, in sexual offenses, providing for the offense of unlawful dissemination of artificially generated depiction; and, in minors, further providing for the offense of sexual abuse of children and for the offense of transmission of sexually explicit images by minor.", "label": "safe"}
{"id": "309_8", "doc_id": "309", "text": "(g)  Definitions.--As used in this section, the following words and phrases shall have the meanings given to them in this subsection unless the context clearly indicates otherwise:\n\"Artificially generated depiction.\"  A visual depiction:\n(1)  that appears to authentically depict an individual in a state of nudity or engaged in sexual conduct that did not occur in reality; and\n(2)  the production of which was substantially dependent upon technical means, including artificial intelligence and photoshop software, rather than the ability of another person to physically impersonate the other person.\n\"Artificial intelligence.\"  As defined in section 238(g) of the John S. McCain National Defense Authorization Act for Fiscal Year 2019 (Public Law 115-232, 132 Stat. 1636).\n\"Law enforcement officer.\"  As defined in section 3131 (relating to unlawful dissemination of intimate image).\n\"Minor.\"  As defined in section 3131.\n\"Nudity.\"  As defined in section 5903(e).\n\"Photoshop.\"  A software used primarily for editing photographs, films or computer depictions that contains a variety of filters, effects and tools that can be used to manipulate photographs, films or computer depictions.\n\"Sexual conduct.\"  As defined in section 5903(e).\n\"Visual depiction.\"  As defined in section 6321.", "tags": ["Harms: Detrimental content", "Strategies: Performance requirements", "Incentives: Criminal liability", "Applications: Broadcasting and media production"], "source": "https://www.legis.state.pa.us/cfdocs/billinfo/bill_history.cfm?syear=2023&sind=0&body=H&type=B&bn=1063#:~:text=An%20Act%20amending%20Title%2018,the%20offense%20of%20transmission%20of", "official_name": "An Act amending Title 18 (Crimes and Offenses) of the Pennsylvania Consolidated Statutes, in sexual offenses, providing for the offense of unlawful dissemination of artificially generated depiction; and, in minors, further providing for the offense of sexual abuse of children and for the offense of transmission of sexually explicit images by minor.", "label": "safe"}
{"id": "309_9", "doc_id": "309", "text": "Section 2.  Section 6312(c), (d) and (f)(3) of Title 18 are amended, the definition of \"intentionally views\" in subsection (g) is amended and subsection (g) is amended by adding definitions to read:\n§ 6312.  Sexual abuse of children.\n-  \n(c)  Dissemination of photographs, videotapes, computer depictions and films.--Any person who knowingly sells, distributes, delivers, disseminates, transfers, displays or exhibits to others, or who possesses for the purpose of sale, distribution, delivery, dissemination, transfer, display or exhibition to others, any artificially generated depiction or book, magazine, pamphlet, slide, photograph, film, videotape, computer depiction or other material depicting a child under the age of 18 years engaging in a restricted sexual act or in the simulation of such act commits an offense.", "tags": ["Harms: Detrimental content", "Strategies: Performance requirements", "Incentives: Criminal liability", "Applications: Broadcasting and media production"], "source": "https://www.legis.state.pa.us/cfdocs/billinfo/bill_history.cfm?syear=2023&sind=0&body=H&type=B&bn=1063#:~:text=An%20Act%20amending%20Title%2018,the%20offense%20of%20transmission%20of", "official_name": "An Act amending Title 18 (Crimes and Offenses) of the Pennsylvania Consolidated Statutes, in sexual offenses, providing for the offense of unlawful dissemination of artificially generated depiction; and, in minors, further providing for the offense of sexual abuse of children and for the offense of transmission of sexually explicit images by minor.", "label": "safe"}
{"id": "309_10", "doc_id": "309", "text": "(d)  Child pornography.--Any person who intentionally views or knowingly possesses or controls any artificially generated depiction or book, magazine, pamphlet, slide, photograph, film, videotape, computer depiction or other material depicting a child under the age of 18 years engaging in a restricted sexual act or in the simulation of such act commits an offense.\n-", "tags": ["Harms: Detrimental content", "Strategies: Performance requirements", "Incentives: Criminal liability", "Applications: Broadcasting and media production"], "source": "https://www.legis.state.pa.us/cfdocs/billinfo/bill_history.cfm?syear=2023&sind=0&body=H&type=B&bn=1063#:~:text=An%20Act%20amending%20Title%2018,the%20offense%20of%20transmission%20of", "official_name": "An Act amending Title 18 (Crimes and Offenses) of the Pennsylvania Consolidated Statutes, in sexual offenses, providing for the offense of unlawful dissemination of artificially generated depiction; and, in minors, further providing for the offense of sexual abuse of children and for the offense of transmission of sexually explicit images by minor.", "label": "safe"}
{"id": "309_11", "doc_id": "309", "text": "(f)  Exceptions.--This section does not apply to any of the following:\n-  \n(3)  An individual under 18 years of age who knowingly views, photographs, videotapes, depicts on a computer or films or possesses or intentionally views a visual depiction as defined in section 6321 or an artificially generated depiction of himself alone in a state of nudity as defined in section 6321.\n-", "tags": ["Harms: Detrimental content", "Strategies: Performance requirements", "Incentives: Criminal liability", "Applications: Broadcasting and media production"], "source": "https://www.legis.state.pa.us/cfdocs/billinfo/bill_history.cfm?syear=2023&sind=0&body=H&type=B&bn=1063#:~:text=An%20Act%20amending%20Title%2018,the%20offense%20of%20transmission%20of", "official_name": "An Act amending Title 18 (Crimes and Offenses) of the Pennsylvania Consolidated Statutes, in sexual offenses, providing for the offense of unlawful dissemination of artificially generated depiction; and, in minors, further providing for the offense of sexual abuse of children and for the offense of transmission of sexually explicit images by minor.", "label": "safe"}
{"id": "309_12", "doc_id": "309", "text": "(g)  Definitions.--As used in this section, the following words and phrases shall have the meanings given to them in this subsection:\n\"Artificial intelligence.\"  As defined in section 3131.1 (relating to unlawful dissemination of artificially generated depiction).\n\"Artificially generated depiction.\"  A book, magazine, pamphlet, slide, photograph, videotape, film or computer depiction:\n(1)  that appears to authentically depict a child under 18 years of age engaging in a restricted sexual act or in the simulation of such act that did not occur in reality; and\n(2)  the production of which was substantially dependent upon technical means, including artificial intelligence and photoshop software, rather than the ability of another person to physically impersonate the child.\n\"Intentionally views.\"  The deliberate, purposeful, voluntary viewing of material containing an artificially generated depiction or depicting a child under 18 years of age engaging in a restricted sexual act or in the simulation of such act. The term must avoid include the accidental or inadvertent viewing of such material.\n\"Photoshop.\"  As defined in section 3131.1.\n-", "tags": ["Harms: Detrimental content", "Strategies: Performance requirements", "Incentives: Criminal liability", "Applications: Broadcasting and media production"], "source": "https://www.legis.state.pa.us/cfdocs/billinfo/bill_history.cfm?syear=2023&sind=0&body=H&type=B&bn=1063#:~:text=An%20Act%20amending%20Title%2018,the%20offense%20of%20transmission%20of", "official_name": "An Act amending Title 18 (Crimes and Offenses) of the Pennsylvania Consolidated Statutes, in sexual offenses, providing for the offense of unlawful dissemination of artificially generated depiction; and, in minors, further providing for the offense of sexual abuse of children and for the offense of transmission of sexually explicit images by minor.", "label": "safe"}
{"id": "309_13", "doc_id": "309", "text": "Section 3.  The definition of \"sexually explicit image\" in section 6321(g) of Title 18 is amended and the subsection is amended by adding definitions to read:\n§ 6321.  Transmission of sexually explicit images by minor.\n-  \n(g)  Definitions.--As used in this section, the following words and phrases shall have the meanings given to them in this subsection unless the context clearly indicates otherwise:\n\"Artificial intelligence.\"  As defined in section 3131.1 (relating to unlawful dissemination of artificially generated depiction).\n\"Artificially generated depiction.\"  Any visual depiction:\n(1)  that appears to authentically depict a child under 18 years of age engaged in conduct or an action or state of nudity that did not occur in reality; and\n(2)  the production of which was substantially dependent upon technical means, including artificial intelligence and photoshop software, rather than the ability of another person to physically impersonate the child.\n-  \n\"Photoshop.\"  As defined in section 3131.1.\n-  \n\"Sexually explicit image.\"  A lewd or lascivious visual depiction or artificially generated depiction of a minor's genitals, pubic area, breast or buttocks or nudity, if such nudity is depicted for the purpose of sexual stimulation or gratification of any person who might view such nudity.\n-  \nSection 4.  This act shall take effect in 60 days.", "tags": ["Harms: Detrimental content", "Strategies: Performance requirements", "Incentives: Criminal liability", "Applications: Broadcasting and media production"], "source": "https://www.legis.state.pa.us/cfdocs/billinfo/bill_history.cfm?syear=2023&sind=0&body=H&type=B&bn=1063#:~:text=An%20Act%20amending%20Title%2018,the%20offense%20of%20transmission%20of", "official_name": "An Act amending Title 18 (Crimes and Offenses) of the Pennsylvania Consolidated Statutes, in sexual offenses, providing for the offense of unlawful dissemination of artificially generated depiction; and, in minors, further providing for the offense of sexual abuse of children and for the offense of transmission of sexually explicit images by minor.", "label": "safe"}
{"id": "327_1", "doc_id": "327", "text": "SEC. 2412. Duty of care.\n\n“(a) In general.—A covered entity may not design or employ services or algorithms, or process, collect, store, or transfer personal data, in a manner that causes or is likely to cause any of the following:\n\n“(1) Physical, economic, relational, or reputational injury to a person.\n\n“(2) Psychological injuries that would be highly offensive to a reasonable person.\n\n“(3) Discrimination on the basis of a person’s or class of persons’ actual or perceived race, color, ethnicity, sex (including sexual orientation, gender identity, and sex characteristics), religion, national origin, familial status, biometric information, or disability status.\n\n“(4) Discrimination regarding a decision that produces a legal effect or similarly significant effect concerning a person.", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Performance requirements", "Strategies: Input controls", "Strategies: Input controls: Data circulation", "Strategies: Input controls: Data use", "Applications: Finance and investment", "Applications: Business services and analytics"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/2597", "official_name": "Sec. 2412. Duty of Care, Digital Consumer Protection Commission Act of 2023", "label": "safe"}
{"id": "327_2", "doc_id": "327", "text": "“(b) Definition.—For purposes of subsection (a)(4), the term ‘decision that produces a legal effect or similarly significant effect concerning a person’ includes denial or degradation of consequential services or support, such as financial or lending services, housing, insurance, educational enrollment, criminal justice, employment opportunities, health care services, and access to basic necessities, such as food and water.\n\n“(c) Exceptions.—Subsection (a) must avoid apply to—\n\n“(1) the design or employment of services or algorithms, or the processing, collecting, storing, or transferring of personal data, for the purpose of—\n\n“(A) a covered entity’s self-testing to prevent or mitigate unlawful discrimination;\n\n“(B) diversifying an applicant, participant, or customer pool; or\n\n“(C) providing resources for the prevention of harm, consistent with evidence-based medical information; or\n\n“(2) any private club or group not open to the public, as described in section 201(e) of the Civil Rights Act of 1964 (42 U.S.C. 2000a(e)).", "tags": ["Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Performance requirements", "Strategies: Input controls", "Strategies: Input controls: Data circulation", "Strategies: Input controls: Data use", "Applications: Finance and investment", "Applications: Business services and analytics"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/2597", "official_name": "Sec. 2412. Duty of Care, Digital Consumer Protection Commission Act of 2023", "label": "safe"}
{"id": "328_1", "doc_id": "328", "text": "SEC. 2501. Corporate citizenship and ownership.\n\n“(a) Definition.—In this section, the term ‘foreign adversary’ has the meaning given the term in section 8(c) of the Secure and Trusted Communications Networks Act of 2019 (47 U.S.C. 1607(c)).\n\n“(b) Corporate citizenship.—\n\n“(1) IN GENERAL.—An operator of a dominant platform shall—\n\n“(A) be a citizen of the United States; or\n\n“(B) own a subsidiary corporation—\n\n“(i) that is a citizen of the United States; and\n\n“(ii) the number of directors of which who are noncitizens is less than half of the number of directors necessary to constitute a quorum.\n\n“(2) DIRECTORS.—No director of a subsidiary corporation described in paragraph (1)(B) may be a citizen of a foreign adversary.", "tags": ["Strategies: Input controls", "Strategies: Input controls: Data circulation", "Applications: Broadcasting and media production"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/2597", "official_name": "Sec. 2501. Corporate Citizenship and Ownership, Digital Consumer Protection Commission Act of 2023", "label": "safe"}
{"id": "328_2", "doc_id": "328", "text": "“(c) Ownership.—If more than 10 percent of the owners of an operator of a dominant platform are citizens of a foreign adversary, the operator of the dominant platform shall sequester any back-end data, algorithm, or information about United States users on the dominant platform so that the back-end data, algorithm, or information is inaccessible to any subsidiary, affiliate, director, employee, or agent of the operator of the dominant platform that is based outside of the United States.\n\n“(d) Review by Committee on Foreign Investment in the United States.—\n\n“(1) IN GENERAL.—The Committee on Foreign Investment in the United States shall—\n\n“(A) treat the application of a foreign person (as defined in section 800.224 of title 31, Code of Federal Regulations (or a successor regulation)) for a license under title VI as a covered transaction, as defined in subsection (a) of section 721 of the Defense Production Act of 1950 (50 U.S.C. 4565); and\n\n“(B) review and, as appropriate, investigate the application in accordance with the procedures set forth in such section 721.\n\n“(2) DENIAL OF LICENSE.—If the Committee determines pursuant to paragraph (1) that providing a license under title VI to a foreign person threatens to impair the national security of the United States, the Office of Licensing for Dominant Platforms shall deny the application for the license.\n\n“(3) CONSULTATION WITH DEPARTMENT OF JUSTICE.—The Committee may, in its discretion, consult with the National Security Division of the Department of Justice in making a determination under paragraph (1).", "tags": ["Strategies: Input controls", "Strategies: Input controls: Data circulation", "Applications: Broadcasting and media production"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/2597", "official_name": "Sec. 2501. Corporate Citizenship and Ownership, Digital Consumer Protection Commission Act of 2023", "label": "safe"}
{"id": "329_2", "doc_id": "329", "text": "B. THE STATE AND ANY GOVERNMENTAL  AGENCY,  POLITICAL  SUBDIVISION  OR\n PUBLIC  BENEFIT  CORPORATION  OF  THE  STATE must avoid PURCHASE, OBTAIN,\n PROCURE, ACQUIRE, EMPLOY, USE, DEPLOY, OR  ACCESS  INFORMATION  FROM  AN\n AUTOMATED  DECISION SYSTEM UNLESS IT FIRST ENGAGES A NEUTRAL THIRD PARTY\n TO CONDUCT AN AUTOMATED DECISION SYSTEM IMPACT ASSESSMENT AND  PUBLISHES\n ON ITS PUBLIC WEBSITE THAT AUTOMATED DECISION SYSTEM IMPACT ASSESSMENT:\n   (I)  OF  EXISTING  AUTOMATED  DECISION  SYSTEM  WITHIN ONE YEAR OF THE\n EFFECTIVE DATE OF THIS SUBDIVISION AND EVERY TWO YEARS THEREAFTER.\n   (II) OF NEW AUTOMATED DECISION SYSTEMS PRIOR TO ACQUISITION AND  EVERY\n TWO YEARS THEREAFTER.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Evaluation: Impact assessment", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure", "Strategies: Evaluation: Post-market monitoring"], "source": "https://www.nysenate.gov/legislation/bills/2023/S2277", "official_name": "An Act to amend the general business law, the executive law, the state finance law and the education law, in relation to enacting the \"digital fairness act\" - Sections 7-9 (\"AUTOMATED DECISION SYSTEM IMPACT ASSESSMENTS,\" etc.)", "label": "safe"}
{"id": "329_4", "doc_id": "329", "text": "D.  THE  STATE  PROCUREMENT  COUNCIL  SHALL,  IN CONSULTATION WITH THE\n OFFICE OF INFORMATION TECHNOLOGY SERVICES, THE DIVISION OF HUMAN  RIGHTS\n AND  EXPERTS  AND  REPRESENTATIVES  FROM  THE  COMMUNITIES  THAT WILL BE\n DIRECTLY AFFECTED BY AUTOMATED DECISION SYSTEMS,  PROMULGATE  RULES  AND\n REGULATIONS  TO SET THE MINIMUM STANDARD ENTITIES SHALL MEET TO SERVE AS\n NEUTRAL  THIRD  PARTIES  CONDUCTING  AUTOMATED  DECISION  SYSTEM  IMPACT\n ASSESSMENTS.\n   E.  THE  STATE PROCUREMENT COUNCIL SHALL MAINTAIN A PUBLICLY AVAILABLE\n LIST OF NEUTRAL THIRD PARTIES THAT MEET THE QUALIFICATIONS  OUTLINED  IN\n PARAGRAPH D OF THIS SUBDIVISION.", "tags": ["Strategies: Convening", "Strategies: Evaluation: External auditing"], "source": "https://www.nysenate.gov/legislation/bills/2023/S2277", "official_name": "An Act to amend the general business law, the executive law, the state finance law and the education law, in relation to enacting the \"digital fairness act\" - Sections 7-9 (\"AUTOMATED DECISION SYSTEM IMPACT ASSESSMENTS,\" etc.)", "label": "safe"}
{"id": "329_13", "doc_id": "329", "text": "§ 8. Section 8 of the state finance law is amended  by  adding  a  new\n subdivision 21 to read as follows:\n   21.  NOTWITHSTANDING  ANY  INCONSISTENT  PROVISION  OF LAW, NO PAYMENT\n SHALL BE MADE FOR AN AUTOMATED DECISION SYSTEM, AS  DEFINED  IN  SECTION\n ONE  HUNDRED  SIXTY-FIVE OF THIS CHAPTER, THAT ASSIGNS OR CONTRIBUTES TO\n THE DETERMINATION OF RIGHTS, BENEFITS, OPPORTUNITIES, OR SERVICES FOR AN\n INDIVIDUAL UNLESS THE AUTOMATED DECISION SYSTEM USES  ONLY  OPEN  SOURCE\n SOFTWARE  AND THE ACQUIRING AGENCY HAS COMPLIED WITH THE AUTOMATED DECI-\n SION SYSTEM IMPACT ASSESSMENT AND AUTOMATED DECISION SYSTEM  USE  POLICY\n REQUIREMENTS  IN SECTION ONE HUNDRED SIXTY-FIVE OF THIS CHAPTER. FOR THE\n PURPOSES OF THIS SUBDIVISION, \"OPEN SOURCE SOFTWARE\" SHALL MEAN SOFTWARE\n FOR WHICH THE HUMAN-READABLE SOURCE CODE IS AVAILABLE  FOR  USE,  STUDY,\n MODIFICATION, AND ENHANCEMENT BY THE USERS OF THAT SOFTWARE.", "tags": ["Applications: Government: benefits and welfare", "Strategies: Performance requirements", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation"], "source": "https://www.nysenate.gov/legislation/bills/2023/S2277", "official_name": "An Act to amend the general business law, the executive law, the state finance law and the education law, in relation to enacting the \"digital fairness act\" - Sections 7-9 (\"AUTOMATED DECISION SYSTEM IMPACT ASSESSMENTS,\" etc.)", "label": "safe"}
{"id": "329_14", "doc_id": "329", "text": "§  9. Section 8 of the state finance law is amended by adding four new\n subdivisions 22, 23, 24 and 25 to read as follows:\n   22. NOTWITHSTANDING ANY INCONSISTENT  PROVISION  OF  LAW,  NO  PAYMENT\n SHALL  BE  MADE  FOR AN AUTOMATED DECISION SYSTEM, AS DEFINED IN SECTION\n ONE HUNDRED SIXTY-FIVE OF THIS CHAPTER, THAT ASSIGNS OR  CONTRIBUTES  TO\n THE DETERMINATION OF RIGHTS, BENEFITS, OPPORTUNITIES, OR SERVICES FOR AN\n INDIVIDUAL,  PRIOR  TO THE APPROVAL FROM THE CITY OR COUNTY COUNCIL WITH\n APPROPRIATE JURISDICTION OR THE STATE LEGISLATURE AS REQUIRED IN SECTION\n ONE HUNDRED SIXTY-FIVE OF THIS CHAPTER.\n   23. NOTWITHSTANDING ANY INCONSISTENT  PROVISION  OF  LAW,  NO  PAYMENT\n SHALL  BE  MADE  FOR AN AUTOMATED DECISION SYSTEM, AS DEFINED IN SECTION\n ONE HUNDRED  SIXTY-FIVE  OF  THIS  CHAPTER,  IF  THE  VENDOR'S  CONTRACT\n CONTAINS  NONDISCLOSURE  OR OTHER PROVISIONS THAT restrict OR IMPAIR THE\n STATE AND ANY GOVERNMENTAL AGENCY OR  POLITICAL  SUBDIVISION  OR  PUBLIC\n BENEFIT  CORPORATION  OF THE STATE'S OBLIGATIONS UNDER SUBDIVISIONS NINE\n AND TEN OF SECTION ONE HUNDRED SIXTY-FIVE OF THIS CHAPTER.\n   24. NOTWITHSTANDING ANY INCONSISTENT  PROVISION  OF  LAW,  NO  PAYMENT\n SHALL  BE  MADE  FOR AN AUTOMATED DECISION SYSTEM, AS DEFINED IN SECTION\n ONE HUNDRED SIXTY-FIVE OF THIS CHAPTER, IF THE AUTOMATED DECISION SYSTEM\n DISCRIMINATES AGAINST AN INDIVIDUAL, OR TREATS AN INDIVIDUAL LESS FAVOR-\n ABLY THAN ANOTHER, IN WHOLE OR IN PART, ON THE  BASIS  OF  ONE  OR  MORE\n FACTORS  ENUMERATED  IN  SECTION TWO HUNDRED NINETY-SIX OF THE EXECUTIVE\n LAW.\n   25. NOTWITHSTANDING ANY INCONSISTENT  PROVISION  OF  LAW,  NO  PAYMENT\n SHALL  BE  MADE  FOR AN AUTOMATED DECISION SYSTEM THAT MAKES FINAL DECI-\n SIONS, JUDGMENTS, OR CONCLUSIONS WITHOUT HUMAN INTERVENTION THAT  IMPACT\n THE  CONSTITUTIONAL  OR LEGAL RIGHTS, DUTIES, OR PRIVILEGES OF ANY INDI-\n   VIDUAL IN NEW YORK STATE OR  FOR  ANY  AUTOMATED  DECISION  SYSTEM  THAT\n DEPLOYS OR TRIGGERS ANY WEAPON.", "tags": ["Applications: Government: benefits and welfare", "Risk factors: Bias", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Safety", "Harms: Harm to health/safety", "Strategies: Performance requirements"], "source": "https://www.nysenate.gov/legislation/bills/2023/S2277", "official_name": "An Act to amend the general business law, the executive law, the state finance law and the education law, in relation to enacting the \"digital fairness act\" - Sections 7-9 (\"AUTOMATED DECISION SYSTEM IMPACT ASSESSMENTS,\" etc.)", "label": "safe"}
{"id": "36_2", "doc_id": "36", "text": "Article 3: The state internet information department is responsible for the overall planning and coordination of the nation's governance of deep synthesis services and related oversight and management efforts. The State Council Departments for telecommunications and public security are responsible for efforts on the oversight and management of deep synthesis services in accordance with their respective duties.\n\nLocal internet information departments are responsible for the overall planning and coordination of the governance of deep synthesis services and related oversight and management efforts within the corresponding administrative region. Local departments for telecommunications and public security are responsible for efforts on the oversight and management of deep synthesis services in the corresponding administrative region, in accordance with their respective duties.", "tags": ["Applications: Government: judicial and law enforcement", "Applications: Networking and telecommunications"], "source": "https://www.chinalawtranslate.com/en/deep-synthesis/", "official_name": "Provisions on the Administration of Deep Synthesis Internet Information Services", "label": "safe"}
{"id": "36_8", "doc_id": "36", "text": "Chapter III: Data and Technical Management Specifications\n\nArticle 14: Deep synthesis service providers and technical supporters shall strengthen the management of training data, employ necessary measures to ensure the security of training data, and where training data includes personal information, they shall comply with relevant provisions on the protection of personal information.\n\nWhere deep synthesis service providers and technical supports provide functions for editing biometric information such as faces and voices, they shall prompt the users of the deep synthesis service to notify the individuals whose personal information is being edited and obtain their independent consent in accordance with law.", "tags": ["Risk factors: Security", "Risk factors: Privacy", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Performance requirements", "Applications: Security"], "source": "https://www.chinalawtranslate.com/en/deep-synthesis/", "official_name": "Provisions on the Administration of Deep Synthesis Internet Information Services", "label": "safe"}
{"id": "36_9", "doc_id": "36", "text": "Article 15: Deep synthesis service providers and technical supporters shall strengthen technical management, periodically reviewing, assessing, and verifying algorithmic mechanisms that produce synthesis.\n\nWhere deep synthesis service providers and technical supporters provide tools that have the following functional models or frameworks, they shall carry out security assessments either on their own or by entrusting a professional body:\n\n(1) Generating or editing biometric information such as faces or voices;\n\n(2) Generating or editing special items, scenarios, or other non-biometric information that might involve national security, the nation's image, national interests, and the societal public interest.\n\nArticle 16: Deep synthesis service providers shall employ technical measures to attach symbols to information content produced or edited by their services' users that do not impact users' usage, and store log information in accordance with laws, administrative regulations, and relevant state provisions.", "tags": ["Risk factors: Security", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Applications: Security", "Strategies: Disclosure: In deployment"], "source": "https://www.chinalawtranslate.com/en/deep-synthesis/", "official_name": "Provisions on the Administration of Deep Synthesis Internet Information Services", "label": "safe"}
{"id": "36_12", "doc_id": "36", "text": "Article 20: Where deep synthesis service providers put new products, usages, or functions online that have public opinion properties or capacity for social mobilization shall follow the relevant state provisions to carry out security assessments.\n\nArticle 21: Internet information departments and departments for telecommunications and public security are to carry out oversight inspections of deep synthesis services in accordance with their duties. Deep synthesis service providers and technical supporters shall lawfully cooperate and provide necessary technological, data, and other support and assistance.\n\nWhere internet information departments and the relevant competent departments discover that deep synthesis services have larger information security risks, they may, in accordance with the law and their duties, request that the deep synthesis service provider and technical supporters employ measures such as suspending information updates, user registration, or other related services. Deep synthesis service providers and technical supporters shall take measures as required to carry out rectification and eliminate threats.\n\nArticle 22: Where Deep synthesis service providers and technical supporters violate these provisions, they are to be punished in accordance with relevant laws and administrative regulations; and where serious consequences were caused, give heavier consequences in accordance with law.\n\nWhere violations of public security are constituted, the public security organs are to give public security administrative sanctions in accordance with law; where a crime is constituted, criminal responsibility is pursued in accordance with law.", "tags": ["Risk factors: Security", "Strategies: Evaluation", "Incentives: Criminal liability", "Applications: Government: judicial and law enforcement", "Incentives: Civil liability"], "source": "https://www.chinalawtranslate.com/en/deep-synthesis/", "official_name": "Provisions on the Administration of Deep Synthesis Internet Information Services", "label": "safe"}
{"id": "395_14", "doc_id": "395", "text": "SEC. 208. ENFORCEMENT.\n(a) In General.—Upon discovering noncompliance with a provision of this Act by a deployer of a high-impact artificial intelligence system or a critical-impact AI organization if the Secretary determines that actions taken by the critical-impact AI organization are insufficient to remedy the noncompliance, the Secretary shall take an action described in this section.\n(b) Civil consequences.—\n(1) IN GENERAL.—The Secretary may impose a consequence described in paragraph (2) on deployer of a high-impact artificial intelligence system or a critical-impact AI organization for each infraction by that entity of this Act or any regulation or order issued under this Act.\n(2) consequence DESCRIBED.—The consequence described in this paragraph is the greater of—\n(A) an amount not to exceed $300,000; or\n(B) an amount that is twice the value of the transaction that is the basis of the infraction with respect to which the consequence is imposed.\n(c) infraction With Intent.—\n(1) IN GENERAL.—If the Secretary determines that a deployer of a high-impact artificial intelligence system or a critical-impact AI organization intentionally violates this Act or any regulation or order issued under this Act, the Secretary may restrict the critical-impact AI organization from deploying a critical-impact artificial intelligence system.\n(2) IN ADDITION.—A prohibition imposed under paragraph (1) shall be in addition to any other civil consequences provided under this Act.\n(d) Factors.—The Secretary may by regulation provide standards for establishing levels of civil consequence under this section based upon factors such as the seriousness of the infraction, the culpability of the violator, and such mitigating factors as the violator’s record of cooperation with the Secretary in disclosing the infraction.\n(e) Civil Action.—\n(1) IN GENERAL.—Upon referral by the Secretary, the Attorney General may bring a civil action in a United States district court to—\n(A) enjoin a infraction of section 207; or\n(B) collect a civil consequence upon a finding of noncompliance with this Act.\n(2) VENUE.—A civil action may be brought under paragraph (1) in the judicial district in which the infraction occurred or the defendant is found, resides, or does business.\n(3) PROCESS.—Process in a civil action under paragraph (1) may be served in any judicial district in which the defendant resides or is found.\n(f) Rule Of Construction.—Nothing in this section shall be construed to require a developer of a critical-impact artificial intelligence system to disclose any information, including data or algorithms—\n(1) relating to a trade secret or other protected intellectual property right;\n(2) that is confidential business information; or\n(3) that is privileged.", "tags": ["Incentives: Civil liability", "Incentives: Fines"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/3312", "official_name": "Artificial Intelligence Research, Innovation, and Accountability Act of 2023", "label": "safe"}
{"id": "396_7", "doc_id": "396", "text": "(d) Accountability Measure Defined.--In this section, the term \n``accountability measure'' means a mechanism, including an review, an \nassessment, or a certification, designed to provide assurance that a \nsystem is trustworthy.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Licensing, registration, and certification", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Risk factors: Reliability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/3369", "official_name": "Artificial Intelligence Accountability Act", "label": "safe"}
{"id": "416_5", "doc_id": "416", "text": "(3) Testing standards shall be issued as a rulemaking under section 553 of title 5, United States Code.\n\n(4) The Director shall consult with outside experts in forensic science, bioethics, algorithmic discrimination, data privacy, racial justice, criminal justice reform, exonerations, and other relevant areas of expertise identified through public input.\n\n(b) Protection of trade secrets.—\n\n(1) There shall be no trade secret evidentiary privilege to withhold relevant evidence in criminal proceedings in the United States courts.\n\n(2) Nothing in this section may be construed to alter the standard operation of the Federal Rules of Criminal Procedure, or the Federal Rules of Evidence, as such rules would function in the absence of an evidentiary privilege.", "tags": ["Risk factors: Bias", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Convening", "Applications: Government: judicial and law enforcement"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7394", "official_name": "Justice in Forensic Algorithms Act of 2024", "label": "safe"}
{"id": "416_6", "doc_id": "416", "text": "(c) Requirements for federal use of forensic algorithms.—Any Federal law enforcement agency or crime laboratory providing services to a Federal law enforcement agency using computational forensic software may use only software that has been tested under the National Institute of Standards and Technology’s Computational Forensic Algorithm Testing Program and shall conduct an internal validation according to the requirements outlined in the Computational Forensic Algorithm Testing Standards and make the results publicly available. The internal validation shall be updated when there is a material change in the software that triggers a retesting by the Computational Forensic Algorithm Testing Program.\n\n(d) Testing program.—The Director of the National Institute of Standards and Technology shall establish a Computational Forensic Algorithm Testing Program, whose activities include the following:\n\n(1) Testing individual software programs using the testing requirements established in the Computational Forensic Algorithm Testing Standards.\n\n(2) Using realistic sample testing data similar to what would be used by law enforcement in criminal investigations in performing such testing, including incomplete and contaminated samples.\n\n(3) Using testing data that represents diversity of racial, ethnic, and gender identities and intersections of these identities in performing such testing.\n\n(4) Using testing data that tests the limits of the software and demonstrates the boundaries of reliability described in the performance measures defined in the Computational Forensic Algorithm Testing Standards in performing such testing.\n\n(5) Publishing the results of testing the software online including results under conditions specified in the testing standards and across diversity of racial, ethnic, and gender identities and intersections of these identities in a publicly available format.", "tags": ["Applications: Government: judicial and law enforcement", "Risk factors: Bias", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: New institution"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7394", "official_name": "Justice in Forensic Algorithms Act of 2024", "label": "safe"}
{"id": "416_8", "doc_id": "416", "text": "(g) Inadmissibility of certain evidence.—In any criminal case, evidence that is the result of analysis by computational forensic software is admissible only if—\n\n(1) the computational forensic software used has been submitted to the Computational Forensic Algorithm Testing Program of the Director of the National Institute of Standards and Technology and there have been no material changes to that software since it was last tested; and\n\n(2) the developers and users of the computational forensic software agree to waive any and all legal claims against the defense or any member of its team for the purposes of the defense analyzing or testing the computational forensic software.", "tags": ["Applications: Government: judicial and law enforcement", "Risk factors: Transparency", "Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7394", "official_name": "Justice in Forensic Algorithms Act of 2024", "label": "safe"}
{"id": "416_9", "doc_id": "416", "text": "(h) Definitions.—In this Act:\n\n(1) COMPUTATIONAL FORENSIC SOFTWARE.—The term “computational forensic software” means software that relies on an automated or semiautomated computational process, including one derived from machine learning, statistics, or other data processing or artificial intelligence techniques, to process, analyze, or interpret evidence.\n\n(2) MATERIAL CHANGE.—The term “material change” means an update to computational forensic software that may affect the performance measures defined in the Computational Forensic Algorithm Testing Standards or the use or output of the software.\n\n(3) NONMATERIAL CHANGE.—The term “nonmaterial change” means an update to computational forensic software that does not affect the performance measures, use, or output of the software.", "tags": ["Applications: Government: judicial and law enforcement"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7394", "official_name": "Justice in Forensic Algorithms Act of 2024", "label": "safe"}
{"id": "439_5", "doc_id": "439", "text": "(c) Unauthorized Simulation Of Voice Or Likeness.—\n\n\n(1) IN GENERAL.—Any person or entity who, in a manner affecting interstate or foreign commerce (or using any means or facility of interstate or foreign commerce), and without consent of the individual holding the voice or likeness rights affected thereby—\n\n\n(A) distributes, transmits, or otherwise makes available to the public a personalized cloning service;\n\n\n(B) publishes, performs, distributes, transmits, or otherwise makes available to the public a digital voice replica or digital depiction with knowledge that the digital voice replica or digital depiction was not authorized by the individual holding the voice or likeness rights affected thereby; or\n\n\n(C) materially contributes to, directs, or otherwise facilitates any of the conduct proscribed in subparagraph (A) or (B) with knowledge that the individual holding the affected voice or likeness rights has not consented to the conduct,\n\n\nshall be responsible for damages as set forth in paragraph (2).\n\n\n(2) REMEDIES.—In any action brought under this section, the following shall apply:\n\n\n(A) The person or entity who violated the section shall be responsible to the injured party or parties in an amount equal to the greater of—\n\n\n(i) in the case of an unauthorized distribution, transmission, or other making available of a personalized cloning service, fifty thousand dollars ($50,000) per infraction or the actual damages suffered by the injured party or parties as a result of the unauthorized use, plus any profits from the unauthorized use that are attributable to such use and are not taken into account in computing the actual damages; and\n\n\n(ii) in the case of an unauthorized publication, performance, distribution, transmission, or other making available of a digital voice replica or digital depiction, five thousand dollars ($5,000) per infraction or the actual damages suffered by the injured party or parties as a result of the unauthorized use, plus any profits from the unauthorized use that are attributable to such use and are not taken into account in computing the actual damages.\n\n\n(B) In establishing profits under this subdivision, the injured party or parties shall be required only to present proof of the gross revenue attributable to the unauthorized use, and the person or entity who violated this section shall be required to prove his or her expenses deductible therefrom.\n\n\n(C) Punitive damages and reasonable attorneys’ fees may also be awarded to the injured party or parties.\n\n\n(D) It must avoid be a defense to an allegation of a infraction of paragraph (1) that the unauthorized user displayed or otherwise communicated to the public a disclaimer stating that the digital depiction, digital voice replica, or personalized cloning service was unauthorized or that the individual rights owner did not participate in the creation, development, distribution, or dissemination of the unauthorized digital depiction, digital voice replica, or personalized cloning service.\n\n\n(E) An action to enforce this section may be brought by—\n\n\n(i) the individual whose voice or likeness is at issue;\n\n\n(ii) any other person or entity to which the individual has assigned or exclusively licensed their voice or likeness rights; or\n\n\n(iii) in the case of an individual who performs music as a profession, and has not authorized the use at issue, by any person or entity that has entered into a contract for the individual’s exclusive personal services as a recording artist or an exclusive license to distribute sound recordings that capture the individual’s audio performances.", "tags": ["Incentives: Fines"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/6943", "official_name": "No AI FRAUD Act", "label": "safe"}
{"id": "439_6", "doc_id": "439", "text": "(d) First Amendment Defense.—First Amendment protections shall constitute a defense to an alleged infraction of subsection (c). In evaluating any such defense, the public interest in access to the use shall be balanced against the intellectual property interest in the voice or likeness. Factors to be considered may include whether—\n\n\n(1) the use is commercial;\n\n\n(2) the individual whose voice or likeness is at issue is necessary for and relevant to the primary expressive purpose of the work in which the use appears; and\n\n\n(3) the use competes with or otherwise adversely affects the value of the work of the owner or licensee of the voice or likeness rights at issue.", "tags": ["Risk factors: Safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Risk factors: Privacy", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Harms: Harm to property", "Risk factors: Safety", "Risk factors: Privacy", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Harms: Harm to property", "Incentives: Fines", "Harms: Harm to health/safety", "Harms: Detrimental content"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/6943", "official_name": "No AI FRAUD Act", "label": "safe"}
{"id": "441_1", "doc_id": "441", "text": "H. R. 5495\n\n\nTo restrict providers of email services from using filtering algorithms to flag emails from political campaigns that consumers have elected to receive as spam.\n\n\nIN THE HOUSE OF REPRESENTATIVES\n\n\nSeptember 14, 2023\n\n\nMrs. Lesko (for herself, Ms. Van Duyne, Mr. Steube, Mr. Pfluger, Mr. Duncan, Mr. Higgins of Louisiana, Mr. Issa, and Ms. Stefanik) introduced the following bill; which was referred to the Committee on Energy and Commerce\n\n\nA BILL\nTo restrict providers of email services from using filtering algorithms to flag emails from political campaigns that consumers have elected to receive as spam.\n\n\nBe it enacted by the Senate and House of Representatives of the United States of America in Congress assembled,", "tags": ["Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: About incidents", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: In deployment", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About incidents", "Strategies: Disclosure: In deployment", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/5495", "official_name": "Political BIAS Emails Act of 2023", "label": "safe"}
{"id": "441_4", "doc_id": "441", "text": "(d) Enforcement By The Federal Trade Commission.—\n\n\n(1) UNFAIR OR DECEPTIVE ACTS OR PRACTICES.—A infraction of subsection (a), (b), or (c) shall be treated as a infraction of a rule defining an unfair or a deceptive act or practice under section 18(a)(1)(B) of the Federal Trade Commission Act (15 U.S.C. 57a(a)(1)(B)).\n\n\n(2) POWERS OF COMMISSION.—\n\n\n(A) IN GENERAL.—The Federal Trade Commission shall enforce this section in the same manner, by the same means, and with the same jurisdiction, powers, and duties as though all applicable terms and provisions of the Federal Trade Commission Act (15 U.S.C. 41 et seq.) were incorporated into and made a part of this section.\n\n\n(B) PRIVILEGES AND IMMUNITIES.—Any person who violates subsection (a), (b), or (c) shall be subject to the consequences and entitled to the privileges and immunities provided in the Federal Trade Commission Act (15 U.S.C. 41 et seq.).\n\n\n(C) AUTHORITY PRESERVED.—Nothing in this section shall be construed to limit the authority of the Federal Trade Commission under any other provision of law.", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/5495", "official_name": "Political BIAS Emails Act of 2023", "label": "safe"}
{"id": "441_5", "doc_id": "441", "text": "SEC. 3. DEFINITIONS.\n\n\nIn this Act:\n\n\n(1) FILTERING ALGORITHM.—The term “filtering algorithm” means a computational process, including one derived from algorithmic decision making, machine learning, statistical analysis, or other data processing or artificial intelligence techniques, used by an email service to identify and filter emails sent to an email account.\n\n\n(2) OPERATOR.—\n\n\n(A) IN GENERAL.—The term “operator” means any person who operates an email service and includes any person that wholly owns a subsidiary entity that operates an email service.\n\n\n(B) EXCLUSIONS.—Such term must avoid include any person who operates an email service if such service is wholly owned, controlled, and operated by a person that—\n\n\n(i) for the most recent 6-month period, did not employ more than 500 employees; and\n\n\n(ii) for the most recent 12-month period, averaged less than $5,000,000,000 in annual gross receipts.\n\n\n(3) POLITICAL CAMPAIGN.—The term “political campaign” includes—\n\n\n(A) an individual who is a candidate (as such term is defined in section 301(2) of the Federal Election Campaign Act of 1971 (52 U.S.C. 30101(2)));\n\n\n(B) an authorized committee (as such term is defined in section 301(6) of such Act);\n\n\n(C) a connected organization (as such term is defined in section 301(7) of such Act);\n\n\n(D) a national committee (as such term is defined in section 301(14) of such Act);\n\n\n(E) a State committee (as such term is defined in section 301(15) of such Act); and\n\n\n(F) a joint fundraising committee that includes any entity described in subparagraphs (A) through (E).", "tags": ["Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: About incidents", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: In deployment", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About incidents", "Strategies: Disclosure: In deployment", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/5495", "official_name": "Political BIAS Emails Act of 2023", "label": "safe"}
{"id": "444_19", "doc_id": "444", "text": "“(b) Privacy, ethics, civil rights and civil liberties, safety, and trustworthiness.—\n\n“(1) IN GENERAL.—\n\n“(A) REQUIREMENTS.—The head of the Program Management Office, acting through the Director of the Operating Entity and in consultation with any relevant Advisory Committee, shall establish requirements, a review process for applications, and a process for auditing resources of the NAIRR and research conducted using resources of the NAIRR on matters related to privacy, ethics, civil rights and civil liberties, safety, security, and trustworthiness of artificial intelligence systems developed using resources of the NAIRR.\n\n“(B) FEDERAL STATISTICAL DATA.—Any auditing process required under subparagraph (A) for Federal statistical data included in a resource of the NAIRR shall be completed by the head of a designated statistical agency (as defined in section 3576(e) of title 44, United States Code), in coordination with the Chief Statistician of the United States, consistent with relevant law.\n\n“(2) CONSISTENCY.—The head of the Program Management Office shall ensure the requirements and processes described in paragraph (1) are consistent with the policies of the Office of Management and Budget policy and relevant policies of other Executive agencies. The head of the Program Management Office shall coordinate with the Senior Agency Official for Privacy and the General Counsel of the National Science Foundation in ensuring adherence with applicable privacy law and policy and Federal laws and regulations.", "tags": ["Risk factors: Privacy", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Reliability", "Strategies: Evaluation", "Strategies: Evaluation: External auditing"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/2714", "official_name": "CREATE AI Act of 2023", "label": "safe"}
{"id": "444_21", "doc_id": "444", "text": "“(c) Scientific integrity.—\n\n“(1) IN GENERAL.—The head of the Program Management Office, acting through the Director of the Operating Entity and in consultation with any relevant Advisory Committee, shall develop guidance for—\n\n“(A) addressing concerns related to matters of scientific integrity, including matters related to the effects or impacts of research and potential research enabled by the NAIRR; and\n\n“(B) mechanisms for an employee of the Operating Entity, an employee of the Program Management Office, a member of the NAIRR Steering Subcommittee or an Advisory Committee, a researcher or student affiliated with a NAIRR user described in subsection (a)(1), an employee of a provider of a resource of the NAIRR, an employee of a NAIRR financial support agency, or a member of the public to report violations of the guidance developed under this paragraph, including by confidential and anonymous means.\n\n“(2) CONSISTENCY WITH GOVERNMENT POLICIES ON SCIENTIFIC INTEGRITY.—The guidance developed under paragraph (1)(A) shall be published in a publicly accessible location on the website of the NAIRR. Such policies shall, to the degree practicable, be consistent with—\n\n“(A) the Presidential memorandum entitled ‘Restoring Trust in Government Through Scientific Integrity and Evidence-Based Policymaking’, dated January 27, 2021, or successor document; and\n\n“(B) reports produced pursuant to such Presidential memorandum (including the reports entitled ‘Protecting the Integrity of Government Science’, dated January 2022, and ‘A Framework for Federal Scientific Integrity Policy and Practice’, dated January 2023, published by the National Science and Technology Council, or successor documents).", "tags": ["Risk factors: Privacy", "Risk factors: Reliability", "Strategies: Disclosure", "Strategies: Disclosure: About incidents"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/2714", "official_name": "CREATE AI Act of 2023", "label": "safe"}
{"id": "451_2", "doc_id": "451", "text": "H. R. 7120\n\n\nTo direct the Federal Trade Commission to revise the Telemarketing Sales Rule to require disclosures for telemarketing using artificial intelligence and to provide for enhanced consequences for violations involving artificial intelligence voice or text message impersonation, and for other purposes.\n\n\nIN THE HOUSE OF REPRESENTATIVES\n\n\nJanuary 29, 2024\n\n\nMs. Schakowsky introduced the following bill; which was referred to the Committee on Energy and Commerce\n\n\nA BILL\nTo direct the Federal Trade Commission to revise the Telemarketing Sales Rule to require disclosures for telemarketing using artificial intelligence and to provide for enhanced consequences for violations involving artificial intelligence voice or text message impersonation, and for other purposes.\n\n\nBe it enacted by the Senate and House of Representatives of the United States of America in Congress assembled,", "tags": ["Risk factors: Transparency", "Harms: Financial loss", "Harms: Detrimental content", "Applications: Networking and telecommunications", "Incentives: Fines", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7120", "official_name": "R U REAL Act", "label": "safe"}
{"id": "456_5", "doc_id": "456", "text": "(b) Criminal consequences.—Section 309(d)(1) of the Federal Election Campaign Act of 1971 (52 U.S.C. 30109(d)(1)) is amended by adding at the end the following new subparagraph:\n\n\n“(E) Any person who knowingly and willfully commits a infraction of section 325 shall be fined under title 18, United States Code, or imprisoned for not more than 2 years, or both.”.", "tags": ["Incentives: Criminal liability", "Incentives: Fines", "Incentives: Imprisonment"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/4611", "official_name": "Candidate Voice Fraud Prohibition Act", "label": "safe"}
{"id": "481_1", "doc_id": "481", "text": "S. 1291\n\n\nTo require that social media platforms verify the age of their users, restrict the use of algorithmic recommendation systems on individuals under age 18, require parental or guardian consent for social media users under age 18, and restrict users who are under age 13 from accessing social media platforms.\n\n\nIN THE SENATE OF THE UNITED STATES\n\n\nApril 26, 2023\n\n\nMr. Schatz (for himself, Mr. Cotton, Mr. Murphy, and Mrs. Britt) introduced the following bill; which was read twice and referred to the Committee on Commerce, Science, and Transportation\n\n\nA BILL\nTo require that social media platforms verify the age of their users, restrict the use of algorithmic recommendation systems on individuals under age 18, require parental or guardian consent for social media users under age 18, and restrict users who are under age 13 from accessing social media platforms.\n\n\nBe it enacted by the Senate and House of Representatives of the United States of America in Congress assembled,", "tags": ["Applications: Arts, sports, leisure, travel, and lifestyle", "Incentives: Civil liability", "Harms: Violation of civil or human rights, including privacy", "Applications: Arts, sports, leisure, travel, and lifestyle", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/1291", "official_name": "Protecting Kids on Social Media Act", "label": "safe"}
{"id": "481_3", "doc_id": "481", "text": "SEC. 3. REASONABLE STEPS FOR AGE VERIFICATION.\n\n\n(a) In General.—A social media platform shall take reasonable steps beyond merely requiring attestation, taking into account existing age verification technologies, to verify the age of individuals who are account holders on the platform.\n\n\n(b) limitation On Use And Retention Of Information.—A social media platform must avoid—\n\n\n(1) use any information collected as part of the platform's age verification process for any other purpose; or\n\n\n(2) retain any information collected from a user as part of the age verification process except to the extent necessary to prove that the platform has taken reasonable steps to verify the age of the user.\n\n\n(c) Rule Of Construction.—Nothing in this section shall be construed to require a social media platform to require users to provide government-issued identification for age verification.\n\n\n(d) Existing Accounts.—A social media platform must avoid be required to verify the age of account holders on the platform for any account that, as of the date of enactment of this Act, has existed for 90 days or more, until 2 years after the date of enactment of this Act.\n\n\n(e) Unverified Accounts.—A social media platform must avoid permit an individual to create a user account (or continue to use an existing user account after the date that is 2 years after the date of enactment of this Act) if the individual's age has not been verified.\n\n\n(f) Safe Harbor.—A social media platform that, for age verification purposes, relies in good faith on information provided by the Pilot Program described in section 7 to verify the age of a user shall be deemed to have taken reasonable steps to verify the age of that user on the platform.", "tags": ["Applications: Arts, sports, leisure, travel, and lifestyle", "Incentives: Civil liability", "Harms: Violation of civil or human rights, including privacy", "Applications: Arts, sports, leisure, travel, and lifestyle", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/1291", "official_name": "Protecting Kids on Social Media Act", "label": "safe"}
{"id": "481_4", "doc_id": "481", "text": "SEC. 4. NO CHILDREN UNDER 13.\n\n\nA social media platform must avoid permit an individual to use the platform (other than merely viewing content, as long as such viewing does not involve logging in or interacting with the content or other users) unless the individual is known or reasonably believed to be age 13 or older according to the age verification process used by the platform.", "tags": ["Applications: Arts, sports, leisure, travel, and lifestyle", "Incentives: Civil liability", "Harms: Violation of civil or human rights, including privacy", "Applications: Arts, sports, leisure, travel, and lifestyle", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/1291", "official_name": "Protecting Kids on Social Media Act", "label": "safe"}
{"id": "481_5", "doc_id": "481", "text": "SEC. 5. PARENT OR GUARDIAN CONSENT FOR MINORS.\n\n\n(a) In General.—A social media platform shall take reasonable steps beyond merely requiring attestation, taking into account current parent or guardian relationship verification technologies and documentation, to require the affirmative consent of a parent or guardian to create an account for any individual who the social media platform knows or reasonably believes to be a minor according to the age verification process used by the platform.\n\n\n(b) limitation On Use And Retention Of Information.—A social media platform must avoid—\n\n\n(1) use any information collected as part of the parent or guardian consent process for any other purpose; or\n\n\n(2) retain any information collected as part of the parent or guardian verification process except to the extent necessary to—\n\n\n(A) provide confirmation of the affirmative consent of a parent or guardian for a minor user to create an account;\n\n\n(B) preserve the ability of the parent or guardian to revoke such consent; and\n\n\n(C) prove that the platform has taken reasonable steps to obtain the affirmative consent of a parent or guardian for a minor user to create an account.\n\n\n(c) Ability To Revoke Consent.—A social media platform shall take reasonable steps to provide a parent or guardian who has consented to their child’s social media use with the ability to revoke such consent.\n\n\n(d) Effect Of Revocation Of Consent.—A social media platform that receives a revocation of consent under subsection (c) shall suspend, delete, or otherwise disable the account of the minor user for whom consent was revoked.\n\n\n(e) Rule Of Construction.—Nothing in this section shall be construed to require a social media platform to require minor users or their parents or guardians to provide government-issued identification for relationship verification or the provision of affirmative consent to create an account.\n\n\n(f) Safe Harbor.—A social media platform that, for parent or guardian relationship verification purposes, relies in good faith on information provided by the Pilot Program described in section 7 shall be deemed to have taken reasonable steps to verify the parent or guardian relationship of the parent or guardian granting consent for a minor user to create an account under this section.", "tags": ["Applications: Arts, sports, leisure, travel, and lifestyle", "Incentives: Civil liability", "Harms: Violation of civil or human rights, including privacy", "Applications: Arts, sports, leisure, travel, and lifestyle", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/1291", "official_name": "Protecting Kids on Social Media Act", "label": "safe"}
{"id": "481_8", "doc_id": "481", "text": "SEC. 8. ENFORCEMENT.\n\n\n(a) Enforcement By Commission.—\n\n\n(1) UNFAIR OR DECEPTIVE ACTS OR PRACTICES.—A infraction of this Act by a social media platform shall be treated as a infraction of a rule defining an unfair or deceptive act or practice prescribed under section 18(a)(1)(B) of the Federal Trade Commission Act (15 U.S.C. 57a(a)(1)(B)).\n\n\n(2) POWERS OF COMMISSION.—\n\n\n(A) IN GENERAL.—Except as provided in subparagraph (C), the Commission shall enforce this Act in the same manner, by the same means, and with the same jurisdiction, powers, and duties as though all applicable terms and provisions of the Federal Trade Commission Act (15 U.S.C. 41 et seq.) were incorporated into and made a part of this Act.\n\n\n(B) PRIVILEGES AND IMMUNITIES.—Except as provided in subparagraph (C), any person who violates this Act shall be subject to the consequences and entitled to the privileges and immunities provided in the Federal Trade Commission Act (15 U.S.C. 41 et seq.).\n\n\n(C) NONPROFIT ORGANIZATIONS AND COMMON CARRIERS.—Notwithstanding section 4 or 5(a)(2) of the Federal Trade Commission Act (15 U.S.C. 44, 45(a)(2)) or any jurisdictional limitation of the Commission, the Commission shall also enforce this Act, in the same manner provided in subparagraphs (A) and (B) of this paragraph, with respect to—\n\n\n(i) organizations not organized to carry on business for their own profit or that of their members; and\n\n\n(ii) common carriers subject to the Communications Act of 1934 (47 U.S.C. 151 et seq.).", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/1291", "official_name": "Protecting Kids on Social Media Act", "label": "safe"}
{"id": "485_11", "doc_id": "485", "text": "SEC. 10. ENFORCEMENT.\n\n\n(a) Investigations.—\n\n\n(1) IN GENERAL.—The President shall rely on, including by delegation, the Secretary, and the heads of other Federal agencies, as appropriate, to conduct investigations of violations of any authorization, order, mitigation measure, regulation, or prohibition issued under this Act.\n\n\n(2) ACTIONS BY DESIGNEES.—In conducting investigations described in paragraph (1), designated officers or employees of Federal agencies described that paragraph may, to the extent necessary or appropriate to enforce this Act, exercise such authority as is conferred upon them by any other Federal law, subject to policies and procedures approved by the Attorney General.\n\n\n(b) Permitted Activities.—Officers and employees of agencies authorized to conduct investigations under subsection (a) may—\n\n\n(1) inspect, search, detain, seize, or impose temporary denial orders with respect to items, in any form, or conveyances on which it is believed that there are items that have been, are being, or are about to be imported into the United States in infraction of this Act or any other applicable Federal law;\n\n\n(2) require, inspect, and obtain books, records, and any other information from any person subject to the provisions of this Act or other applicable Federal law;\n\n\n(3) administer oaths or affirmations and, by subpoena, require any person to appear and testify or to appear and produce books, records, and other writings, or both; and\n\n\n(4) obtain court orders and issue legal process to the extent authorized under chapters 119, 121, and 206 of title 18, United States Code, or any other applicable Federal law.\n\n\n(c) Enforcement Of Subpoenas.—In the case of contumacy by, or refusal to obey a subpoena issued to, any person under subsection (b)(3), a district court of the United States, after notice to such person and a hearing, shall have jurisdiction to issue an order requiring such person to appear and give testimony or to appear and produce books, records, and other writings, regardless of format, that are the subject of the subpoena. Any failure to obey such order of the court may be punished by such court as a contempt thereof.\n\n\n(d) Actions By The Attorney General.—The Attorney General may bring an action in an appropriate district court of the United States for appropriate relief, including declaratory and injunctive, or divestment relief, against any person who violates this Act or any regulation, order, direction, mitigation measure, prohibition, or other authorization or directive issued under this Act. In any such action, the limitations as described under section 12(b) shall apply.", "tags": ["Risk factors: Security", "Risk factors: Privacy", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/686", "official_name": "RESTRICT Act", "label": "safe"}
{"id": "522_15", "doc_id": "522", "text": "“SEC. 1007. consequences AND ENFORCEMENT.\n\n\n“(a) consequences.—\n\n\n“(1) UNLAWFUL ACTS.—Subject to regulations prescribed in accordance with section 1008, it shall be unlawful—\n\n\n“(A) to engage in a covered activity restricted under this title or pursuant to such regulations;\n\n\n“(B) to fail to submit a notification under subsection (a) of section 1003 with respect to a covered activity or to submit other information as required by the Committee;\n\n\n“(C) to make a material misstatement or to omit a material fact in any information submitted to the Committee under this title;\n\n\n“(D) to breach or violate a mitigation agreement entered into or condition imposed under subsection (c) of that section; or\n\n\n“(E) to engage in any activity that evades or avoids, has the purpose of evading or avoiding, causes a infraction of, or attempts to violate a prohibition under this title or pursuant to such regulations.\n\n\n“(2) CIVIL consequences.—A civil consequence may be imposed on any person who commits an unlawful act described in paragraph (1) in an amount not to exceed the greater of—\n\n\n“(A) $250,000; or\n\n\n“(B) an amount that is twice the amount of the covered activity that is the basis of the infraction with respect to which the consequence is imposed.\n\n\n“(b) Enforcement.—The President may direct the Attorney General to seek appropriate relief, including divestment relief, in the district courts of the United States, in order to implement and enforce this title.", "tags": ["Incentives: Civil liability", "Incentives: Fines"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/3136", "official_name": "National Critical Capabilities Defense Act of 2023", "label": "safe"}
{"id": "531_2", "doc_id": "531", "text": "SEC. 2. CIVIL ACTION RELATING TO DISCLOSURE OF INTIMATE IMAGES.\n(a) Definitions.—Section 1309(a) of the Consolidated Appropriations Act, 2022 (15 U.S.C. 6851(a)) is amended—\n(1) in paragraph (2), by inserting “competent,” after “conscious,”;\n(2) by redesignating paragraphs (5) and (6) as paragraphs (6) and (7), respectively;\n(3) by redesignating paragraph (3) as paragraph (5);\n(4) by inserting after paragraph (2) the following:", "tags": ["Harms: Detrimental content", "Incentives: Civil liability", "Incentives: Civil liability", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Incentives: Civil liability", "Harms: Discrimination"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/3696", "official_name": "Disrupt Explicit Forged Images And Non-Consensual Edits Act of 2024", "label": "safe"}
{"id": "531_4", "doc_id": "531", "text": "(b) Civil Action.—Section 1309(b) of the Consolidated Appropriations Act, 2022 (15 U.S.C. 6851(b)) is amended—\n(1) in paragraph (1)—\nby striking paragraph (A) and inserting the following:\n“(A) IN GENERAL.—Except as provided in paragraph (5)—\n“(i) an identifiable individual whose intimate visual depiction is disclosed, in or affecting interstate or foreign commerce or using any means or facility of interstate or foreign commerce, without the consent of the identifiable individual, where such disclosure was made by a person who knows or recklessly disregards that the identifiable individual has not consented to such disclosure, may bring a civil action against that person in an appropriate district court of the United States for relief as set forth in paragraph (3);\n“(ii) an identifiable individual who is the subject of a digital forgery may bring a civil action in an appropriate district court of the United States for relief as set forth in paragraph (3) against any person that knowingly produced or possessed the digital forgery with intent to disclose it, or knowingly disclosed or solicited the digital forgery, if—\n“(I) the identifiable individual did not consent to such production, disclosure, solicitation, or possession;\n“(II) the person knew or recklessly disregarded that the identifiable individual did not consent to such production, disclosure, solicitation, or possession; and\n“(III) such production, disclosure, solicitation, or possession is in or affects interstate or foreign commerce or uses any means or facility of interstate or foreign commerce; and\n“(iii) an identifiable individual who is the subject of a digital forgery may bring a civil action in an appropriate district court of the United States for relief as set forth in paragraph (3) against any person that knowingly produced the digital forgery if—\n“(I) the identifiable individual did not consent to such production;\n“(II) the person knew or recklessly disregarded that the identifiable individual did not consent to such production; and\n“(III) such production is in or affects interstate or foreign commerce or uses any means or facility of interstate or foreign commerce.”; and\n(B) in subparagraph (B)—\n(i) in the heading, by inserting “IDENTIFIABLE” before “INDIVIDUALS”; and\n(ii) by striking “an individual who is under 18 years of age, incompetent, incapacitated, or deceased, the legal guardian of the individual” and inserting “an identifiable individual who is under 18 years of age, incompetent, incapacitated, or deceased, the legal guardian of the identifiable individual”;", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/3696", "official_name": "Disrupt Explicit Forged Images And Non-Consensual Edits Act of 2024", "label": "safe"}
{"id": "531_5", "doc_id": "531", "text": "(2) in paragraph (2)—\n(A) in subparagraph (A)—\n(i) by inserting “identifiable” before “individual”;\n(ii) by striking “depiction” and inserting “intimate visual depiction or digital forgery”; and\n(iii) by striking “distribution” and inserting “disclosure, solicitation, or possession”; and\n(B) in subparagraph (B)—\n(i) by inserting “identifiable” before individual;\n(ii) by inserting “or digital forgery” after each place the term “depiction” appears; and\n(iii) by inserting “, solicitation, or possession” after “disclosure”;\n(3) by redesignating paragraph (4) as paragraph (5);\n(4) by striking paragraph (3) and inserting the following:", "tags": ["Harms: Detrimental content", "Incentives: Civil liability", "Incentives: Civil liability", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Incentives: Civil liability", "Harms: Discrimination"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/3696", "official_name": "Disrupt Explicit Forged Images And Non-Consensual Edits Act of 2024", "label": "safe"}
{"id": "531_6", "doc_id": "531", "text": "“(3) RELIEF.—In a civil action filed under this section—\n“(A) an identifiable individual may recover the actual damages sustained by the individual or liquidated damages in the amount of $150,000, and the cost of the action, including reasonable attorney’s fees and other litigation costs reasonably incurred; and\n“(B) the court may, in addition to any other relief available at law, order equitable relief, including a temporary restraining order, a preliminary injunction, or a permanent injunction ordering the defendant to cease display or disclosure of the intimate visual depiction or digital forgery.", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/3696", "official_name": "Disrupt Explicit Forged Images And Non-Consensual Edits Act of 2024", "label": "safe"}
{"id": "531_9", "doc_id": "531", "text": "“(7) DUPLICATIVE RECOVERY BARRED.—No relief may be ordered under paragraph (3) against a person who is subject to a judgment under section 2255 of title 18, United States Code, for the same conduct involving the same identifiable individual and the same intimate visual depiction or digital forgery.”.", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/3696", "official_name": "Disrupt Explicit Forged Images And Non-Consensual Edits Act of 2024", "label": "safe"}
{"id": "531_11", "doc_id": "531", "text": "SEC. 3. SEVERABILITY.\nIf any provision of this Act, an amendment made by this Act, or the application of such a provision or amendment to any person or circumstance, is held to be unconstitutional, the remaining provisions of and amendments made by this Act, and the application of the provision or amendment held to be unconstitutional to any other person or circumstance, must avoid be affected thereby.", "tags": ["Harms: Detrimental content", "Incentives: Civil liability", "Incentives: Civil liability", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Incentives: Civil liability", "Harms: Discrimination"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/3696", "official_name": "Disrupt Explicit Forged Images And Non-Consensual Edits Act of 2024", "label": "safe"}
{"id": "546_7", "doc_id": "546", "text": "SEC. 5. ENFORCEMENT BY THE COMMISSION.\n(a) Unfair Or Deceptive Acts Or Practices.—A infraction of section 3 or a regulation promulgated under this Act shall be treated as a infraction of a rule defining an unfair or deceptive act or practice under section 18(a)(1)(B) of the Federal Trade Commission Act (15 U.S.C. 57a(a)(1)(B)).\n(b) Powers Of The Commission.—\n(1) IN GENERAL.—The Commission shall enforce section 3 and the regulations promulgated under this Act in the same manner, by the same means, and with the same jurisdiction, powers, and duties as though all applicable terms and provisions of the Federal Trade Commission Act (15 U.S.C. 41 et seq.) were incorporated into and made a part of this Act.\n(2) PRIVILEGES AND IMMUNITIES.—Any person who violates section 3 or a regulation promulgated under this Act shall be subject to the consequences and entitled to the privileges and immunities provided in the Federal Trade Commission Act (15 U.S.C. 41 et seq.).\n(3) AUTHORITY PRESERVED.—Nothing in this Act shall be construed to limit or expand the authority of the Commission under any other provision of law.", "tags": ["Incentives: Criminal liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/2708", "official_name": "Deceptive Experiences To Online Users Reduction Act", "label": "safe"}
{"id": "566_7", "doc_id": "566", "text": "SEC. 104. DISCLOSURE REQUIRED FOR ROBOCALLS USING AI.\n\n\nSection 227 of the Communications Act of 1934 (47 U.S.C. 227) is amended by adding at the end the following:\n\n\n“(k) Disclosure Required For Robocalls Using AI.—If a person making a robocall uses artificial intelligence to emulate a human being, such person shall disclose at the beginning of the call or text message the fact that artificial intelligence is being used.”.", "tags": ["Strategies: Disclosure", "Strategies: Evaluation: Impact assessment", "Risk factors: Interpretability and explainability", "Risk factors: Security: Dissemination", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7116", "official_name": "Do Not Disturb Act", "label": "safe"}
{"id": "566_8", "doc_id": "566", "text": "SEC. 105. ENHANCED consequences FOR VIOLATIONS INVOLVING AI VOICE OR TEXT MESSAGE IMPERSONATION.\n\n\n(a) In General.—Section 227 of the Communications Act of 1934 (47 U.S.C. 227), as amended by the preceding provisions of this Act, is further amended by adding at the end the following:\n\n\n\n\n“(l) Enhanced consequences For Violations Involving AI Voice Or Text Message Impersonation.—In the case of a infraction of this section with respect to which the party making the call or sending the text message uses artificial intelligence to impersonate an individual or entity with the intent to defraud, cause harm, or wrongfully obtain anything of value—\n\n\n“(1) the maximum amount of the forfeiture consequence that may be imposed under subsection (b)(4) or (e)(5)(A) of this section or subsection (b) of section 503 (as the case may be) shall be twice the maximum amount that may be imposed for such infraction under such subsection without regard to this subsection; and\n\n\n“(2) the maximum amount of the criminal charge that may be imposed under subsection (e)(5)(B) of this section or section 501 (as the case may be) shall be twice the maximum amount that may be imposed for such infraction under such subsection or section without regard to this subsection.”.\n\n\n(b) Applicability.—The amendment made by subsection (a) shall apply with respect to violations occurring after the date of the enactment of this Act.", "tags": ["Incentives: Civil liability", "Incentives: Fines", "Incentives: Criminal liability", "Harms: Financial loss", "Harms: Violation of civil or human rights, including privacy"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7116", "official_name": "Do Not Disturb Act", "label": "safe"}
{"id": "566_16", "doc_id": "566", "text": "SEC. 204. ENHANCED consequences FOR VIOLATIONS OF TELEMARKETING RULES INVOLVING AI VOICE OR TEXT MESSAGE IMPERSONATION.\n\n\n(a) In General.—Section 6 of the Telemarketing and Consumer Fraud and Abuse Prevention Act (15 U.S.C. 6105) is amended by adding at the end the following:\n\n\n“(e) Enhanced consequences For Violations Involving AI Voice Or Text Message Impersonation.—\n\n\n“(1) consequences UNDER FEDERAL TRADE COMMISSION ACT.—Notwithstanding subsection (b), in the case of a infraction described in paragraph (2), the maximum amount of the civil consequence that may be imposed for such infraction under subsection (l) or (m) (as the case may be) of section 5 of the Federal Trade Commission Act (15 U.S.C. 45) shall be twice the maximum amount that may be imposed for such infraction under such subsection without regard to this subsection.\n\n\n“(2) VIOLATIONS DESCRIBED.—The violations described in this paragraph are the following:\n\n\n“(A) A infraction of a rule prescribed by the Commission under section 3 with respect to which the party making the call or sending the text message uses artificial intelligence to impersonate an individual or entity with the intent to defraud, cause harm, or wrongfully obtain anything of value.\n\n\n“(B) A infraction of a cease-and-desist order issued by the Commission under section 5(b) of the Federal Trade Commission Act (15 U.S.C. 45(b)) with respect to a infraction described in subparagraph (A).”.\n\n\n(b) Applicability.—The amendment made by subsection (a) shall apply with respect to violations occurring after the date of the enactment of this Act.\n\n\nSEC. 205. COMMISSION DEFINED.\n\n\nIn this title, the term “Commission” means the Federal Trade Commission.", "tags": ["Risk factors: Safety", "Risk factors: Security", "Risk factors: Privacy", "Harms: Financial loss", "Incentives: Fines", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7116", "official_name": "Do Not Disturb Act", "label": "safe"}
{"id": "590_5", "doc_id": "590", "text": "(b) Enforcement.—\n\n\n(1) UNFAIR OR DECEPTIVE ACTS OR PRACTICES.—A infraction of this section or a regulation promulgated thereunder shall be treated as a infraction of a rule defining an unfair or deceptive act or practice under section 18(a)(1)(B) of the Federal Trade Commission Act (15 U.S.C. 57a(a)(1)(B)).\n\n\n(2) POWERS OF THE COMMISSION.—\n\n\n(A) IN GENERAL.—The Commission shall enforce this section in the same manner, by the same means, and with the same jurisdiction, powers, and duties as though all applicable terms and provisions of the Federal Trade Commission Act (15 U.S.C. 41 et seq.) were incorporated into and made a part of this Act.\n\n\n(B) PRIVILEGES AND IMMUNITIES.—Any person who violates this section or a regulation promulgated thereunder shall be subject to the consequences and entitled to the privileges and immunities provided in the Federal Trade Commission Act (15 U.S.C. 41 et seq.).\n\n\n(C) AUTHORITY PRESERVED.—Nothing in this section shall be construed to limit the authority of the Commission under any other provision of law.\n\n\n(D) RULEMAKING.—The Commission shall promulgate in accordance with section 553 of title 5, United States Code, such rules as may be necessary to carry out this section.", "tags": ["Risk factors: Transparency", "Incentives: Civil liability", "Incentives: Fines", "Harms: Detrimental content"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/2765", "official_name": "Advisory for AI-Generated Content Act", "label": "safe"}
{"id": "604_1", "doc_id": "604", "text": "SECTION 1. SHORT TITLE; TABLE OF CONTENTS.\n(a) Short Title.—This Act may be cited as the “Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act” or the “Kids PRIVACY Act”.\n(b) Table Of Contents.—The table of contents for this Act is as follows:\nSec. 1. Short title; table of contents.\nSec. 2. Definitions.\nSec. 3. Requirements for processing of covered information of children or teenagers.\nSec. 4. Repeal of safe harbors provision.\nSec. 5. Administration and applicability of Act.\nSec. 6. Review.\nSec. 7. Private right of action.\nSec. 8. Relationship to other law.\nSec. 9. Additional conforming amendment.\nSec. 10. Youth Privacy and Marketing Division.\nSec. 11. Commission defined.\nSec. 12. Effective date.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_4", "doc_id": "604", "text": "“(B) REQUIREMENTS.—In conducting a PSIAM with respect to a digital service, the operator of the service shall do the following:\n“(i) Embed the PSIAM into the design process of the service and complete the PSIAM before the launch of the service and on an ongoing basis, and before making significant changes to the processing of covered information.\n“(ii) Publicly disclose the nature, scope, context, and purposes of the processing of covered information.\n“(iii) Depending on the size of the service and level of risks identified—\n“(I) seek and document the views of children, teenagers, and parents (or their representatives), as well as experts in children’s and teenagers’ developmental needs; and\n“(II) take such views into account in the design of the service.\n“(iv) Publicly disclose an explanation of why the operator’s processing of covered information is necessary and proportionate vis a vis the risks for the service, and how the operator complies with the requirements of this title.\n“(v) Assess any processing of covered information that is not in the best interests of children or teenagers or that can be detrimental to their well-being and safety, whether physical, emotional, developmental, or material.\n“(vi) Identify, assess, and mitigate high-risk processing of covered information.\n“(vii) Identify measures taken to mitigate the risks identified under clause (vi) and comply with the other requirements of this title.\n“(viii) Provide for regular internal reporting on the effectiveness of controls and residual risks of the operator.", "tags": ["Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_6", "doc_id": "604", "text": "“(22) LIKELY TO BE ACCESSED BY CHILDREN OR TEENAGERS.—The term ‘likely to be accessed by children or teenagers’ means, with respect to a digital service, that the possibility of more than a de minimis number of children or teenagers accessing the digital service is more probable than not. In determining whether a digital service is likely to be accessed by children or teenagers, the operator of the service shall consider whether the service has particular appeal to children or teenagers and whether effective measures are in place that prevent children or teenagers from gaining access to the service.\n“(23) AGE ASSURANCE.—The term ‘age assurance’ means a verifiable process to estimate or determine the age of a user of a digital service with a given and documented degree of certainty.”.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_7", "doc_id": "604", "text": "SEC. 3. REQUIREMENTS FOR PROCESSING OF COVERED INFORMATION OF CHILDREN OR TEENAGERS.\n(a) In General.—Section 1303 of the Children’s Online Privacy Protection Act of 1998 (15 U.S.C. 6502) is amended to read as follows:\n“SEC. 1303. REQUIREMENTS FOR PROCESSING OF COVERED INFORMATION OF CHILDREN OR TEENAGERS.\n“(a) Requirements For Children’s Services.—\n“(1) DATA MINIMIZATION.—An operator of a children’s service shall process covered information under the principle of data minimization, requiring the operator to process only the minimum amount necessary for each purpose for which the covered information is processed.", "tags": ["Strategies: Performance requirements", "Risk factors: Privacy"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_12", "doc_id": "604", "text": "“(D) PROHIBITION ON LIMITING OR DISCONTINUING SERVICE.—An operator of a children’s service may not refuse to provide a service, or discontinue a service provided, to a teenager or child, if the teenager or parent of the child, as applicable, refuses to consent, or withdraws consent, to the processing of any covered information not technically required for the operator to provide such service.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_15", "doc_id": "604", "text": "“(B) RESPONSIBILITIES OF OPERATORS REGARDING THIRD PARTIES.—An operator of a children’s service—\n“(i) shall perform reasonable due diligence in selecting any third party with which to enter into an agreement described in subparagraph (A) and shall exercise reasonable oversight over all such third parties to assure adherence with the requirements of this title and the regulations promulgated under this title; and\n“(ii) if the operator has actual or constructive knowledge that a third party has violated an agreement described in subparagraph (A), shall—\n“(I) to the extent practicable, promptly take steps to ensure adherence with such agreement; and\n“(II) promptly report to the Commission that such a infraction occurred.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_16", "doc_id": "604", "text": "“(C) EXCEPTIONS.—An operator of a children’s service may disclose covered information to a third party other than under an agreement described in subparagraph (A) if such disclosure is necessary and done solely for the purposes of—\n“(i) adherence with—\n“(I) requirements to document adherence under this title; or\n“(II) other laws, regulations, or legal obligations;\n“(ii) preventing risks to the health or safety of a child or teenager or groups of children or teenagers; or\n“(iii) repairing errors that impair the existing (as of the time when the repairs are made) functionality of the children’s service.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_21", "doc_id": "604", "text": "“(E) PROHIBITION ON LIMITING OR DISCONTINUING SERVICE.—An operator of a children’s service may not refuse to provide a service, or discontinue a service provided, to a teenager or child on the basis of the exercise by the teenager or the parent of the child, as applicable, of any of the rights set forth in this paragraph.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_24", "doc_id": "604", "text": "“(8) SECURITY REQUIREMENTS.—\n“(A) IN GENERAL.—An operator of a children’s service shall establish, implement, and maintain reasonable security policies, practices, and procedures for the protection of covered information, taking into consideration—\n“(i) the size, nature, scope, and complexity of the activities engaged in by such operator;\n“(ii) the sensitivity of any covered information at issue; and\n“(iii) the cost of implementing such policies, practices, and procedures.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_25", "doc_id": "604", "text": "“(B) SPECIFIC REQUIREMENTS.—The policies, practices, and procedures established by an operator under subparagraph (A) shall include the following:\n“(i) A written security policy with respect to the processing of such covered information.\n“(ii) The identification of an officer or other individual as the point of contact with responsibility for the management of information security.\n“(iii) A process for identifying and assessing any reasonably foreseeable vulnerabilities in the system or systems maintained by such operator that contain such covered information, including regular monitoring for a breach of security of such system or systems.\n“(iv) A process for taking preventive and corrective action to mitigate against any vulnerabilities identified in the process required by clause (iii), which may include—\n“(I) implementing any changes to the security practices, architecture, installation, or implementation of network or operating software; and\n“(II) regular testing or otherwise monitoring the effectiveness of the safeguards.\n“(v) A process for determining if the covered information is no longer needed and deleting such covered information by shredding, permanently erasing, or otherwise modifying the covered information to make such covered information permanently unreadable or indecipherable.\n“(vi) A process for overseeing persons (other than users of the children’s service) who have access to covered information, including through internet-connected devices, by—\n“(I) taking reasonable steps to select and retain persons that are capable of maintaining appropriate safeguards for the covered information or internet-connected devices at issue; and\n“(II) requiring all such persons to implement and maintain such safeguards.\n“(vii) A process for employee training and supervision for implementation of the policies, practices, and procedures required by this subsection.\n“(viii) A written plan or protocol for internal and public response in the event of a breach of security.", "tags": ["Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_26", "doc_id": "604", "text": "“(C) PERIODIC ASSESSMENT AND CONSUMER PRIVACY AND DATA SECURITY MODERNIZATION.—An operator of a children’s service shall, not less frequently than every 12 months, monitor, evaluate, and adjust, as appropriate, the policies, practices, and procedures of such operator in light of any relevant changes in—\n“(i) technology;\n“(ii) internal or external threats and vulnerabilities to covered information; and\n“(iii) the changing business arrangements of the operator.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_27", "doc_id": "604", "text": "“(D) SUBMISSION OF POLICIES TO THE FTC.—An operator of a children’s service shall submit the policies, practices, and procedures established by the operator under subparagraph (A) to the Commission in conjunction with a notification of a breach of security required by any Federal or State statute or regulation or upon request of the Commission.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_28", "doc_id": "604", "text": "“(b) Rulemaking Regarding Requirements For Digital Services Likely To Be Accessed By Children Or Teenagers.—\n“(1) IN GENERAL.—The Commission shall promulgate regulations under section 553 of title 5, United States Code, that contain requirements for operators of digital services that are not children’s services but are likely to be accessed by children or teenagers, which shall be based on the requirements of subsection (a) but modified as the Commission considers appropriate given a risk-based approach to determine age and to determine and mitigate privacy risks and security risks to the child or teenager, and given differing developmental needs and cognitive capacities of children or teenagers. The Commission may include in such regulations different requirements for operators of different types of such services.\n“(2) BEST INTERESTS OF CHILD OR TEENAGER.—The regulations promulgated under paragraph (1) shall require an operator to make the best interests of children and teenagers a primary design consideration when designing its service, including by conducting a privacy and security impact assessment and mitigation for the service.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_29", "doc_id": "604", "text": "“(3) RISK-BASED APPROACH TO DETERMINING AGE OF USER.—\n“(A) IN GENERAL.—The regulations promulgated under paragraph (1) shall require a risk-based approach to determining the age of a specific user of a digital service under which higher privacy risks and security risks from the processing of covered information require a higher certainty of age assurance.\n“(B) AGE ASSURANCE.—The regulations promulgated under paragraph (1) shall require an operator to conduct an age assurance to determine the age of each specific user.\n“(C) APPROVAL OF AGE ASSURANCE MECHANISMS.—The Commission shall establish in the regulations promulgated under paragraph (1) a process under which an operator may obtain the approval of the Commission of particular mechanisms of age assurance as meeting the age assurance requirements of such regulations for particular levels of privacy risks.\n“(D) DATA MINIMIZATION.—The regulations required by paragraph (1) shall provide that any data collected for age assurance shall be the minimal amount necessary and destroyed immediately or as determined by the Commission, but consistent with standards that still allow for auditing and adherence.", "tags": ["Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_31", "doc_id": "604", "text": "“(d) Implementing Regulations.—\n“(1) IN GENERAL.—Not later than 1 year after the date of the enactment of the Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act, the Commission shall promulgate, under section 553 of title 5, United States Code, such regulations as may be necessary to carry out this section, including the regulations required by subsection (b).\n“(2) REVIEW AND REVISION.—Not later than 10 years after the date on which the Commission promulgates the regulations required by paragraph (1), the Commission shall review such regulations and, if the Commission considers revisions to such regulations appropriate, promulgate such revisions under section 553 of title 5, United States Code.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_32", "doc_id": "604", "text": "“(e) Enforcement.—Subject to section 1306, a infraction of this section or a regulation promulgated under this section shall be treated as a infraction of a rule defining an unfair or deceptive act or practice prescribed under section 18(a)(1)(B) of the Federal Trade Commission Act (15 U.S.C. 57a(a)(1)(B)).”.", "tags": ["Incentives: Criminal liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_33", "doc_id": "604", "text": "(b) Conforming Amendments.—Section 1305 of the Children’s Online Privacy Protection Act of 1998 (15 U.S.C. 6504) is amended—\n(1) in subsection (a)(1)—\n(A) by striking “any regulation of the Commission prescribed under section 1303(b)” and inserting “section 1303 or a regulation promulgated under such section”; and\n(B) in subparagraph (B), by striking “the regulation” and inserting “such section or such regulation”; and\n(2) in subsection (d)—\n(A) by striking “any regulation prescribed under section 1303” and inserting “section 1303 or a regulation promulgated under such section”; and\n(B) by striking “that regulation” and inserting “such section or such regulation”.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_34", "doc_id": "604", "text": "SEC. 4. REPEAL OF SAFE HARBORS PROVISION.\n(a) In General.—Section 1304 of the Children’s Online Privacy Protection Act of 1998 (15 U.S.C. 6503) is repealed.\n(b) Conforming Amendment.—Section 1305(b) of the Children’s Online Privacy Protection Act of 1998 (15 U.S.C. 6504(b)) is amended by striking paragraph (3).", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_35", "doc_id": "604", "text": "SEC. 5. ADMINISTRATION AND APPLICABILITY OF ACT.\n(a) Enforcement By Federal Trade Commission.—Section 1306(d) of the Children’s Online Privacy Protection Act of 1998 (15 U.S.C. 6505(d)) is amended to read as follows:", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_36", "doc_id": "604", "text": "“(d) Actions By The Commission.—\n“(1) IN GENERAL.—Except as provided in paragraphs (2) and (3), the Commission shall prevent any person from violating section 1303 or a regulation promulgated under such section in the same manner, by the same means, and with the same jurisdiction, powers, and duties as though all applicable terms and provisions of the Federal Trade Commission Act (15 U.S.C. 41 et seq.) were incorporated into and made a part of this title, and any person who violates such section or such regulation shall be subject to the consequences and entitled to the privileges and immunities provided in the Federal Trade Commission Act in the same manner, by the same means, and with the same jurisdiction, power, and duties as though all applicable terms and provisions of the Federal Trade Commission Act were incorporated into and made a part of this title.\n“(2) INCREASED CIVIL consequence AMOUNT.—In the case of a civil consequence under subsection (l) or (m) of section 5 of the Federal Trade Commission Act (15 U.S.C. 45) relating to acts or practices in infraction of section 1303 or a regulation promulgated under such section, the maximum dollar amount per infraction shall be $63,795.\n“(3) NONPROFIT ORGANIZATIONS AND COMMON CARRIERS.—Notwithstanding section 4, 5(a)(2), or 6 of the Federal Trade Commission Act (15 U.S.C. 44; 45(a)(2); 46) or any other jurisdictional limitation of the Commission, the Commission shall also enforce section 1303 or a regulation promulgated under such section in the same manner as otherwise provided in this title with respect to—\n“(A) any organization not organized to carry on business for its own profit or that of its members; and\n“(B) any common carrier subject to the Communications Act of 1934 (47 U.S.C. 151 et seq.) and all Acts amendatory thereof and supplementary thereto.”.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_37", "doc_id": "604", "text": "(b) Enforcement By Certain Other Agencies.—Section 1306 of the Children’s Online Privacy Protection Act of 1998 (15 U.S.C. 6505) is amended—\n(1) in subsection (b)—\n(A) in paragraph (1), by striking “, in the case of” and all that follows and inserting the following: “by the appropriate Federal banking agency, with respect to any insured depository institution (as those terms are defined in section 3 of that Act (12 U.S.C. 1813));”;\n(B) in paragraph (6), by striking “Federal land bank, Federal land bank association, Federal intermediate benefit bank, or production benefit association” and inserting “Farm benefit Bank, Agricultural benefit Bank (to the extent exercising the authorities of a Farm benefit Bank), Federal Land benefit Association, or agricultural benefit association”; and\n(C) by striking paragraph (2) and redesignating paragraphs (3) through (6) as paragraphs (2) through (5), respectively; and\n(2) in subsection (c), by striking “subsection (a)” each place it appears and inserting “subsection (b)”.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_39", "doc_id": "604", "text": "SEC. 7. PRIVATE RIGHT OF ACTION.\nThe Children’s Online Privacy Protection Act of 1998 (15 U.S.C. 6501 et seq.) is amended—\n(1) by redesignating sections 1307 and 1308 as sections 1308 and 1309, respectively; and\n(2) by inserting after section 1306 the following:\n“SEC. 1307. PRIVATE RIGHT OF ACTION.\n\n“(a) Right Of Action.—Any parent of a teenager or parent of a child alleging a infraction of section 1303 or a regulation promulgated under such section with respect to the covered information of such teenager or child may bring a civil action in any court of competent jurisdiction.\n“(b) Injury In Fact.—A infraction of section 1303 or a regulation promulgated under such section with respect to the covered information of a teenager or child constitutes an injury in fact to that teenager or child.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_40", "doc_id": "604", "text": "“(c) Relief.—In a civil action brought under subsection (a) in which the plaintiff prevails, the court may award—\n“(1) injunctive relief;\n“(2) actual damages;\n“(3) punitive damages;\n“(4) reasonable attorney’s fees and costs; and\n“(5) any other relief that the court determines appropriate.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_41", "doc_id": "604", "text": "“(d) Pre-Dispute Arbitration Agreements.—\n“(1) IN GENERAL.—No pre-dispute arbitration agreement or pre-dispute joint-action waiver shall be valid or enforceable with respect to any claim arising under section 1303 or a regulation promulgated under such section.\n“(2) DETERMINATION.—A determination as to whether and how this title or a regulation promulgated under this title applies to an arbitration agreement shall be determined under Federal law by the court, rather than the arbitrator, irrespective of whether the party opposing arbitration challenges such agreement specifically or in conjunction with any other term of the contract containing such agreement.\n“(3) DEFINITIONS.—As used in this subsection—\n“(A) the term ‘pre-dispute arbitration agreement’ means any agreement to arbitrate a dispute that has not arisen at the time of the making of the agreement; and\n“(B) the term ‘pre-dispute joint-action waiver’ means an agreement, whether or not part of a pre-dispute arbitration agreement, that would restrict, or waive the right of, one of the parties to the agreement to participate in a joint, class, or collective action in a judicial, arbitral, administrative, or other forum, concerning a dispute that has not yet arisen at the time of the making of the agreement.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_43", "doc_id": "604", "text": "SEC. 8. RELATIONSHIP TO OTHER LAW.\nSection 1306 of the Children’s Online Privacy Protection Act of 1998 (15 U.S.C. 6505) is further amended by adding at the end the following:\n“(f) Relationship To Other Law.—\n“(1) OTHER FEDERAL PRIVACY OR SECURITY PROVISIONS.—Nothing in this title or a regulation promulgated under this title may be construed to modify, limit, or supersede the operation of any privacy or security provision in any other Federal statute or regulation.\n“(2) STATE LAW.—Nothing in this title or a regulation promulgated under this title may be construed to preempt, displace, or supplant any State common law or statute, except to the extent that any such common law or statute specifically and directly conflicts with the provisions of this title or a regulation promulgated under this title, and then only to the extent of the specific and direct conflict. Any such common law or statute is not in specific and direct conflict if it affords a greater level of protection to a child or teenager than the provisions of this title or a regulation promulgated under this title.\n“(3) SECTION 230 OF THE COMMUNICATIONS ACT OF 1934.—Nothing in section 230 of the Communications Act of 1934 (47 U.S.C. 230) may be construed to impair or limit the provisions of this title or a regulation promulgated under this title.”.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_44", "doc_id": "604", "text": "SEC. 9. ADDITIONAL CONFORMING AMENDMENT.\nThe heading of title XIII of division C of the Omnibus Consolidated and Emergency Supplemental Appropriations Act, 1999 (Public Law 105–277; 112 Stat. 2681–728) is amended by inserting “AND TEENAGER’S” after “CHILDREN’S”.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_45", "doc_id": "604", "text": "SEC. 10. YOUTH PRIVACY AND MARKETING DIVISION.\n(a) Establishment.—There is established within the Commission a division to be known as the Youth Privacy and Marketing Division.\n(b) Director.—The Youth Privacy and Marketing Division shall be headed by a Director, who shall be appointed by the Chairman of the Commission.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_46", "doc_id": "604", "text": "(c) Duties.—The Youth Privacy and Marketing Division shall be responsible for assisting the Commission in addressing, as it relates to this Act and the amendments made by this Act—\n(1) the privacy of children and teenagers; and\n(2) marketing directed at children and teenagers.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_47", "doc_id": "604", "text": "(d) Staff.—The Youth Privacy and Marketing Division shall be comprised of adequate staff to carry out the duties under subsection (c), including individuals who are experts in data protection, digital advertising, data analytics, and youth development.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_48", "doc_id": "604", "text": "(e) Reports.—Not later than 1 year after the date of the enactment of this Act, and every 2 years thereafter, the Director of the Youth Privacy and Marketing Division shall submit to the Committee on Commerce, Science, and Transportation of the Senate and the Committee on Energy and Commerce of the House of Representatives a report that includes—\n(1) a description of the work of the Youth Privacy and Marketing Division on emerging concerns relating to youth privacy and marketing practices; and\n(2) an assessment of how effectively the Commission has, during the period for which the report is submitted, addressed youth privacy and marketing practices.\n(f) Definitions.—In this section, the terms “child” and “teenager” have the meanings given such terms in section 1302 of the Children’s Online Privacy Protection Act of 1998 (15 U.S.C. 6501), as amended by this Act.", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "604_50", "doc_id": "604", "text": "SEC. 12. EFFECTIVE DATE.\nThe amendments made by this Act, except for subsection (d)(1) of section 1303 of the Children’s Online Privacy Protection Act of 1998 (15 U.S.C. 6502), shall take effect on the date that is 1 year after the date on which the Commission promulgates the regulations required by such subsection (d)(1).", "tags": ["Risk factors: Privacy", "Harms: Financial loss", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Evaluation", "Risk factors: Privacy", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About inputs", "Strategies: Evaluation", "Risk factors: Transparency", "Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Privacy", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Bias", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Privacy", "Strategies: Performance requirements", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Performance requirements", "Risk factors: Privacy", "Incentives: Criminal liability", "Strategies: Evaluation"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/2801", "official_name": "Protecting the Information of our Vulnerable Adolescents, Children, and Youth Act", "label": "safe"}
{"id": "637_2", "doc_id": "637", "text": "SEC. 2. INELIGIBILITY FOR CERTAIN FUNDS.\n\n\nIn the case of a State or unit of local government that received a financial support allocation award under subpart 1 of part E of title I of the Omnibus Crime Control and Safe Streets Act of 1968 (42 U.S.C. 3750 et seq.), if the State or unit of local government fails to substantially to comply with the requirements under this Act for a fiscal year, the Attorney General shall reduce the amount that would otherwise be awarded to that State or unit of local government under such financial support allocation program in the following fiscal year by 15 percent.", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/6092", "official_name": "Facial Recognition Act of 2023", "label": "safe"}
{"id": "637_6", "doc_id": "637", "text": "SEC. 103. LOGGING OF SEARCHES.\n\n\nA law enforcement agency whose investigative or law enforcement officers use facial recognition shall log its use of the facial recognition to the extent necessary to comply with the public reporting and review requirements of sections 104 and 105 of this Act.", "tags": ["Strategies: Disclosure", "Applications: Government: judicial and law enforcement"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/6092", "official_name": "Facial Recognition Act of 2023", "label": "safe"}
{"id": "637_8", "doc_id": "637", "text": "SEC. 105. reviews.\n\n\n(a) Federal Level review.—\n\n\n(1) IN GENERAL.—Any Federal law enforcement agency whose investigative or law enforcement officers use facial recognition, regardless of whether they use a system operated by that agency or another agency, shall annually submit data with respect to their use of facial recognition for review by the Government Accountability Office to prevent and identify misuse and to ensure adherence with sections 101, 102, and 103 of this Act, including—\n\n\n(A) a summary of the findings of the review, including the number and nature of violations identified; and\n\n\n(B) information about the procedures used by the law enforcement agency to remove arrest photos from databases in accordance with this Act.\n\n\n(2) SUSPENSION.—\n\n\n(A) IN GENERAL.—If a infraction is uncovered by the review conducted under paragraph (1), the Federal law enforcement agency shall cease using facial recognition until such time that all violations have been corrected.\n\n\n(B) PUBLIC NOTICE.—If use of facial recognition is suspended pursuant to subparagraph (A), the Federal law enforcement agency shall notify the public of such suspension.\n\n\n(b) State Level review.—\n\n\n(1) IN GENERAL.—Any State or local law enforcement agency whose investigative or law enforcement officers use facial recognition, regardless of whether they use a system operated by that agency or another agency, shall annually submit data with respect to their use of facial recognition to an independent State agency (as determined by the chief executive of the State) to prevent and identify misuse and to ensure adherence with sections 101, 102, and 103 of this Act. Such independent State agency shall report—\n\n\n(A) a summary of the findings of the review, including the number and nature of violations identified, to Director of the Administrative Office of the United States Courts, and subsequently release that information to the public and post it online;\n\n\n(B) information about the procedures used by the law enforcement agency to remove arrest photos from databases in accordance with this section; and\n\n\n(C) any violations identified by the independent State agency.\n\n\n(2) SUSPENSION.—\n\n\n(A) IN GENERAL.—If a infraction is uncovered by the review conducted under paragraph (1), the State or local law enforcement agency shall cease using facial recognition until such time that all violations have been corrected.\n\n\n(B) PUBLIC NOTICE.—If use of facial recognition is suspended pursuant to subparagraph (A), the State or local law enforcement agency shall notify the public of such suspension.\n\n\n(c) Disaggregated Data.—Data collected pursuant to subsection (a) or (b) shall, when feasible, be collected in a manner that allows such data to be disaggregated by race, ethnicity, gender, and age.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Disclosure", "Strategies: Disclosure: About incidents", "Risk factors: Transparency", "Strategies: Evaluation: Conformity assessment"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/6092", "official_name": "Facial Recognition Act of 2023", "label": "safe"}
{"id": "637_10", "doc_id": "637", "text": "SEC. 107. ENFORCEMENT.\n\n\n(a) Suppression.—In the case that the use of facial recognition has occurred, no results from the use and no evidence derived therefrom may be received in evidence in any trial, hearing, or other proceeding in or before any court, grand jury, department, officer, agency, regulatory body, legislative committee, or other authority of the United States, a State, or a political subdivision thereof if the use of facial recognition violated this Act or if the use was conducted in an emergency under section 101 and the officer or agency did not subsequently obtain an order for that use as required under such section.\n\n\n(b) Administrative Discipline.—If a court or law enforcement agency determines that an investigative or law enforcement officer has violated any provision of this Act, and the court or agency finds that the circumstances surrounding the infraction raise serious questions about whether or not the officer acted intentionally with respect to the infraction, the agency shall promptly initiate a proceeding to determine whether disciplinary action against the officer is warranted.\n\n\n(c) Civil Action.—\n\n\n(1) IN GENERAL.—Any person who is subject to identification or attempted identification through facial recognition in infraction of this Act may bring a civil action in the appropriate court to recover such relief as may be appropriate from the investigative or law enforcement officer or the State or Federal law enforcement agency which engaged in that infraction.\n\n\n(2) RELIEF.—In an action under this subsection, appropriate relief includes—\n\n\n(A) such preliminary and other equitable or declaratory relief as may be appropriate;\n\n\n(B) damages under paragraph (3) and punitive damages in appropriate cases; and\n\n\n(C) a reasonable attorney’s fee and other litigation costs reasonably incurred.\n\n\n(3) COMPUTATION OF DAMAGES.—The court may assess as damages whichever is the greater of—\n\n\n(A) any profits made with respect to the infraction suffered by the plaintiff; or\n\n\n(B) $50,000 for each infraction.\n\n\n(4) DEFENSE.—A good faith reliance on—\n\n\n(A) a court warrant or order, a grand jury subpoena, a legislative authorization, or a statutory authorization; or\n\n\n(B) a good faith determination that section 101 permitted the conduct complained of,\n\n\nis a complete defense against any civil action brought under this Act.\n\n\n(5) LIMITATION.—A civil action under this section may not be commenced later than two years after the date upon which the claimant first has a reasonable opportunity to discover the infraction.\n\n\n(d) Civil Action For Disparate Impact.—An individual may bring a civil action when use of facial recognition or face surveillance by a law enforcement agency, or any technological element, criteria, method, or design feature thereof acting individually or in concert, results in disparate treatment or adverse impact against an individual or class of individuals on the basis of race, ethnicity, gender, or age.", "tags": ["Incentives: Civil liability", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/6092", "official_name": "Facial Recognition Act of 2023", "label": "safe"}
{"id": "637_15", "doc_id": "637", "text": "SEC. 204. LIMITATION ON responsibility.\n\n\nA State must avoid be immune under the eleventh amendment to the Constitution of the United States from an action in Federal or State court of competent jurisdiction for a infraction of this Act. In any action against a State for a infraction of the requirements of this Act, remedies (including remedies both at law and in equity) are available for such a infraction to the same extent as such remedies are available for such a infraction in an action against any public or private entity other than a State.", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/6092", "official_name": "Facial Recognition Act of 2023", "label": "safe"}
{"id": "757_7", "doc_id": "757", "text": "(21)\t‘conformity assessment body’ means a body that performs third-party conformity assessment activities, including testing, certification and inspection;\n\n(22)\t‘notified body’ means a conformity assessment body notified in accordance with this Regulation and other relevant Union harmonisation legislation;\n\n(23)\t‘substantial modification’ means a change to an AI system after its placing on the market or putting into service which is not foreseen or planned in the initial conformity assessment carried out by the provider and as a result of which the adherence of the AI system with the requirements set out in Chapter III, Section 2 is affected or results in a modification to the intended purpose for which the AI system has been assessed;\n\n(24)\t‘CE marking’ means a marking by which a provider indicates that an AI system is in conformity with the requirements set out in Chapter III, Section 2 and other applicable Union harmonisation legislation providing for its affixing;\n\n(25)\t‘post-market monitoring system’ means all activities carried out by providers of AI systems to collect and review experience gained from the use of AI systems they place on the market or put into service for the purpose of identifying any need to immediately apply any necessary corrective or preventive actions;\n\n(26)\t‘market surveillance authority’ means the national authority carrying out the activities and taking the measures pursuant to Regulation (EU) 2019/1020;\n\n(27)\t‘harmonised standard’ means a harmonised standard as defined in Article 2(1), point (c), of Regulation (EU) No 1025/2012;\n\n(28)\t‘common specification’ means a set of technical specifications as defined in Article 2, point (4) of Regulation (EU) No 1025/2012, providing means to comply with certain requirements established under this Regulation;\n\n(29)\t‘training data’ means data used for training an AI system through fitting its learnable parameters;\n\n(30)\t‘validation data’ means data used for providing an evaluation of the trained AI system and for tuning its non-learnable parameters and its learning process in order, inter alia, to prevent underfitting or overfitting;", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_8", "doc_id": "757", "text": "(31)\t‘validation data set’ means a separate data set or part of the training data set, either as a fixed or variable split;\n\n(32)\t‘testing data’ means data used for providing an independent evaluation of the AI system in order to confirm the expected performance of that system before its placing on the market or putting into service;\n\n(33)\t‘input data’ means data provided to or directly acquired by an AI system on the basis of which the system produces an output;\n\n(34)\t‘biometric data’ means personal data resulting from specific technical processing relating to the physical, physiological or behavioural characteristics of a natural person, such as facial images or dactyloscopic data;\n\n(35)\t‘biometric identification’ means the automated recognition of physical, physiological, behavioural, or psychological human features for the purpose of establishing the identity of a natural person by comparing biometric data of that individual to biometric data of individuals stored in a database;\n\n(36)\t‘biometric verification’ means the automated, one-to-one verification, including authentication, of the identity of natural persons by comparing their biometric data to previously provided biometric data;\n\n(37)\t‘special categories of personal data’ means the categories of personal data referred to in Article 9(1) of Regulation (EU) 2016/679, Article 10 of Directive (EU) 2016/680 and Article 10(1) of Regulation (EU) 2018/1725;\n\n(38)\t‘sensitive operational data’ means operational data related to activities of prevention, detection, investigation or prosecution of criminal offences, the disclosure of which could jeopardise the integrity of criminal proceedings;\n\n(39)\t‘emotion recognition system’ means an AI system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data;\n\n(40)\t‘biometric categorisation system’ means an AI system for the purpose of assigning natural persons to specific categories on the basis of their biometric data, unless it is ancillary to another commercial service and strictly necessary for objective technical reasons;", "tags": ["Applications: Government: judicial and law enforcement"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_9", "doc_id": "757", "text": "(41)\t‘remote biometric identification system’ means an AI system for the purpose of identifying natural persons, without their active involvement, typically at a distance through the comparison of a person’s biometric data with the biometric data contained in a reference database;\n\n(42)\t‘real-time remote biometric identification system’ means a remote biometric identification system, whereby the capturing of biometric data, the comparison and the identification all occur without a significant delay, comprising not only instant identification, but also limited short delays in order to avoid circumvention;\n\n(43)\t‘post-remote biometric identification system’ means a remote biometric identification system other than a real-time remote biometric identification system;\n\n(44)\t‘publicly accessible space’ means any publicly or privately owned physical place accessible to an undetermined number of natural persons, regardless of whether certain conditions for access may apply, and regardless of the potential capacity restrictions;\n\n(45)\t‘law enforcement authority’ means:\n\n(a)\tany public authority competent for the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal consequences, including the safeguarding against and the prevention of threats to public security; or\n\n(b)\tany other body or entity entrusted by Member State law to exercise public authority and public powers for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal consequences, including the safeguarding against and the prevention of threats to public security;\n\n(46)\t‘law enforcement’ means activities carried out by law enforcement authorities or on their behalf for the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal consequences, including safeguarding against and preventing threats to public security;\n\n(47)\t‘AI Office’ means the Commission’s function of contributing to the implementation, monitoring and supervision of AI systems and general-purpose AI models, and AI governance, provided for in Commission Decision of 24 January 2024; references in this Regulation to the AI Office shall be construed as references to the Commission;\n\n(48)\t‘national competent authority’ means a notifying authority or a market surveillance authority; as regards AI systems put into service or used by Union institutions, agencies, offices and bodies, references to national competent authorities or market surveillance authorities in this Regulation shall be construed as references to the European Data Protection Supervisor;\n\n(49)\t‘serious incident’ means an incident or malfunctioning of an AI system that directly or indirectly leads to any of the following:\n\n(a)\tthe death of a person, or serious harm to a person’s health;\n\n(b)\ta serious and irreversible disruption of the management or operation of critical infrastructure;\n\n(c)\tthe infringement of obligations under Union law intended to protect fundamental rights;\n\n(d)\tserious harm to property or the environment;\n\n(50)\t‘personal data’ means personal data as defined in Article 4, point (1), of Regulation (EU) 2016/679;", "tags": ["Harms: Harm to health/safety", "Risk factors: Safety", "Risk factors: Reliability", "Harms: Harm to property", "Harms: Harm to infrastructure", "Harms: Violation of civil or human rights, including privacy", "Harms: Ecological harm"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_13", "doc_id": "757", "text": "CHAPTER II: restricted AI PRACTICES\n\nArticle 5: restricted AI practices\n\n1.   The following AI practices shall be restricted:\n\n(a)\tthe placing on the market, the putting into service or the use of an AI system that deploys subliminal techniques beyond a person’s consciousness or purposefully manipulative or deceptive techniques, with the objective, or the effect of materially distorting the behaviour of a person or a group of persons by appreciably impairing their ability to make an informed decision, thereby causing them to take a decision that they would not have otherwise taken in a manner that causes or is reasonably likely to cause that person, another person or group of persons significant harm;\n\n(b)\tthe placing on the market, the putting into service or the use of an AI system that exploits any of the vulnerabilities of a natural person or a specific group of persons due to their age, disability or a specific social or economic situation, with the objective, or the effect, of materially distorting the behaviour of that person or a person belonging to that group in a manner that causes or is reasonably likely to cause that person or another person significant harm;\n\n(c)\tthe placing on the market, the putting into service or the use of AI systems for the evaluation or classification of natural persons or groups of persons over a certain period of time based on their social behaviour or known, inferred or predicted personal or personality characteristics, with the social score leading to either or both of the following:\n\n(i)\tdetrimental or unfavourable treatment of certain natural persons or groups of persons in social contexts that are unrelated to the contexts in which the data was originally generated or collected;\n\n(ii)\tdetrimental or unfavourable treatment of certain natural persons or groups of persons that is unjustified or disproportionate to their social behaviour or its gravity;", "tags": ["Harms: Detrimental content", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls", "Strategies: Input controls: Data use", "Risk factors: Privacy"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_14", "doc_id": "757", "text": "(d)\tthe placing on the market, the putting into service for this specific purpose, or the use of an AI system for making risk assessments of natural persons in order to assess or predict the risk of a natural person committing a criminal offence, based solely on the profiling of a natural person or on assessing their personality traits and characteristics; this prohibition must avoid apply to AI systems used to support the human assessment of the involvement of a person in a criminal activity, which is already based on objective and verifiable facts directly linked to a criminal activity;\n\n(e)\tthe placing on the market, the putting into service for this specific purpose, or the use of AI systems that create or expand facial recognition databases through the untargeted scraping of facial images from the internet or CCTV footage;\n\n(f)\tthe placing on the market, the putting into service for this specific purpose, or the use of AI systems to infer emotions of a natural person in the areas of workplace and education institutions, except where the use of the AI system is intended to be put in place or into the market for medical or safety reasons;", "tags": ["Applications: Government: judicial and law enforcement", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Applications: Education", "Applications: Medicine, life sciences and public health", "Applications: Business services and analytics"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_15", "doc_id": "757", "text": "(g)\tthe placing on the market, the putting into service for this specific purpose, or the use of biometric categorisation systems that categorise individually natural persons based on their biometric data to deduce or infer their race, political opinions, trade union membership, religious or philosophical beliefs, sex life or sexual orientation; this prohibition does not cover any labelling or filtering of lawfully acquired biometric datasets, such as images, based on biometric data or categorizing of biometric data in the area of law enforcement;\n\n(h)\tthe use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purposes of law enforcement, unless and in so far as such use is strictly necessary for one of the following objectives:\n\n(i)\tthe targeted search for specific victims of abduction, trafficking in human beings or sexual exploitation of human beings, as well as the search for missing persons;\n\n(ii)\tthe prevention of a specific, substantial and imminent threat to the life or physical safety of natural persons or a genuine and present or genuine and foreseeable threat of a terrorist attack;\n\n(iii)\tthe localisation or identification of a person suspected of having committed a criminal offence, for the purpose of conducting a criminal investigation or prosecution or executing a criminal consequence for offences referred to in Annex II and punishable in the Member State concerned by a custodial sentence or a detention order for a maximum period of at least four years.\n\nPoint (h) of the first subparagraph is without prejudice to Article 9 of Regulation (EU) 2016/679 for the processing of biometric data for purposes other than law enforcement.", "tags": ["Applications: Government: judicial and law enforcement", "Harms: Harm to health/safety", "Applications: Medicine, life sciences and public health", "Risk factors: Security: Cybersecurity", "Risk factors: Safety", "Applications: Government: military and public safety", "Harms: Violation of civil or human rights, including privacy"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_16", "doc_id": "757", "text": "2.   The use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purposes of law enforcement for any of the objectives referred to in paragraph 1, first subparagraph, point (h), shall be deployed for the purposes set out in that point only to confirm the identity of the specifically targeted individual, and it shall take into account the following elements:\n\n(a)\tthe nature of the situation giving rise to the possible use, in particular the seriousness, probability and scale of the harm that would be caused if the system were not used;\n\n(b)\tthe consequences of the use of the system for the rights and freedoms of all persons concerned, in particular the seriousness, probability and scale of those consequences.\n\nIn addition, the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purposes of law enforcement for any of the objectives referred to in paragraph 1, first subparagraph, point (h), of this Article shall comply with necessary and proportionate safeguards and conditions in relation to the use in accordance with the national law authorising the use thereof, in particular as regards the temporal, geographic and personal limitations. The use of the ‘real-time’ remote biometric identification system in publicly accessible spaces shall be authorised only if the law enforcement authority has completed a fundamental rights impact assessment as provided for in Article 27 and has registered the system in the EU database according to Article 49. However, in duly justified cases of urgency, the use of such systems may be commenced without the registration in the EU database, provided that such registration is completed without undue delay.", "tags": ["Applications: Government: judicial and law enforcement", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Safety", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Licensing, registration, and certification", "Strategies: Performance requirements"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_17", "doc_id": "757", "text": "3.   For the purposes of paragraph 1, first subparagraph, point (h) and paragraph 2, each use for the purposes of law enforcement of a ‘real-time’ remote biometric identification system in publicly accessible spaces shall be subject to a prior authorisation granted by a judicial authority or an independent administrative authority whose decision is binding of the Member State in which the use is to take place, issued upon a reasoned request and in accordance with the detailed rules of national law referred to in paragraph 5. However, in a duly justified situation of urgency, the use of such system may be commenced without an authorisation provided that such authorisation is requested without undue delay, at the latest within 24 hours. If such authorisation is rejected, the use shall be stopped with immediate effect and all the data, as well as the results and outputs of that use shall be immediately discarded and deleted.\n\nThe competent judicial authority or an independent administrative authority whose decision is binding shall financial support allocation the authorisation only where it is satisfied, on the basis of objective evidence or clear indications presented to it, that the use of the ‘real-time’ remote biometric identification system concerned is necessary for, and proportionate to, achieving one of the objectives specified in paragraph 1, first subparagraph, point (h), as identified in the request and, in particular, remains limited to what is strictly necessary concerning the period of time as well as the geographic and personal scope. In deciding on the request, that authority shall take into account the elements referred to in paragraph 2. No decision that produces an adverse legal effect on a person may be taken based solely on the output of the ‘real-time’ remote biometric identification system.", "tags": ["Applications: Government: judicial and law enforcement", "Applications: Medicine, life sciences and public health"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_20", "doc_id": "757", "text": "CHAPTER III: HIGH-RISK AI SYSTEMS\n\nSECTION 1\nClassification of AI systems as high-risk\n\nArticle 6: Classification rules for high-risk AI systems\n\n1.   Irrespective of whether an AI system is placed on the market or put into service independently of the products referred to in points (a) and (b), that AI system shall be considered to be high-risk where both of the following conditions are fulfilled:\n\n(a)\tthe AI system is intended to be used as a safety component of a product, or the AI system is itself a product, covered by the Union harmonisation legislation listed in Annex I;\n\n(b)\tthe product whose safety component pursuant to point (a) is the AI system, or the AI system itself as a product, is required to undergo a third-party conformity assessment, with a view to the placing on the market or the putting into service of that product pursuant to the Union harmonisation legislation listed in Annex I.\n\n2.   In addition to the high-risk AI systems referred to in paragraph 1, AI systems referred to in Annex III shall be considered to be high-risk.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Tiering", "Strategies: Tiering: Tiering based on impact", "Risk factors: Safety"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_30", "doc_id": "757", "text": "6.   High-risk AI systems shall be tested for the purpose of identifying the most appropriate and targeted risk management measures. Testing shall ensure that high-risk AI systems perform consistently for their intended purpose and that they are in adherence with the requirements set out in this Section.\n\n7.   Testing procedures may include testing in real-world conditions in accordance with Article 60.\n\n8.   The testing of high-risk AI systems shall be performed, as appropriate, at any time throughout the development process, and, in any event, prior to their being placed on the market or put into service. Testing shall be carried out against prior defined metrics and probabilistic thresholds that are appropriate to the intended purpose of the high-risk AI system.\n\n9.   When implementing the risk management system as provided for in paragraphs 1 to 7, providers shall give consideration to whether in view of its intended purpose the high-risk AI system is likely to have an adverse impact on persons under the age of 18 and, as appropriate, other vulnerable groups.\n\n10.   For providers of high-risk AI systems that are subject to requirements regarding internal risk management processes under other relevant provisions of Union law, the aspects provided in paragraphs 1 to 9 may be part of, or combined with, the risk management procedures established pursuant to that law.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Tiering", "Strategies: Evaluation: Impact assessment", "Harms: Discrimination"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_32", "doc_id": "757", "text": "3.   Training, validation and testing data sets shall be relevant, sufficiently representative, and to the best extent possible, free of errors and complete in view of the intended purpose. They shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons in relation to whom the high-risk AI system is intended to be used. Those characteristics of the data sets may be met at the level of individual data sets or at the level of a combination thereof.\n\n4.   Data sets shall take into account, to the extent required by the intended purpose, the characteristics or elements that are particular to the specific geographical, contextual, behavioural or functional setting within which the high-risk AI system is intended to be used.", "tags": ["Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_33", "doc_id": "757", "text": "5.   To the extent that it is strictly necessary for the purpose of ensuring bias detection and correction in relation to the high-risk AI systems in accordance with paragraph (2), points (f) and (g) of this Article, the providers of such systems may exceptionally process special categories of personal data, subject to appropriate safeguards for the fundamental rights and freedoms of natural persons. In addition to the provisions set out in Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, all the following conditions must be met in order for such processing to occur:\n\n(a)\tthe bias detection and correction cannot be effectively fulfilled by processing other data, including synthetic or anonymised data;\n\n(b)\tthe special categories of personal data are subject to technical limitations on the re-use of the personal data, and state-of-the-art security and privacy-preserving measures, including pseudonymisation;\n\n(c)\tthe special categories of personal data are subject to measures to ensure that the personal data processed are secured, protected, subject to suitable safeguards, including strict controls and documentation of the access, to avoid misuse and ensure that only authorised persons have access to those personal data with appropriate confidentiality obligations;\n\n(d)\tthe special categories of personal data are not to be transmitted, transferred or otherwise accessed by other parties;\n\n(e)\tthe special categories of personal data are deleted once the bias has been corrected or the personal data has reached the end of its retention period, whichever comes first;\n\n(f)\tthe records of processing activities pursuant to Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680 include the reasons why the processing of special categories of personal data was strictly necessary to detect and correct biases, and why that objective could not be achieved by processing other data.\n\n6.   For the development of high-risk AI systems not using techniques involving the training of AI models, paragraphs 2 to 5 apply only to the testing data sets.", "tags": ["Risk factors: Bias", "Harms: Violation of civil or human rights, including privacy", "Strategies: Tiering", "Strategies: Tiering: Tiering based on inputs", "Strategies: New institution", "Strategies: Input controls: Data use", "Risk factors: Privacy", "Risk factors: Security", "Strategies: Input controls: Data circulation", "Strategies: Input controls"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_35", "doc_id": "757", "text": "Article 12: Record-keeping\n\n1.   High-risk AI systems shall technically allow for the automatic recording of events (logs) over the lifetime of the system.\n\n2.   In order to ensure a level of traceability of the functioning of a high-risk AI system that is appropriate to the intended purpose of the system, logging capabilities shall enable the recording of events relevant for:\n\n(a)\tidentifying situations that may result in the high-risk AI system presenting a risk within the meaning of Article 79(1) or in a substantial modification;\n\n(b)\tfacilitating the post-market monitoring referred to in Article 72; and\n\n(c)\tmonitoring the operation of high-risk AI systems referred to in Article 26(5).\n\n3.   For high-risk AI systems referred to in point 1 (a), of Annex III, the logging capabilities shall provide, at a minimum:\n\n(a)\trecording of the period of each use of the system (start date and time and end date and time of each use);\n\n(b)\tthe reference database against which input data has been checked by the system;\n\n(c)\tthe input data for which the search has led to a match;\n\n(d)\tthe identification of the natural persons involved in the verification of the results, as referred to in Article 14(5).", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: About inputs"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_36", "doc_id": "757", "text": "Article 13: Transparency and provision of information to deployers\n\n1.   High-risk AI systems shall be designed and developed in such a way as to ensure that their operation is sufficiently transparent to enable deployers to interpret a system’s output and use it appropriately. An appropriate type and degree of transparency shall be ensured with a view to achieving adherence with the relevant obligations of the provider and deployer set out in Section 3.\n\n2.   High-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or otherwise that include concise, complete, correct and clear information that is relevant, accessible and comprehensible to deployers.\n\n3.   The instructions for use shall contain at least the following information:\n\n(a)\tthe identity and the contact details of the provider and, where applicable, of its authorised representative;\n\n(b)\tthe characteristics, capabilities and limitations of performance of the high-risk AI system, including:\n\n(i)\tits intended purpose;\n\n(ii)\tthe level of accuracy, including its metrics, robustness and cybersecurity referred to in Article 15 against which the high-risk AI system has been tested and validated and which can be expected, and any known and foreseeable circumstances that may have an impact on that expected level of accuracy, robustness and cybersecurity;\n\n(iii)\tany known or foreseeable circumstance, related to the use of the high-risk AI system in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety or fundamental rights referred to in Article 9(2);\n\n(iv)\twhere applicable, the technical capabilities and characteristics of the high-risk AI system to provide information that is relevant to explain its output;\n\n(v)\twhen appropriate, its performance regarding specific persons or groups of persons on which the system is intended to be used;\n\n(vi)\twhen appropriate, specifications for the input data, or any other relevant information in terms of the training, validation and testing data sets used, taking into account the intended purpose of the high-risk AI system;\n\n(vii)\twhere applicable, information to enable deployers to interpret the output of the high-risk AI system and use it appropriately;", "tags": ["Risk factors: Transparency", "Strategies: Tiering", "Strategies: Tiering: Tiering based on impact", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: In deployment", "Risk factors: Reliability", "Risk factors: Reliability: Robustness", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure: About inputs", "Harms: Discrimination", "Risk factors: Interpretability and explainability"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_37", "doc_id": "757", "text": "(c)\tthe changes to the high-risk AI system and its performance which have been pre-determined by the provider at the moment of the initial conformity assessment, if any;\n\n(d)\tthe human oversight measures referred to in Article 14, including the technical measures put in place to facilitate the interpretation of the outputs of the high-risk AI systems by the deployers;\n\n(e)\tthe computational and hardware resources needed, the expected lifetime of the high-risk AI system and any necessary maintenance and care measures, including their frequency, to ensure the proper functioning of that AI system, including as regards software updates;\n\n(f)\twhere relevant, a description of the mechanisms included within the high-risk AI system that allows deployers to properly collect, store and interpret the logs in accordance with Article 12.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Risk factors: Interpretability and explainability", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Risk factors: Reliability", "Strategies: Disclosure: About evaluation"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_40", "doc_id": "757", "text": "5.   For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 of this Article shall be such as to ensure that, in addition, no action or decision is taken by the deployer on the basis of the identification resulting from the system unless that identification has been separately verified and confirmed by at least two natural persons with the necessary competence, training and authority.\n\nThe requirement for a separate verification by at least two natural persons must avoid apply to high-risk AI systems used for the purposes of law enforcement, migration, border control or asylum, where Union or national law considers the application of this requirement to be disproportionate.", "tags": ["Applications: Government: judicial and law enforcement", "Applications: Government: other applications/unspecified"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_42", "doc_id": "757", "text": "4.   High-risk AI systems shall be as resilient as possible regarding errors, faults or inconsistencies that may occur within the system or the environment in which the system operates, in particular due to their interaction with natural persons or other systems. Technical and organisational measures shall be taken in this regard.\n\nThe robustness of high-risk AI systems may be achieved through technical redundancy solutions, which may include backup or fail-safe plans.\n\nHigh-risk AI systems that continue to learn after being placed on the market or put into service shall be developed in such a way as to eliminate or reduce as far as possible the risk of possibly biased outputs influencing input for future operations (feedback loops), and as to ensure that any such feedback loops are duly addressed with appropriate mitigation measures.\n\n5.   High-risk AI systems shall be resilient against attempts by unauthorised third parties to alter their use, outputs or performance by exploiting system vulnerabilities.\n\nThe technical solutions aiming to ensure the cybersecurity of high-risk AI systems shall be appropriate to the relevant circumstances and the risks.\n\nThe technical solutions to address AI specific vulnerabilities shall include, where appropriate, measures to prevent, detect, respond to, resolve and control for attacks trying to manipulate the training data set (data poisoning), or pre-trained components used in training (model poisoning), inputs designed to cause the AI model to make a mistake (adversarial examples or model evasion), confidentiality attacks or model flaws.\n\nSECTION 3\nObligations of providers and deployers of high-risk AI systems and other parties", "tags": ["Risk factors: Reliability", "Risk factors: Reliability: Robustness", "Strategies: Input controls", "Strategies: Input controls: Data use", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Performance requirements", "Risk factors: Bias"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_43", "doc_id": "757", "text": "Article 16: Obligations of providers of high-risk AI systems\n\nProviders of high-risk AI systems shall:\n\n(a)\tensure that their high-risk AI systems are compliant with the requirements set out in Section 2;\n\n(b)\tindicate on the high-risk AI system or, where that is not possible, on its packaging or its accompanying documentation, as applicable, their name, registered trade name or registered trade mark, the address at which they can be contacted;\n\n(c)\thave a quality management system in place which complies with Article 17;\n\n(d)\tkeep the documentation referred to in Article 18;\n\n(e)\twhen under their control, keep the logs automatically generated by their high-risk AI systems as referred to in Article 19;\n\n(f)\tensure that the high-risk AI system undergoes the relevant conformity assessment procedure as referred to in Article 43, prior to its being placed on the market or put into service;", "tags": ["Strategies: Licensing, registration, and certification", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Performance requirements"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_45", "doc_id": "757", "text": "Article 17: Quality management system\n\n1.   Providers of high-risk AI systems shall put a quality management system in place that ensures adherence with this Regulation. That system shall be documented in a systematic and orderly manner in the form of written policies, procedures and instructions, and shall include at least the following aspects:\n\n(a)\ta strategy for regulatory adherence, including adherence with conformity assessment procedures and procedures for the management of modifications to the high-risk AI system;\n\n(b)\ttechniques, procedures and systematic actions to be used for the design, design control and design verification of the high-risk AI system;\n\n(c)\ttechniques, procedures and systematic actions to be used for the development, quality control and quality assurance of the high-risk AI system;\n\n(d)\texamination, test and validation procedures to be carried out before, during and after the development of the high-risk AI system, and the frequency with which they have to be carried out;\n\n(e)\ttechnical specifications, including standards, to be applied and, where the relevant harmonised standards are not applied in full or do not cover all of the relevant requirements set out in Section 2, the means to be used to ensure that the high-risk AI system complies with those requirements;", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Risk factors: Reliability", "Strategies: Performance requirements", "Strategies: Evaluation: Impact assessment"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_50", "doc_id": "757", "text": "Article 20: Corrective actions and duty of information\n\n1.   Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system that they have placed on the market or put into service is not in conformity with this Regulation shall immediately take the necessary corrective actions to bring that system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They shall inform the distributors of the high-risk AI system concerned and, where applicable, the deployers, the authorised representative and importers accordingly.\n\n2.   Where the high-risk AI system presents a risk within the meaning of Article 79(1) and the provider becomes aware of that risk, it shall immediately investigate the causes, in collaboration with the reporting deployer, where applicable, and inform the market surveillance authorities competent for the high-risk AI system concerned and, where applicable, the notified body that issued a certificate for that high-risk AI system in accordance with Article 44, in particular, of the nature of the non-adherence and of any relevant corrective action taken.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About incidents"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_52", "doc_id": "757", "text": "Article 22: Authorised representatives of providers of high-risk AI systems\n\n1.   Prior to making their high-risk AI systems available on the Union market, providers established in third countries shall, by written mandate, appoint an authorised representative which is established in the Union.\n\n2.   The provider shall enable its authorised representative to perform the tasks specified in the mandate received from the provider.\n\n3.   The authorised representative shall perform the tasks specified in the mandate received from the provider. It shall provide a copy of the mandate to the market surveillance authorities upon request, in one of the official languages of the institutions of the Union, as indicated by the competent authority. For the purposes of this Regulation, the mandate shall empower the authorised representative to carry out the following tasks:\n\n(a)\tverify that the EU declaration of conformity referred to in Article 47 and the technical documentation referred to in Article 11 have been drawn up and that an appropriate conformity assessment procedure has been carried out by the provider;\n\n(b)\tkeep at the disposal of the competent authorities and national authorities or bodies referred to in Article 74(10), for a period of 10 years after the high-risk AI system has been placed on the market or put into service, the contact details of the provider that appointed the authorised representative, a copy of the EU declaration of conformity referred to in Article 47, the technical documentation and, if applicable, the certificate issued by the notified body;", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_53", "doc_id": "757", "text": "(c)\tprovide a competent authority, upon a reasoned request, with all the information and documentation, including that referred to in point (b) of this subparagraph, necessary to demonstrate the conformity of a high-risk AI system with the requirements set out in Section 2, including access to the logs, as referred to in Article 12(1), automatically generated by the high-risk AI system, to the extent such logs are under the control of the provider;\n\n(d)\tcooperate with competent authorities, upon a reasoned request, in any action the latter take in relation to the high-risk AI system, in particular to reduce and mitigate the risks posed by the high-risk AI system;\n\n(e)\twhere applicable, comply with the registration obligations referred to in Article 49(1), or, if the registration is carried out by the provider itself, ensure that the information referred to in point 3 of Section A of Annex VIII is correct.\n\nThe mandate shall empower the authorised representative to be addressed, in addition to or instead of the provider, by the competent authorities, on all issues related to ensuring adherence with this Regulation.\n\n4.   The authorised representative shall terminate the mandate if it considers or has reason to consider the provider to be acting contrary to its obligations pursuant to this Regulation. In such a case, it shall immediately inform the relevant market surveillance authority, as well as, where applicable, the relevant notified body, about the termination of the mandate and the reasons therefor.", "tags": ["Strategies: Licensing, registration, and certification"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_54", "doc_id": "757", "text": "Article 23: Obligations of importers\n\n1.   Before placing a high-risk AI system on the market, importers shall ensure that the system is in conformity with this Regulation by verifying that:\n\n(a)\tthe relevant conformity assessment procedure referred to in Article 43 has been carried out by the provider of the high-risk AI system;\n\n(b)\tthe provider has drawn up the technical documentation in accordance with Article 11 and Annex IV;\n\n(c)\tthe system bears the required CE marking and is accompanied by the EU declaration of conformity referred to in Article 47 and instructions for use;\n\n(d)\tthe provider has appointed an authorised representative in accordance with Article 22(1).\n\n2.   Where an importer has sufficient reason to consider that a high-risk AI system is not in conformity with this Regulation, or is falsified, or accompanied by falsified documentation, it must avoid place the system on the market until it has been brought into conformity. Where the high-risk AI system presents a risk within the meaning of Article 79(1), the importer shall inform the provider of the system, the authorised representative and the market surveillance authorities to that effect.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_55", "doc_id": "757", "text": "3.   Importers shall indicate their name, registered trade name or registered trade mark, and the address at which they can be contacted on the high-risk AI system and on its packaging or its accompanying documentation, where applicable.\n\n4.   Importers shall ensure that, while a high-risk AI system is under their responsibility, storage or transport conditions, where applicable, do not jeopardise its adherence with the requirements set out in Section 2.\n\n5.   Importers shall keep, for a period of 10 years after the high-risk AI system has been placed on the market or put into service, a copy of the certificate issued by the notified body, where applicable, of the instructions for use, and of the EU declaration of conformity referred to in Article 47.\n\n6.   Importers shall provide the relevant competent authorities, upon a reasoned request, with all the necessary information and documentation, including that referred to in paragraph 5, to demonstrate the conformity of a high-risk AI system with the requirements set out in Section 2 in a language which can be easily understood by them. For this purpose, they shall also ensure that the technical documentation can be made available to those authorities.\n\n7.   Importers shall cooperate with the relevant competent authorities in any action those authorities take in relation to a high-risk AI system placed on the market by the importers, in particular to reduce and mitigate the risks posed by it.", "tags": ["Strategies: Licensing, registration, and certification", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: In standard form"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_56", "doc_id": "757", "text": "Article 24: Obligations of distributors\n\n1.   Before making a high-risk AI system available on the market, distributors shall verify that it bears the required CE marking, that it is accompanied by a copy of the EU declaration of conformity referred to in Article 47 and instructions for use, and that the provider and the importer of that system, as applicable, have complied with their respective obligations as laid down in Article 16, points (b) and (c) and Article 23(3).\n\n2.   Where a distributor considers or has reason to consider, on the basis of the information in its possession, that a high-risk AI system is not in conformity with the requirements set out in Section 2, it must avoid make the high-risk AI system available on the market until the system has been brought into conformity with those requirements. Furthermore, where the high-risk AI system presents a risk within the meaning of Article 79(1), the distributor shall inform the provider or the importer of the system, as applicable, to that effect.\n\n3.   Distributors shall ensure that, while a high-risk AI system is under their responsibility, storage or transport conditions, where applicable, do not jeopardise the adherence of the system with the requirements set out in Section 2.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_61", "doc_id": "757", "text": "Article 26: Obligations of deployers of high-risk AI systems\n\n1.   Deployers of high-risk AI systems shall take appropriate technical and organisational measures to ensure they use such systems in accordance with the instructions for use accompanying the systems, pursuant to paragraphs 3 and 6.\n\n2.   Deployers shall assign human oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support.\n\n3.   The obligations set out in paragraphs 1 and 2, are without prejudice to other deployer obligations under Union or national law and to the deployer’s freedom to organise its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider.\n\n4.   Without prejudice to paragraphs 1 and 2, to the extent the deployer exercises control over the input data, that deployer shall ensure that input data is relevant and sufficiently representative in view of the intended purpose of the high-risk AI system.", "tags": ["Strategies: Input controls"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_64", "doc_id": "757", "text": "10.   Without prejudice to Directive (EU) 2016/680, in the framework of an investigation for the targeted search of a person suspected or convicted of having committed a criminal offence, the deployer of a high-risk AI system for post-remote biometric identification shall request an authorisation, ex ante, or without undue delay and no later than 48 hours, by a judicial authority or an administrative authority whose decision is binding and subject to judicial review, for the use of that system, except when it is used for the initial identification of a potential suspect based on objective and verifiable facts directly linked to the offence. Each use shall be limited to what is strictly necessary for the investigation of a specific criminal offence.\n\nIf the authorisation requested pursuant to the first subparagraph is rejected, the use of the post-remote biometric identification system linked to that requested authorisation shall be stopped with immediate effect and the personal data linked to the use of the high-risk AI system for which the authorisation was requested shall be deleted.\n\nIn no case shall such high-risk AI system for post-remote biometric identification be used for law enforcement purposes in an untargeted way, without any link to a criminal offence, a criminal proceeding, a genuine and present or genuine and foreseeable threat of a criminal offence, or the search for a specific missing person. It shall be ensured that no decision that produces an adverse legal effect on a person may be taken by the law enforcement authorities based solely on the output of such post-remote biometric identification systems.\n\nThis paragraph is without prejudice to Article 9 of Regulation (EU) 2016/679 and Article 10 of Directive (EU) 2016/680 for the processing of biometric data.", "tags": ["Applications: Medicine, life sciences and public health", "Applications: Government: judicial and law enforcement"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_65", "doc_id": "757", "text": "Regardless of the purpose or deployer, each use of such high-risk AI systems shall be documented in the relevant police file and shall be made available to the relevant market surveillance authority and the national data protection authority upon request, excluding the disclosure of sensitive operational data related to law enforcement. This subparagraph shall be without prejudice to the powers conferred by Directive (EU) 2016/680 on supervisory authorities.\n\nDeployers shall submit annual reports to the relevant market surveillance and national data protection authorities on their use of post-remote biometric identification systems, excluding the disclosure of sensitive operational data related to law enforcement. The reports may be aggregated to cover more than one deployment.\n\nMember States may introduce, in accordance with Union law, more restrictive laws on the use of post-remote biometric identification systems.", "tags": ["Applications: Government: judicial and law enforcement", "Applications: Medicine, life sciences and public health", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Disclosure: In deployment"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_69", "doc_id": "757", "text": "Article 28: Notifying authorities\n\n1.   Each Member State shall designate or establish at least one notifying authority responsible for setting up and carrying out the necessary procedures for the assessment, designation and notification of conformity assessment bodies and for their monitoring. Those procedures shall be developed in cooperation between the notifying authorities of all Member States.\n\n2.   Member States may decide that the assessment and monitoring referred to in paragraph 1 is to be carried out by a national accreditation body within the meaning of, and in accordance with, Regulation (EC) No 765/2008.\n\n3.   Notifying authorities shall be established, organised and operated in such a way that no conflict of interest arises with conformity assessment bodies, and that the objectivity and impartiality of their activities are safeguarded.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: New institution"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_71", "doc_id": "757", "text": "Article 29: Application of a conformity assessment body for notification\n\n1.   Conformity assessment bodies shall submit an application for notification to the notifying authority of the Member State in which they are established.\n\n2.   The application for notification shall be accompanied by a description of the conformity assessment activities, the conformity assessment module or modules and the types of AI systems for which the conformity assessment body claims to be competent, as well as by an accreditation certificate, where one exists, issued by a national accreditation body attesting that the conformity assessment body fulfils the requirements laid down in Article 31.\n\nAny valid document related to existing designations of the applicant notified body under any other Union harmonisation legislation shall be added.\n\n3.   Where the conformity assessment body concerned cannot provide an accreditation certificate, it shall provide the notifying authority with all the documentary evidence necessary for the verification, recognition and regular monitoring of its adherence with the requirements laid down in Article 31.\n\n4.   For notified bodies which are designated under any other Union harmonisation legislation, all documents and certificates linked to those designations may be used to support their designation procedure under this Regulation, as appropriate. The notified body shall update the documentation referred to in paragraphs 2 and 3 of this Article whenever relevant changes occur, in order to enable the authority responsible for notified bodies to monitor and verify continuous adherence with all the requirements laid down in Article 31.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Licensing, registration, and certification", "Strategies: Performance requirements"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_73", "doc_id": "757", "text": "4.   The conformity assessment body concerned may perform the activities of a notified body only where no objections are raised by the Commission or the other Member States within two weeks of a notification by a notifying authority where it includes an accreditation certificate referred to in Article 29(2), or within two months of a notification by the notifying authority where it includes documentary evidence referred to in Article 29(3).\n\n5.   Where objections are raised, the Commission shall, without delay, enter into consultations with the relevant Member States and the conformity assessment body. In view thereof, the Commission shall decide whether the authorisation is justified. The Commission shall address its decision to the Member State concerned and to the relevant conformity assessment body.", "tags": ["Strategies: Licensing, registration, and certification", "Strategies: Convening"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_76", "doc_id": "757", "text": "9.   Notified bodies shall take out appropriate responsibility insurance for their conformity assessment activities, unless responsibility is assumed by the Member State in which they are established in accordance with national law or that Member State is itself directly responsible for the conformity assessment.\n\n10.   Notified bodies shall be capable of carrying out all their tasks under this Regulation with the highest degree of professional integrity and the requisite competence in the specific field, whether those tasks are carried out by notified bodies themselves or on their behalf and under their responsibility.\n\n11.   Notified bodies shall have sufficient internal competences to be able effectively to evaluate the tasks conducted by external parties on their behalf. The notified body shall have permanent availability of sufficient administrative, technical, legal and scientific personnel who possess experience and knowledge relating to the relevant types of AI systems, data and data computing, and relating to the requirements set out in Section 2.\n\n12.   Notified bodies shall participate in coordination activities as referred to in Article 38. They shall also take part directly, or be represented in, European standardisation organisations, or ensure that they are aware and up to date in respect of relevant standards.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Convening"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_79", "doc_id": "757", "text": "Article 34: Operational obligations of notified bodies\n\n1.   Notified bodies shall verify the conformity of high-risk AI systems in accordance with the conformity assessment procedures set out in Article 43.\n\n2.   Notified bodies shall avoid unnecessary burdens for providers when performing their activities, and take due account of the size of the provider, the sector in which it operates, its structure and the degree of complexity of the high-risk AI system concerned, in particular in view of minimising administrative burdens and adherence costs for micro- and small enterprises within the meaning of Recommendation 2003/361/EC. The notified body shall, nevertheless, respect the degree of rigour and the level of protection required for the adherence of the high-risk AI system with the requirements of this Regulation.\n\n3.   Notified bodies shall make available and submit upon request all relevant documentation, including the providers’ documentation, to the notifying authority referred to in Article 28 to allow that authority to conduct its assessment, designation, notification and monitoring activities, and to facilitate the assessment outlined in this Section.", "tags": ["Strategies: Tiering", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_81", "doc_id": "757", "text": "Article 36: Changes to notifications\n\n1.   The notifying authority shall notify the Commission and the other Member States of any relevant changes to the notification of a notified body via the electronic notification tool referred to in Article 30(2).\n\n2.   The procedures laid down in Articles 29 and 30 shall apply to extensions of the scope of the notification.\n\nFor changes to the notification other than extensions of its scope, the procedures laid down in paragraphs (3) to (9) shall apply.\n\n3.   Where a notified body decides to cease its conformity assessment activities, it shall inform the notifying authority and the providers concerned as soon as possible and, in the case of a planned cessation, at least one year before ceasing its activities. The certificates of the notified body may remain valid for a period of nine months after cessation of the notified body’s activities, on condition that another notified body has confirmed in writing that it will assume responsibilities for the high-risk AI systems covered by those certificates. The latter notified body shall complete a full assessment of the high-risk AI systems affected by the end of that nine-month-period before issuing new certificates for those systems. Where the notified body has ceased its activity, the notifying authority shall withdraw the designation.", "tags": ["Strategies: Licensing, registration, and certification"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_84", "doc_id": "757", "text": "8.   With the exception of certificates unduly issued, and where a designation has been suspended or restricted, the certificates shall remain valid in one of the following circumstances:\n\n(a)\tthe notifying authority has confirmed, within one month of the suspension or limitation, that there is no risk to health, safety or fundamental rights in relation to certificates affected by the suspension or limitation, and the notifying authority has outlined a timeline for actions to remedy the suspension or limitation; or\n\n(b)\tthe notifying authority has confirmed that no certificates relevant to the suspension will be issued, amended or re-issued during the course of the suspension or limitation, and states whether the notified body has the capability of continuing to monitor and remain responsible for existing certificates issued for the period of the suspension or limitation; in the event that the notifying authority determines that the notified body does not have the capability to support existing certificates issued, the provider of the system covered by the certificate shall confirm in writing to the national competent authorities of the Member State in which it has its registered place of business, within three months of the suspension or limitation, that another qualified notified body is temporarily assuming the functions of the notified body to monitor and remain responsible for the certificates during the period of suspension or limitation.", "tags": ["Strategies: Licensing, registration, and certification", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_92", "doc_id": "757", "text": "Article 42: Presumption of conformity with certain requirements\n\n1.   High-risk AI systems that have been trained and tested on data reflecting the specific geographical, behavioural, contextual or functional setting within which they are intended to be used shall be presumed to comply with the relevant requirements laid down in Article 10(4).\n\n2.   High-risk AI systems that have been certified or for which a statement of conformity has been issued under a cybersecurity scheme pursuant to Regulation (EU) 2019/881 and the references of which have been published in the Official Journal of the European Union shall be presumed to comply with the cybersecurity requirements set out in Article 15 of this Regulation in so far as the cybersecurity certificate or statement of conformity or parts thereof cover those requirements.", "tags": ["Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_93", "doc_id": "757", "text": "Article 43: Conformity assessment\n\n1.   For high-risk AI systems listed in point 1 of Annex III, where, in demonstrating the adherence of a high-risk AI system with the requirements set out in Section 2, the provider has applied harmonised standards referred to in Article 40, or, where applicable, common specifications referred to in Article 41, the provider shall opt for one of the following conformity assessment procedures based on:\n\n(a)\tthe internal control referred to in Annex VI; or\n\n(b)\tthe assessment of the quality management system and the assessment of the technical documentation, with the involvement of a notified body, referred to in Annex VII.\n\nIn demonstrating the adherence of a high-risk AI system with the requirements set out in Section 2, the provider shall follow the conformity assessment procedure set out in Annex VII where:\n\n(a)\tharmonised standards referred to in Article 40 do not exist, and common specifications referred to in Article 41 are not available;\n\n(b)\tthe provider has not applied, or has applied only part of, the harmonised standard;\n\n(c)\tthe common specifications referred to in point (a) exist, but the provider has not applied them;\n\n(d)\tone or more of the harmonised standards referred to in point (a) has been published with a limitation, and only on the part of the standard that was restricted.\n\nFor the purposes of the conformity assessment procedure referred to in Annex VII, the provider may choose any of the notified bodies. However, where the high-risk AI system is intended to be put into service by law enforcement, immigration or asylum authorities or by Union institutions, bodies, offices or agencies, the market surveillance authority referred to in Article 74(8) or (9), as applicable, shall act as a notified body.", "tags": ["Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Applications: Government: judicial and law enforcement", "Applications: Government: other applications/unspecified"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_96", "doc_id": "757", "text": "Article 44: Certificates\n\n1.   Certificates issued by notified bodies in accordance with Annex VII shall be drawn-up in a language which can be easily understood by the relevant authorities in the Member State in which the notified body is established.\n\n2.   Certificates shall be valid for the period they indicate, which must avoid exceed five years for AI systems covered by Annex I, and four years for AI systems covered by Annex III. At the request of the provider, the validity of a certificate may be extended for further periods, each not exceeding five years for AI systems covered by Annex I, and four years for AI systems covered by Annex III, based on a re-assessment in accordance with the applicable conformity assessment procedures. Any supplement to a certificate shall remain valid, provided that the certificate which it supplements is valid.\n\n3.   Where a notified body finds that an AI system no longer meets the requirements set out in Section 2, it shall, taking account of the principle of proportionality, suspend or withdraw the certificate issued or impose restrictions on it, unless adherence with those requirements is ensured by appropriate corrective action taken by the provider of the system within an appropriate deadline set by the notified body. The notified body shall give reasons for its decision.\n\nAn appeal procedure against decisions of the notified bodies, including on conformity certificates issued, shall be available.", "tags": ["Strategies: Licensing, registration, and certification", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_98", "doc_id": "757", "text": "Article 46: Derogation from conformity assessment procedure\n\n1.   By way of derogation from Article 43 and upon a duly justified request, any market surveillance authority may authorise the placing on the market or the putting into service of specific high-risk AI systems within the territory of the Member State concerned, for exceptional reasons of public security or the protection of life and health of persons, environmental protection or the protection of key industrial and infrastructural assets. That authorisation shall be for a limited period while the necessary conformity assessment procedures are being carried out, taking into account the exceptional reasons justifying the derogation. The completion of those procedures shall be undertaken without undue delay.\n\n2.   In a duly justified situation of urgency for exceptional reasons of public security or in the case of specific, substantial and imminent threat to the life or physical safety of natural persons, law-enforcement authorities or civil protection authorities may put a specific high-risk AI system into service without the authorisation referred to in paragraph 1, provided that such authorisation is requested during or after the use without undue delay. If the authorisation referred to in paragraph 1 is refused, the use of the high-risk AI system shall be stopped with immediate effect and all the results and outputs of such use shall be immediately discarded.\n\n3.   The authorisation referred to in paragraph 1 shall be issued only if the market surveillance authority concludes that the high-risk AI system complies with the requirements of Section 2. The market surveillance authority shall inform the Commission and the other Member States of any authorisation issued pursuant to paragraphs 1 and 2. This obligation must avoid cover sensitive operational data in relation to the activities of law-enforcement authorities.\n\n4.   Where, within 15 calendar days of receipt of the information referred to in paragraph 3, no objection has been raised by either a Member State or the Commission in respect of an authorisation issued by a market surveillance authority of a Member State in accordance with paragraph 1, that authorisation shall be deemed justified.", "tags": ["Applications: Government: military and public safety", "Harms: Harm to infrastructure", "Harms: Ecological harm", "Harms: Harm to health/safety", "Applications: Government: judicial and law enforcement"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_101", "doc_id": "757", "text": "Article 48: CE marking\n\n1.   The CE marking shall be subject to the general principles set out in Article 30 of Regulation (EC) No 765/2008.\n\n2.   For high-risk AI systems provided digitally, a digital CE marking shall be used, only if it can easily be accessed via the interface from which that system is accessed or via an easily accessible machine-readable code or other electronic means.\n\n3.   The CE marking shall be affixed visibly, legibly and indelibly for high-risk AI systems. Where that is not possible or not warranted on account of the nature of the high-risk AI system, it shall be affixed to the packaging or to the accompanying documentation, as appropriate.\n\n4.   Where applicable, the CE marking shall be followed by the identification number of the notified body responsible for the conformity assessment procedures set out in Article 43. The identification number of the notified body shall be affixed by the body itself or, under its instructions, by the provider or by the provider’s authorised representative. The identification number shall also be indicated in any promotional material which mentions that the high-risk AI system fulfils the requirements for CE marking.\n\n5.   Where high-risk AI systems are subject to other Union law which also provides for the affixing of the CE marking, the CE marking shall indicate that the high-risk AI system also fulfil the requirements of that other law.", "tags": ["Strategies: Licensing, registration, and certification"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_102", "doc_id": "757", "text": "Article 49: Registration\n\n1.   Before placing on the market or putting into service a high-risk AI system listed in Annex III, with the exception of high-risk AI systems referred to in point 2 of Annex III, the provider or, where applicable, the authorised representative shall register themselves and their system in the EU database referred to in Article 71.\n\n2.   Before placing on the market or putting into service an AI system for which the provider has concluded that it is not high-risk according to Article 6(3), that provider or, where applicable, the authorised representative shall register themselves and that system in the EU database referred to in Article 71.\n\n3.   Before putting into service or using a high-risk AI system listed in Annex III, with the exception of high-risk AI systems listed in point 2 of Annex III, deployers that are public authorities, Union institutions, bodies, offices or agencies or persons acting on their behalf shall register themselves, select the system and register its use in the EU database referred to in Article 71.", "tags": ["Strategies: Licensing, registration, and certification"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_103", "doc_id": "757", "text": "4.   For high-risk AI systems referred to in points 1, 6 and 7 of Annex III, in the areas of law enforcement, migration, asylum and border control management, the registration referred to in paragraphs 1, 2 and 3 of this Article shall be in a secure non-public section of the EU database referred to in Article 71 and shall include only the following information, as applicable, referred to in:\n\n(a)\tSection A, points 1 to 10, of Annex VIII, with the exception of points 6, 8 and 9;\n\n(b)\tSection B, points 1 to 5, and points 8 and 9 of Annex VIII;\n\n(c)\tSection C, points 1 to 3, of Annex VIII;\n\n(d)\tpoints 1, 2, 3 and 5, of Annex IX.\n\nOnly the Commission and national authorities referred to in Article 74(8) shall have access to the respective restricted sections of the EU database listed in the first subparagraph of this paragraph.\n\n5.   High-risk AI systems referred to in point 2 of Annex III shall be registered at national level.", "tags": ["Strategies: Licensing, registration, and certification"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_104", "doc_id": "757", "text": "CHAPTER IV: TRANSPARENCY OBLIGATIONS FOR PROVIDERS AND DEPLOYERS OF CERTAIN AI SYSTEMS\n\nArticle 50: Transparency obligations for providers and deployers of certain AI systems\n\n1.   Providers shall ensure that AI systems intended to interact directly with natural persons are designed and developed in such a way that the natural persons concerned are informed that they are interacting with an AI system, unless this is obvious from the point of view of a natural person who is reasonably well-informed, observant and circumspect, taking into account the circumstances and the context of use. This obligation must avoid apply to AI systems authorised by law to detect, prevent, investigate or prosecute criminal offences, subject to appropriate safeguards for the rights and freedoms of third parties, unless those systems are available for the public to report a criminal offence.\n\n2.   Providers of AI systems, including general-purpose AI systems, generating synthetic audio, image, video or text content, shall ensure that the outputs of the AI system are marked in a machine-readable format and detectable as artificially generated or manipulated. Providers shall ensure their technical solutions are effective, interoperable, robust and reliable as far as this is technically feasible, taking into account the specificities and limitations of various types of content, the costs of implementation and the generally acknowledged state of the art, as may be reflected in relevant technical standards. This obligation must avoid apply to the extent the AI systems perform an assistive function for standard editing or do not substantially alter the input data provided by the deployer or the semantics thereof, or where authorised by law to detect, prevent, investigate or prosecute criminal offences.\n\n3.   Deployers of an emotion recognition system or a biometric categorisation system shall inform the natural persons exposed thereto of the operation of the system, and shall process the personal data in accordance with Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, as applicable. This obligation must avoid apply to AI systems used for biometric categorisation and emotion recognition, which are permitted by law to detect, prevent or investigate criminal offences, subject to appropriate safeguards for the rights and freedoms of third parties, and in accordance with Union law.", "tags": ["Risk factors: Transparency", "Applications: Government: judicial and law enforcement", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Risk factors: Reliability", "Risk factors: Reliability: Robustness", "Strategies: Tiering", "Strategies: Tiering: Tiering based on inputs", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Disclosure: In deployment"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_107", "doc_id": "757", "text": "Article 52: Procedure\n\n1.   Where a general-purpose AI model meets the condition referred to in Article 51(1), point (a), the relevant provider shall notify the Commission without delay and in any event within two weeks after that requirement is met or it becomes known that it will be met. That notification shall include the information necessary to demonstrate that the relevant requirement has been met. If the Commission becomes aware of a general-purpose AI model presenting systemic risks of which it has not been notified, it may decide to designate it as a model with systemic risk.\n\n2.   The provider of a general-purpose AI model that meets the condition referred to in Article 51(1), point (a), may present, with its notification, sufficiently substantiated arguments to demonstrate that, exceptionally, although it meets that requirement, the general-purpose AI model does not present, due to its specific characteristics, systemic risks and therefore should not be classified as a general-purpose AI model with systemic risk.\n\n3.   Where the Commission concludes that the arguments submitted pursuant to paragraph 2 are not sufficiently substantiated and the relevant provider was not able to demonstrate that the general-purpose AI model does not present, due to its specific characteristics, systemic risks, it shall reject those arguments, and the general-purpose AI model shall be considered to be a general-purpose AI model with systemic risk.", "tags": ["Strategies: Licensing, registration, and certification", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In deployment"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_123", "doc_id": "757", "text": "(f)\tthat AI regulatory sandboxes facilitate the involvement of other relevant actors within the AI ecosystem, such as notified bodies and standardisation organisations, SMEs, including start-ups, enterprises, innovators, testing and experimentation facilities, research and experimentation labs and European Digital advancement Hubs, centres of excellence, individual researchers, in order to allow and facilitate cooperation with the public and private sectors;\n\n(g)\tthat procedures, processes and administrative requirements for application, selection, participation and exiting the AI regulatory sandbox are simple, easily intelligible, and clearly communicated in order to facilitate the participation of SMEs, including start-ups, with limited legal and administrative capacities and are streamlined across the Union, in order to avoid fragmentation and that participation in an AI regulatory sandbox established by a Member State, or by the European Data Protection Supervisor is mutually and uniformly recognised and carries the same legal effects across the Union;\n\n(h)\tthat participation in the AI regulatory sandbox is limited to a period that is appropriate to the complexity and scale of the project and that may be extended by the national competent authority;\n\n(i)\tthat AI regulatory sandboxes facilitate the development of tools and infrastructure for testing, benchmarking, assessing and explaining dimensions of AI systems relevant for regulatory learning, such as accuracy, robustness and cybersecurity, as well as measures to mitigate risks to fundamental rights and society at large.", "tags": ["Risk factors: Interpretability and explainability", "Risk factors: Reliability", "Risk factors: Reliability: Robustness", "Risk factors: Security", "Risk factors: Security: Cybersecurity"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_125", "doc_id": "757", "text": "Article 59: Further processing of personal data for developing certain AI systems in the public interest in the AI regulatory sandbox\n\n1.   In the AI regulatory sandbox, personal data lawfully collected for other purposes may be processed solely for the purpose of developing, training and testing certain AI systems in the sandbox when all of the following conditions are met:\n\n(a)\tAI systems shall be developed for safeguarding substantial public interest by a public authority or another natural or legal person and in one or more of the following areas:\n\n(i)\tpublic safety and public health, including disease detection, diagnosis prevention, control and treatment and improvement of health care systems;\n\n(ii)\ta high level of protection and improvement of the quality of the environment, protection of biodiversity, protection against pollution, green transition measures, climate change mitigation and adaptation measures;\n\n(iii)\tenergy sustainability;\n\n(iv)\tsafety and resilience of transport systems and mobility, critical infrastructure and networks;\n\n(v)\tefficiency and quality of public administration and public services;", "tags": ["Applications: Medicine, life sciences and public health", "Applications: Agriculture and resource extraction", "Applications: Energy and utilities", "Applications: Transportation", "Applications: Construction and field services", "Risk factors: Reliability", "Applications: Government: benefits and welfare", "Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_126", "doc_id": "757", "text": "(b)\tthe data processed are necessary for complying with one or more of the requirements referred to in Chapter III, Section 2 where those requirements cannot effectively be fulfilled by processing anonymised, synthetic or other non-personal data;\n\n(c)\tthere are effective monitoring mechanisms to identify if any high risks to the rights and freedoms of the data subjects, as referred to in Article 35 of Regulation (EU) 2016/679 and in Article 39 of Regulation (EU) 2018/1725, may arise during the sandbox experimentation, as well as response mechanisms to promptly mitigate those risks and, where necessary, stop the processing;\n\n(d)\tany personal data to be processed in the context of the sandbox are in a functionally separate, isolated and protected data processing environment under the control of the prospective provider and only authorised persons have access to those data;\n\n(e)\tproviders can further share the originally collected data only in accordance with Union data protection law; any personal data created in the sandbox cannot be shared outside the sandbox;\n\n(f)\tany processing of personal data in the context of the sandbox neither leads to measures or decisions affecting the data subjects nor does it affect the application of their rights laid down in Union law on the protection of personal data;", "tags": ["Strategies: Input controls", "Strategies: Input controls: Data circulation", "Strategies: Input controls: Data use"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_128", "doc_id": "757", "text": "2.   For the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal consequences, including safeguarding against and preventing threats to public security, under the control and responsibility of law enforcement authorities, the processing of personal data in AI regulatory sandboxes shall be based on a specific Union or national law and subject to the same cumulative conditions as referred to in paragraph 1.\n\n3.   Paragraph 1 is without prejudice to Union or national law which excludes processing of personal data for other purposes than those explicitly mentioned in that law, as well as to Union or national law laying down the basis for the processing of personal data which is necessary for the purpose of developing, testing or training of innovative AI systems or any other legal basis, in adherence with Union law on the protection of personal data.", "tags": ["Applications: Government: judicial and law enforcement", "Applications: Government: military and public safety"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_130", "doc_id": "757", "text": "4.   Providers or prospective providers may conduct the testing in real world conditions only where all of the following conditions are met:\n\n(a)\tthe provider or prospective provider has drawn up a real-world testing plan and submitted it to the market surveillance authority in the Member State where the testing in real world conditions is to be conducted;\n\n(b)\tthe market surveillance authority in the Member State where the testing in real world conditions is to be conducted has approved the testing in real world conditions and the real-world testing plan; where the market surveillance authority has not provided an answer within 30 days, the testing in real world conditions and the real-world testing plan shall be understood to have been approved; where national law does not provide for a tacit approval, the testing in real world conditions shall remain subject to an authorisation;\n\n(c)\tthe provider or prospective provider, with the exception of providers or prospective providers of high-risk AI systems referred to in points 1, 6 and 7 of Annex III in the areas of law enforcement, migration, asylum and border control management, and high-risk AI systems referred to in point 2 of Annex III has registered the testing in real world conditions in accordance with Article 71(4) with a Union-wide unique single identification number and with the information specified in Annex IX; the provider or prospective provider of high-risk AI systems referred to in points 1, 6 and 7 of Annex III in the areas of law enforcement, migration, asylum and border control management, has registered the testing in real-world conditions in the secure non-public section of the EU database according to Article 49(4), point (d), with a Union-wide unique single identification number and with the information specified therein; the provider or prospective provider of high-risk AI systems referred to in point 2 of Annex III has registered the testing in real-world conditions in accordance with Article 49(5);\n\n(d)\tthe provider or prospective provider conducting the testing in real world conditions is established in the Union or has appointed a legal representative who is established in the Union;", "tags": ["Strategies: Licensing, registration, and certification", "Strategies: Disclosure"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_131", "doc_id": "757", "text": "(e)\tdata collected and processed for the purpose of the testing in real world conditions shall be transferred to third countries only provided that appropriate and applicable safeguards under Union law are implemented;\n\n(f)\tthe testing in real world conditions does not last longer than necessary to achieve its objectives and in any case not longer than six months, which may be extended for an additional period of six months, subject to prior notification by the provider or prospective provider to the market surveillance authority, accompanied by an explanation of the need for such an extension;\n\n(g)\tthe subjects of the testing in real world conditions who are persons belonging to vulnerable groups due to their age or disability, are appropriately protected;\n\n(h)\twhere a provider or prospective provider organises the testing in real world conditions in cooperation with one or more deployers or prospective deployers, the latter have been informed of all aspects of the testing that are relevant to their decision to participate, and given the relevant instructions for use of the AI system referred to in Article 13; the provider or prospective provider and the deployer or prospective deployer shall conclude an agreement specifying their roles and responsibilities with a view to ensuring adherence with the provisions for testing in real world conditions under this Regulation and under other applicable Union and national law;", "tags": ["Strategies: Input controls", "Strategies: Input controls: Data circulation", "Strategies: Disclosure"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_132", "doc_id": "757", "text": "(i)\tthe subjects of the testing in real world conditions have given informed consent in accordance with Article 61, or in the case of law enforcement, where the seeking of informed consent would prevent the AI system from being tested, the testing itself and the outcome of the testing in the real world conditions must avoid have any negative effect on the subjects, and their personal data shall be deleted after the test is performed;\n\n(j)\tthe testing in real world conditions is effectively overseen by the provider or prospective provider, as well as by deployers or prospective deployers through persons who are suitably qualified in the relevant field and have the necessary capacity, training and authority to perform their tasks;\n\n(k)\tthe predictions, recommendations or decisions of the AI system can be effectively reversed and disregarded.", "tags": ["Applications: Government: judicial and law enforcement", "Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_170", "doc_id": "757", "text": "Article 78: Confidentiality\n\n1.   The Commission, market surveillance authorities and notified bodies and any other natural or legal person involved in the application of this Regulation shall, in accordance with Union or national law, respect the confidentiality of information and data obtained in carrying out their tasks and activities in such a manner as to protect, in particular:\n\n(a)\tthe intellectual property rights and confidential business information or trade secrets of a natural or legal person, including source code, except in the cases referred to in Article 5 of Directive (EU) 2016/943 of the European Parliament and of the Council (57);\n\n(b)\tthe effective implementation of this Regulation, in particular for the purposes of inspections, investigations or reviews;\n\n(c)\tpublic and national security interests;\n\n(d)\tthe conduct of criminal or administrative proceedings;\n\n(e)\tinformation classified pursuant to Union or national law.\n\n2.   The authorities involved in the application of this Regulation pursuant to paragraph 1 shall request only data that is strictly necessary for the assessment of the risk posed by AI systems and for the exercise of their powers in accordance with this Regulation and with Regulation (EU) 2019/1020. They shall put in place adequate and effective cybersecurity measures to protect the security and confidentiality of the information and data obtained, and shall delete the data collected as soon as it is no longer needed for the purpose for which it was obtained, in accordance with applicable Union or national law.", "tags": ["Risk factors: Security", "Applications: Government: military and public safety", "Applications: Government: judicial and law enforcement", "Risk factors: Security: Cybersecurity", "Strategies: Input controls", "Strategies: Input controls: Data use"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_175", "doc_id": "757", "text": "Article 80: Procedure for dealing with AI systems classified by the provider as non-high-risk in application of Annex III\n\n1.   Where a market surveillance authority has sufficient reason to consider that an AI system classified by the provider as non-high-risk pursuant to Article 6(3) is indeed high-risk, the market surveillance authority shall carry out an evaluation of the AI system concerned in respect of its classification as a high-risk AI system based on the conditions set out in Article 6(3) and the Commission guidelines.\n\n2.   Where, in the course of that evaluation, the market surveillance authority finds that the AI system concerned is high-risk, it shall without undue delay require the relevant provider to take all necessary actions to bring the AI system into adherence with the requirements and obligations laid down in this Regulation, as well as take appropriate corrective action within a period the market surveillance authority may prescribe.\n\n3.   Where the market surveillance authority considers that the use of the AI system concerned is not restricted to its national territory, it shall inform the Commission and the other Member States without undue delay of the results of the evaluation and of the actions which it has required the provider to take.\n\n4.   The provider shall ensure that all necessary action is taken to bring the AI system into adherence with the requirements and obligations laid down in this Regulation. Where the provider of an AI system concerned does not bring the AI system into adherence with those requirements and obligations within the period referred to in paragraph 2 of this Article, the provider shall be subject to charges in accordance with Article 99.", "tags": ["Strategies: Evaluation", "Strategies: Tiering", "Strategies: Tiering: Tiering based on impact", "Incentives: Fines"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_176", "doc_id": "757", "text": "5.   The provider shall ensure that all appropriate corrective action is taken in respect of all the AI systems concerned that it has made available on the Union market.\n\n6.   Where the provider of the AI system concerned does not take adequate corrective action within the period referred to in paragraph 2 of this Article, Article 79(5) to (9) shall apply.\n\n7.   Where, in the course of the evaluation pursuant to paragraph 1 of this Article, the market surveillance authority establishes that the AI system was misclassified by the provider as non-high-risk in order to circumvent the application of requirements in Chapter III, Section 2, the provider shall be subject to charges in accordance with Article 99.\n\n8.   In exercising their power to monitor the application of this Article, and in accordance with Article 11 of Regulation (EU) 2019/1020, market surveillance authorities may perform appropriate checks, taking into account in particular information stored in the EU database referred to in Article 71 of this Regulation.", "tags": ["Incentives: Fines"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_179", "doc_id": "757", "text": "Article 83: Formal non-adherence\n\n1.   Where the market surveillance authority of a Member State makes one of the following findings, it shall require the relevant provider to put an end to the non-adherence concerned, within a period it may prescribe:\n\n(a)\tthe CE marking has been affixed in infraction of Article 48;\n\n(b)\tthe CE marking has not been affixed;\n\n(c)\tthe EU declaration of conformity referred to in Article 47 has not been drawn up;\n\n(d)\tthe EU declaration of conformity referred to in Article 47 has not been drawn up correctly;\n\n(e)\tthe registration in the EU database referred to in Article 71 has not been carried out;\n\n(f)\twhere applicable, no authorised representative has been appointed;\n\n(g)\ttechnical documentation is not available.\n\n2.   Where the non-adherence referred to in paragraph 1 persists, the market surveillance authority of the Member State concerned shall take appropriate and proportionate measures to restrict or restrict the high-risk AI system being made available on the market or to ensure that it is recalled or withdrawn from the market without delay.", "tags": ["Strategies: Licensing, registration, and certification"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_187", "doc_id": "757", "text": "Article 91: Power to request documentation and information\n\n1.   The Commission may request the provider of the general-purpose AI model concerned to provide the documentation drawn up by the provider in accordance with Articles 53 and 55, or any additional information that is necessary for the purpose of assessing adherence of the provider with this Regulation.\n\n2.   Before sending the request for information, the AI Office may initiate a structured dialogue with the provider of the general-purpose AI model.\n\n3.   Upon a duly substantiated request from the scientific panel, the Commission may issue a request for information to a provider of a general-purpose AI model, where the access to information is necessary and proportionate for the fulfilment of the tasks of the scientific panel under Article 68(2).\n\n4.   The request for information shall state the legal basis and the purpose of the request, specify what information is required, set a period within which the information is to be provided, and indicate the charges provided for in Article 101 for supplying incorrect, incomplete or misleading information.\n\n5.   The provider of the general-purpose AI model concerned, or its representative shall supply the information requested. In the case of legal persons, companies or firms, or where the provider has no legal personality, the persons authorised to represent them by law or by their statutes, shall supply the information requested on behalf of the provider of the general-purpose AI model concerned. Lawyers duly authorised to act may supply information on behalf of their clients. The clients shall nevertheless remain fully responsible if the information supplied is incomplete, incorrect or misleading.", "tags": ["Strategies: Convening", "Incentives: Fines"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_188", "doc_id": "757", "text": "Article 92: Power to conduct evaluations\n\n1.   The AI Office, after consulting the Board, may conduct evaluations of the general-purpose AI model concerned:\n\n(a)\tto assess adherence of the provider with obligations under this Regulation, where the information gathered pursuant to Article 91 is insufficient; or\n\n(b)\tto investigate systemic risks at Union level of general-purpose AI models with systemic risk, in particular following a qualified alert from the scientific panel in accordance with Article 90(1), point (a).\n\n2.   The Commission may decide to appoint independent experts to carry out evaluations on its behalf, including from the scientific panel established pursuant to Article 68. Independent experts appointed for this task shall meet the criteria outlined in Article 68(2).\n\n3.   For the purposes of paragraph 1, the Commission may request access to the general-purpose AI model concerned through APIs or further appropriate technical means and tools, including source code.\n\n4.   The request for access shall state the legal basis, the purpose and reasons of the request and set the period within which the access is to be provided, and the charges provided for in Article 101 for failure to provide access.", "tags": ["Strategies: Evaluation", "Strategies: Convening", "Incentives: Fines"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_199", "doc_id": "757", "text": "CHAPTER XII: consequences\n\nArticle 99: consequences\n\n1.   In accordance with the terms and conditions laid down in this Regulation, Member States shall lay down the rules on consequences and other enforcement measures, which may also include warnings and non-monetary measures, applicable to infringements of this Regulation by operators, and shall take all measures necessary to ensure that they are properly and effectively implemented, thereby taking into account the guidelines issued by the Commission pursuant to Article 96. The consequences provided for shall be effective, proportionate and dissuasive. They shall take into account the interests of SMEs, including start-ups, and their economic viability.\n\n2.   The Member States shall, without delay and at the latest by the date of entry into application, notify the Commission of the rules on consequences and of other enforcement measures referred to in paragraph 1, and shall notify it, without delay, of any subsequent amendment to them.\n\n3.   Non-adherence with the prohibition of the AI practices referred to in Article 5 shall be subject to administrative charges of up to EUR 35 000 000 or, if the offender is an undertaking, up to 7 % of its total worldwide annual turnover for the preceding financial year, whichever is higher.", "tags": ["Incentives: Fines"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_200", "doc_id": "757", "text": "4.   Non-adherence with any of the following provisions related to operators or notified bodies, other than those laid down in Articles 5, shall be subject to administrative charges of up to EUR 15 000 000 or, if the offender is an undertaking, up to 3 % of its total worldwide annual turnover for the preceding financial year, whichever is higher:\n\n(a)\tobligations of providers pursuant to Article 16;\n\n(b)\tobligations of authorised representatives pursuant to Article 22;\n\n(c)\tobligations of importers pursuant to Article 23;\n\n(d)\tobligations of distributors pursuant to Article 24;\n\n(e)\tobligations of deployers pursuant to Article 26;\n\n(f)\trequirements and obligations of notified bodies pursuant to Article 31, Article 33(1), (3) and (4) or Article 34;\n\n(g)\ttransparency obligations for providers and deployers pursuant to Article 50.\n\n5.   The supply of incorrect, incomplete or misleading information to notified bodies or national competent authorities in reply to a request shall be subject to administrative charges of up to EUR 7 500 000 or, if the offender is an undertaking, up to 1 % of its total worldwide annual turnover for the preceding financial year, whichever is higher.\n\n6.   In the case of SMEs, including start-ups, each charge referred to in this Article shall be up to the percentages or amount referred to in paragraphs 3, 4 and 5, whichever thereof is lower.", "tags": ["Incentives: Fines", "Risk factors: Transparency"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_201", "doc_id": "757", "text": "7.   When deciding whether to impose an administrative charge and when deciding on the amount of the administrative charge in each individual case, all relevant circumstances of the specific situation shall be taken into account and, as appropriate, regard shall be given to the following:\n\n(a)\tthe nature, gravity and duration of the infringement and of its consequences, taking into account the purpose of the AI system, as well as, where appropriate, the number of affected persons and the level of damage suffered by them;\n\n(b)\twhether administrative charges have already been applied by other market surveillance authorities to the same operator for the same infringement;\n\n(c)\twhether administrative charges have already been applied by other authorities to the same operator for infringements of other Union or national law, when such infringements result from the same activity or omission constituting a relevant infringement of this Regulation;\n\n(d)\tthe size, the annual turnover and market share of the operator committing the infringement;\n\n(e)\tany other aggravating or mitigating factor applicable to the circumstances of the case, such as financial benefits gained, or losses avoided, directly or indirectly, from the infringement;\n\n(f)\tthe degree of cooperation with the national competent authorities, in order to remedy the infringement and mitigate the possible adverse effects of the infringement;\n\n(g)\tthe degree of responsibility of the operator taking into account the technical and organisational measures implemented by it;\n\n(h)\tthe manner in which the infringement became known to the national competent authorities, in particular whether, and if so to what extent, the operator notified the infringement;\n\n(i)\tthe intentional or negligent character of the infringement;\n\n(j)\tany action taken by the operator to mitigate the harm suffered by the affected persons.", "tags": ["Strategies: Tiering", "Strategies: Tiering: Tiering based on impact", "Strategies: Disclosure", "Strategies: Convening", "Incentives: Fines"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_203", "doc_id": "757", "text": "Article 100: Administrative charges on Union institutions, bodies, offices and agencies\n\n1.   The European Data Protection Supervisor may impose administrative charges on Union institutions, bodies, offices and agencies falling within the scope of this Regulation. When deciding whether to impose an administrative charge and when deciding on the amount of the administrative charge in each individual case, all relevant circumstances of the specific situation shall be taken into account and due regard shall be given to the following:\n\n(a)\tthe nature, gravity and duration of the infringement and of its consequences, taking into account the purpose of the AI system concerned, as well as, where appropriate, the number of affected persons and the level of damage suffered by them;\n\n(b)\tthe degree of responsibility of the Union institution, body, office or agency, taking into account technical and organisational measures implemented by them;\n\n(c)\tany action taken by the Union institution, body, office or agency to mitigate the damage suffered by affected persons;\n\n(d)\tthe degree of cooperation with the European Data Protection Supervisor in order to remedy the infringement and mitigate the possible adverse effects of the infringement, including adherence with any of the measures previously ordered by the European Data Protection Supervisor against the Union institution, body, office or agency concerned with regard to the same subject matter;\n\n(e)\tany similar previous infringements by the Union institution, body, office or agency;\n\n(f)\tthe manner in which the infringement became known to the European Data Protection Supervisor, in particular whether, and if so to what extent, the Union institution, body, office or agency notified the infringement;\n\n(g)\tthe annual budget of the Union institution, body, office or agency.", "tags": ["Incentives: Fines", "Strategies: Tiering", "Strategies: Tiering: Tiering based on impact", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Convening", "Strategies: Disclosure"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_204", "doc_id": "757", "text": "2.   Non-adherence with the prohibition of the AI practices referred to in Article 5 shall be subject to administrative charges of up to EUR 1 500 000.\n\n3.   The non-adherence of the AI system with any requirements or obligations under this Regulation, other than those laid down in Article 5, shall be subject to administrative charges of up to EUR 750 000.\n\n4.   Before taking decisions pursuant to this Article, the European Data Protection Supervisor shall give the Union institution, body, office or agency which is the subject of the proceedings conducted by the European Data Protection Supervisor the opportunity of being heard on the matter regarding the possible infringement. The European Data Protection Supervisor shall base his or her decisions only on elements and circumstances on which the parties concerned have been able to comment. Complainants, if any, shall be associated closely with the proceedings.\n\n5.   The rights of defence of the parties concerned shall be fully respected in the proceedings. They shall be entitled to have access to the European Data Protection Supervisor’s file, subject to the legitimate interest of individuals or undertakings in the protection of their personal data or business secrets.\n\n6.   Funds collected by imposition of charges in this Article shall contribute to the general budget of the Union. The charges must avoid affect the effective operation of the Union institution, body, office or agency fined.\n\n7.   The European Data Protection Supervisor shall, on an annual basis, notify the Commission of the administrative charges it has imposed pursuant to this Article and of any litigation or judicial proceedings it has initiated.", "tags": ["Incentives: Fines", "Risk factors: Transparency"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_205", "doc_id": "757", "text": "Article 101: charges for providers of general-purpose AI models\n\n1.   The Commission may impose on providers of general-purpose AI models charges not exceeding 3 % of their annual total worldwide turnover in the preceding financial year or EUR 15 000 000, whichever is higher., when the Commission finds that the provider intentionally or negligently:\n\n(a)\tinfringed the relevant provisions of this Regulation;\n\n(b)\tfailed to comply with a request for a document or for information pursuant to Article 91, or supplied incorrect, incomplete or misleading information;\n\n(c)\tfailed to comply with a measure requested under Article 93;\n\n(d)\tfailed to make available to the Commission access to the general-purpose AI model or general-purpose AI model with systemic risk with a view to conducting an evaluation pursuant to Article 92.\n\nIn fixing the amount of the charge or periodic consequence payment, regard shall be had to the nature, gravity and duration of the infringement, taking due account of the principles of proportionality and appropriateness. The Commission shall also into account commitments made in accordance with Article 93(3) or made in relevant codes of practice in accordance with Article 56.\n\n2.   Before adopting the decision pursuant to paragraph 1, the Commission shall communicate its preliminary findings to the provider of the general-purpose AI model and give it an opportunity to be heard.", "tags": ["Incentives: Fines"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_227", "doc_id": "757", "text": "5.\tAccess to and enjoyment of essential private services and essential public services and benefits:\n\n(a)\tAI systems intended to be used by public authorities or on behalf of public authorities to evaluate the eligibility of natural persons for essential public assistance benefits and services, including healthcare services, as well as to financial support allocation, reduce, revoke, or reclaim such benefits and services;\n\n(b)\tAI systems intended to be used to evaluate the creditworthiness of natural persons or establish their benefit score, with the exception of AI systems used for the purpose of detecting financial fraud;\n\n(c)\tAI systems intended to be used for risk assessment and pricing in relation to natural persons in the case of life and health insurance;\n\n(d)\tAI systems intended to evaluate and classify emergency calls by natural persons or to be used to dispatch, or to establish priority in the dispatching of, emergency first response services, including by police, firefighters and medical aid, as well as of emergency healthcare patient triage systems.\n\n6.\tLaw enforcement, in so far as their use is permitted under relevant Union or national law:\n\n(a)\tAI systems intended to be used by or on behalf of law enforcement authorities, or by Union institutions, bodies, offices or agencies in support of law enforcement authorities or on their behalf to assess the risk of a natural person becoming the victim of criminal offences;\n\n(b)\tAI systems intended to be used by or on behalf of law enforcement authorities or by Union institutions, bodies, offices or agencies in support of law enforcement authorities as polygraphs or similar tools;\n\n(c)\tAI systems intended to be used by or on behalf of law enforcement authorities, or by Union institutions, bodies, offices or agencies, in support of law enforcement authorities to evaluate the reliability of evidence in the course of the investigation or prosecution of criminal offences;\n\n(d)\tAI systems intended to be used by law enforcement authorities or on their behalf or by Union institutions, bodies, offices or agencies in support of law enforcement authorities for assessing the risk of a natural person offending or re-offending not solely on the basis of the profiling of natural persons as referred to in Article 3(4) of Directive (EU) 2016/680, or to assess personality traits and characteristics or past criminal behaviour of natural persons or groups;\n\n(e)\tAI systems intended to be used by or on behalf of law enforcement authorities or by Union institutions, bodies, offices or agencies in support of law enforcement authorities for the profiling of natural persons as referred to in Article 3(4) of Directive (EU) 2016/680 in the course of the detection, investigation or prosecution of criminal offences.", "tags": ["Applications: Medicine, life sciences and public health", "Applications: Government: benefits and welfare", "Applications: Finance and investment", "Applications: Business services and analytics", "Applications: Government: judicial and law enforcement"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_231", "doc_id": "757", "text": "(e)\tassessment of the human oversight measures needed in accordance with Article 14, including an assessment of the technical measures needed to facilitate the interpretation of the outputs of AI systems by the deployers, in accordance with Article 13(3), point (d);\n\n(f)\twhere applicable, a detailed description of pre-determined changes to the AI system and its performance, together with all the relevant information related to the technical solutions adopted to ensure continuous adherence of the AI system with the relevant requirements set out in Chapter III, Section 2;\n\n(g)\tthe validation and testing procedures used, including information about the validation and testing data used and their main characteristics; metrics used to measure accuracy, robustness and adherence with other relevant requirements set out in Chapter III, Section 2, as well as potentially discriminatory impacts; test logs and all test reports dated and signed by the responsible persons, including with regard to pre-determined changes as referred to under point (f);\n\n(h)\tcybersecurity measures put in place;", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: Accuracy thereof", "Risk factors: Reliability", "Risk factors: Reliability: Robustness", "Harms: Discrimination", "Risk factors: Security", "Risk factors: Security: Cybersecurity"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_232", "doc_id": "757", "text": "3.\tDetailed information about the monitoring, functioning and control of the AI system, in particular with regard to: its capabilities and limitations in performance, including the degrees of accuracy for specific persons or groups of persons on which the system is intended to be used and the overall expected level of accuracy in relation to its intended purpose; the foreseeable unintended outcomes and sources of risks to health and safety, fundamental rights and discrimination in view of the intended purpose of the AI system; the human oversight measures needed in accordance with Article 14, including the technical measures put in place to facilitate the interpretation of the outputs of AI systems by the deployers; specifications on input data, as appropriate;\n\n4.\tA description of the appropriateness of the performance metrics for the specific AI system;\n\n5.\tA detailed description of the risk management system in accordance with Article 9;\n\n6.\tA description of relevant changes made by the provider to the system through its lifecycle;\n\n7.\tA list of the harmonised standards applied in full or in part the references of which have been published in the Official Journal of the European Union; where no such harmonised standards have been applied, a detailed description of the solutions adopted to meet the requirements set out in Chapter III, Section 2, including a list of other relevant standards and technical specifications applied;\n\n8.\tA copy of the EU declaration of conformity referred to in Article 47;\n\n9.\tA detailed description of the system in place to evaluate the AI system performance in the post-market phase in accordance with Article 72, including the post-market monitoring plan referred to in Article 72(3).", "tags": ["Strategies: Performance requirements", "Strategies: Disclosure", "Strategies: Disclosure: About incidents", "Risk factors: Reliability", "Strategies: Input controls", "Strategies: Disclosure: About evaluation"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_234", "doc_id": "757", "text": "4.\tA statement that the AI system is in conformity with this Regulation and, if applicable, with any other relevant Union law that provides for the issuing of the EU declaration of conformity referred to in Article 47;\n\n5.\tWhere an AI system involves the processing of personal data, a statement that that AI system complies with Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680;\n\n6.\tReferences to any relevant harmonised standards used or any other common specification in relation to which conformity is declared;\n\n7.\tWhere applicable, the name and identification number of the notified body, a description of the conformity assessment procedure performed, and identification of the certificate issued;\n\n8.\tThe place and date of issue of the declaration, the name and function of the person who signed it, as well as an indication for, or on behalf of whom, that person signed, a signature.", "tags": ["Strategies: Licensing, registration, and certification"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_235", "doc_id": "757", "text": "ANNEX VI\n\nConformity assessment procedure based on internal control\n\n1.   The conformity assessment procedure based on internal control is the conformity assessment procedure based on points 2, 3 and 4.\n\n2.   The provider verifies that the established quality management system is in adherence with the requirements of Article 17.\n\n3.   The provider examines the information contained in the technical documentation in order to assess the adherence of the AI system with the relevant essential requirements set out in Chapter III, Section 2.\n\n4.   The provider also verifies that the design and development process of the AI system and its post-market monitoring as referred to in Article 72 is consistent with the technical documentation.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_239", "doc_id": "757", "text": "4.4.\tIn examining the technical documentation, the notified body may require that the provider supply further evidence or carry out further tests so as to enable a proper assessment of the conformity of the AI system with the requirements set out in Chapter III, Section 2. Where the notified body is not satisfied with the tests carried out by the provider, the notified body shall itself directly carry out adequate tests, as appropriate.\t\n\t\n4.5.\tWhere necessary to assess the conformity of the high-risk AI system with the requirements set out in Chapter III, Section 2, after all other reasonable means to verify conformity have been exhausted and have proven to be insufficient, and upon a reasoned request, the notified body shall also be granted access to the training and trained models of the AI system, including its relevant parameters. Such access shall be subject to existing Union law on the protection of intellectual property and trade secrets.\t\n\t\n4.6.\tThe decision of the notified body shall be notified to the provider or its authorised representative. The notification shall contain the conclusions of the assessment of the technical documentation and the reasoned assessment decision.\n\nWhere the AI system is in conformity with the requirements set out in Chapter III, Section 2, the notified body shall issue a Union technical documentation assessment certificate. The certificate shall indicate the name and address of the provider, the conclusions of the examination, the conditions (if any) for its validity and the data necessary for the identification of the AI system.\n\nThe certificate and its annexes shall contain all relevant information to allow the conformity of the AI system to be evaluated, and to allow for control of the AI system while in use, where applicable.\n\nWhere the AI system is not in conformity with the requirements set out in Chapter III, Section 2, the notified body shall refuse to issue a Union technical documentation assessment certificate and shall inform the applicant accordingly, giving detailed reasons for its refusal.\n\nWhere the AI system does not meet the requirement relating to the data used to train it, re-training of the AI system will be needed prior to the application for a new conformity assessment. In this case, the reasoned assessment decision of the notified body refusing to issue the Union technical documentation assessment certificate shall contain specific considerations on the quality data used to train the AI system, in particular on the reasons for non-adherence.\t\n\n4.7.\tAny change to the AI system that could affect the adherence of the AI system with the requirements or its intended purpose shall be assessed by the notified body which issued the Union technical documentation assessment certificate. The provider shall inform such notified body of its intention to introduce any of the abovementioned changes, or if it otherwise becomes aware of the occurrence of such changes. The intended changes shall be assessed by the notified body, which shall decide whether those changes require a new conformity assessment in accordance with Article 43(4) or whether they could be addressed by means of a supplement to the Union technical documentation assessment certificate. In the latter case, the notified body shall assess the changes, notify the provider of its decision and, where the changes are approved, issue to the provider a supplement to the Union technical documentation assessment certificate.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Licensing, registration, and certification", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_249", "doc_id": "757", "text": "ANNEX XI\n\nTechnical documentation referred to in Article 53(1), point (a) — technical documentation for providers of general-purpose AI models\n\nSection 1\n\nInformation to be provided by all providers of general-purpose AI models\n\nThe technical documentation referred to in Article 53(1), point (a) shall contain at least the following information as appropriate to the size and risk profile of the model:\n\n1.\tA general description of the general-purpose AI model including:\n\n(a)\tthe tasks that the model is intended to perform and the type and nature of AI systems in which it can be integrated;\n\n(b)\tthe acceptable use policies applicable;\n\n(c)\tthe date of release and methods of distribution;\n\n(d)\tthe architecture and number of parameters;\n\n(e)\tthe modality (e.g. text, image) and format of inputs and outputs;\n\n(f)\tthe licence.", "tags": ["Strategies: Disclosure", "Strategies: Licensing, registration, and certification"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "757_251", "doc_id": "757", "text": "ANNEX XII\n\nTransparency information referred to in Article 53(1), point (b) — technical documentation for providers of general-purpose AI models to downstream providers that integrate the model into their AI system\n\nThe information referred to in Article 53(1), point (b) shall contain at least the following:\n\n1.\tA general description of the general-purpose AI model including:\n\n(a)\tthe tasks that the model is intended to perform and the type and nature of AI systems into which it can be integrated;\n\n(b)\tthe acceptable use policies applicable;\n\n(c)\tthe date of release and methods of distribution;\n\n(d)\thow the model interacts, or can be used to interact, with hardware or software that is not part of the model itself, where applicable;\n\n(e)\tthe versions of relevant software related to the use of the general-purpose AI model, where applicable;\n\n(f)\tthe architecture and number of parameters;\n\n(g)\tthe modality (e.g. text, image) and format of inputs and outputs;\n\n(h)\tthe licence for the model.", "tags": ["Strategies: Licensing, registration, and certification", "Strategies: Disclosure", "Strategies: Disclosure: About inputs"], "source": "https://eur-lex.europa.eu/eli/reg/2024/1689/oj", "official_name": "REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)", "label": "safe"}
{"id": "763_2", "doc_id": "763", "text": "MODEL EVALUATIONS AND RED TEAMING\nOur recent public launches of Llama 2 and some of our new consumer AI products announced in October as part of our Connect developer conference, highlight our thoughtful and careful approach to AI development. As part of the Llama 2 launch, we released both a Responsible Use Guide and the Llama 2 research paper. The research paper includes extensive information about how we charge-tuned Llama 2, and the benchmarks we evaluated the model’s performance against. The research papers that we released alongside our recent announcement of new generative AI features also include such sections (see Annex 1), as does our report on Building Generative AI Responsibly.\nThe Llama 2 paper goes into great depth about the steps we took to identify, evaluate and mitigate risks in our model. The risk categories that we considered can be broadly divided into the following three categories:\n\tIllicit and criminal activities (e.g., terrorism, theft, human trafficking); \n\tHateful and harmful activities (e.g., defamation, self harm, eating disorders, discrimination); and \n\tUnqualified advice (e.g., medical advice, financial advice, legal advice). \nAnd for each of those risk categories, we explored numerous potential attack vectors. For example:\n\tPsychological manipulation (e.g., authority manipulation), \n\tLogic manipulation (e.g., false premises), \n\tSyntactic manipulation (e.g., misspelling), \n\tSemantic manipulation (e.g., metaphor), \n\tPerspective manipulation (e.g., role playing), \n\tNon-English languages, and others. \nThe paper outlines different techniques that we used to charge-tune for safety, with examples of the improvements that charge-tuning delivered. We also provide further guidance on responsible charge-tuning in the Responsible Use Guide that we published to support developers building with Llama 2, including detailed safety alignment efforts and evaluation results.\nOne key element of testing Llama 2 was our significant investments in red teaming with various groups of internal employees, contract workers, and external vendors. The red teamers probed our models across a wide range of risk categories (such as criminal planning, human trafficking, regulated or controlled substances, sexually explicit content, unqualified health or financial advice, privacy violations, and more), as well as different attack vectors (such as hypothetical questions, malformed/misspelt inputs, or extended dialogues). Additionally, we conducted specific tests to determine the capabilities of our models to facilitate the production of weapons (e.g. nuclear, biological, chemical, and cyber); findings on these topics were marginal and were mitigated.\nOur red teaming efforts for Llama 2 also included external red teaming efforts. We submitted our model to the DEFCON convention in Las Vegas this August alongside other companies like Anthropic, Google, Hugging Face, Stability, and OpenAI where over 2500 hackers analysed and stress tested their capabilities - making this - to date - the largest public red-teaming event for AI. Our extensive testing through both internal and external red teaming is continuing to help improve our AI work across Meta.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Risk factors: Safety", "Risk factors: Privacy", "Risk factors: Reliability", "Risk factors: Reliability: Robustness"], "source": "https://transparency.meta.com/en-gb/policies/ai-safety-policies-for-safety-summit/", "official_name": "2023 UK AI Safety Summit Response (Meta)", "label": "safe"}
{"id": "763_9", "doc_id": "763", "text": "DATA INPUT CONTROLS AND review\nGenerative AI models take a large amount of data to effectively train, so a combination of sources are used for training, including information that’s publicly available online, licensed data and information from Meta’s products and services. For publicly available online information, we filtered the dataset to exclude certain websites that commonly share personal information about private individuals. Publicly shared posts from Instagram and Facebook – including photos and text – were part of the data used to train the generative AI models underlying the features we announced at Connect. We didn’t train these models using people’s private posts. We also do not use the content of your private messages with friends and family to train our AIs.\nFor example Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The charge-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining or charge-tuning datasets include Meta user data.\nWe ran each dataset used to train Llama 2 through Meta’s standard privacy review process, which is a central part of developing new and updated products, services, and practices at Meta. Through this process, we identify potential privacy risks and develop mitigations for those risks. For example, in training Llama 2 we: (1) ensured that the training data excluded Meta user data; (2) excluded data from certain sites known to contain a high volume of information about private individuals [like directories].\nWe use the information people share when interacting with our generative AI features, such as Meta AI or businesses who use generative AI, to improve our products and for other purposes. You can read more about the data we collect and how we use your information in our new Privacy Guide, the Meta AI Terms of Service and our Privacy Policy. Additionally, our generative AI tools may retain and use information you share in a chat to provide more personalised responses or relevant information in that conversation, and we may share certain questions you ask with trusted partners, such as search providers, to give you more relevant, accurate, and up-to-date responses.\nIt’s important to know that we train and tune our generative AI models to limit the possibility of private information that you may share with generative AI features from appearing in responses to other people. We use automated technology and people to review interactions with our AI so we can, among other things, reduce the likelihood that models’ outputs would include someone’s personal information as well as improve model performance.\nTo give you more control, we’ve built in commands that allow you to delete information shared in any chat with an AI across Messenger, Instagram, or WhatsApp. For example you can delete your AI messages by typing “/reset-ai” in a conversation. Using a generative AI feature provided by Meta does not link your WhatsApp account information to your account information on Facebook, Instagram, or any other apps provided by Meta.", "tags": ["Strategies: Input controls", "Strategies: Input controls: Data use", "Risk factors: Privacy", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Performance requirements"], "source": "https://transparency.meta.com/en-gb/policies/ai-safety-policies-for-safety-summit/", "official_name": "2023 UK AI Safety Summit Response (Meta)", "label": "safe"}
{"id": "765_6", "doc_id": "765", "text": "Post-deployment monitoring for patterns of misuse\nWe work hard to prevent foreseeable risks before deployment. However, there are also limits to what anyone can learn in a lab. Even after extensive research and testing, we cannot predict all of the beneficial ways people will use our technology, nor all the ways people may abuse it. Building the capacity to quickly detect and address unforeseen risks is a high priority for us, as this capacity is a critical safeguard for frontier systems where not all risks can be fully anticipated. We build internal measures designed to detect unexpected types of abuse, have processes to respond to them,and use the learnings to improve our usage policies, safety systems, and model outputs. After releasing a system, we do proactive investigation, monitoring, and vetting of inbound reports to detect abuse or unforeseen risks. We then aim to quickly and iteratively address surfaced issues via policy and technical solutions. We are continuing to scale our operations and reduce response time.", "tags": ["Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation"], "source": "https://openai.com/global-affairs/our-approach-to-frontier-risk/", "official_name": "OpenAI's Approach to Frontier Risk (An Update for the UK AI Safety Summit)", "label": "safe"}
{"id": "765_7", "doc_id": "765", "text": "Security controls including securing model weights\nWe dedicate significant resources to the protection of OpenAI’s technology, intellectual property, and data.\nWe deploy our most powerful AI models as services. We do not distribute weights for such models outside of OpenAI and our technology partner Microsoft, and we provide third-party access to our most capable models via API so the model weights, source code, and other sensitive information remain controlled.\nWe also implement commercially reasonable technical, administrative, and organizational measures designed to prevent personal information loss, misuse, and unauthorized access. This includes undergoing third-party reviews of our security program including SOC 2 Type 2. We have also started a bug bounty program that invites independent researchers to report vulnerabilities in our systems in exchange for cash rewards. Our Trust Portal allows customers and other stakeholders to review our security controls and review reports. As part of our cybersecurity efforts, we regularly conduct internal and third-party penetration testing, and review the suitability and effectiveness of our security controls.", "tags": ["Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Security: Dissemination", "Strategies: Evaluation: External auditing", "Strategies: Evaluation", "Harms: Harm to property"], "source": "https://openai.com/global-affairs/our-approach-to-frontier-risk/", "official_name": "OpenAI's Approach to Frontier Risk (An Update for the UK AI Safety Summit)", "label": "safe"}
{"id": "765_9", "doc_id": "765", "text": "Data Input Controls and review\nOpenAI’s large language models, including the models that power ChatGPT, are developed using three primary sources of information: (1) information that is publicly available on the internet, (2) information that we license from third parties, and (3) information that our users or our human trainers provide.  \nThe vast majority of our training data comes from publicly available information that is freely and openly available on the Internet – for example, we do not seek information behind paywalls or from the “deep web.” We apply filters and remove certain data that we do not want our models to learn from or output, such as hate speech, adult content, sites that primarily aggregate personal information, and spam. \nWe also have implemented measures to enable creators, rightsholders, and website operators to express their preferences regarding AI training with respect to the content that they own or control. For example, OpenAI has implemented an easy means for website operators to exclude their content from being accessed by OpenAI’s “GPTBot” web crawler, relying on the robots.txt web standard. Similarly, OpenAI has documented the user-agent-string (“ChatGPT-user”) used by ChatGPT and ChatGPT plugins to access websites, so that site operators can block access for those purposes, as well. We provide instructions online for how to disallow either bot from accessing  sites. We also provide a self-service form\n(opens in a new window)\nfor image creators to opt their content out from training of our future DALL-E image generation models.", "tags": ["Risk factors: Bias", "Risk factors: Privacy", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls"], "source": "https://openai.com/global-affairs/our-approach-to-frontier-risk/", "official_name": "OpenAI's Approach to Frontier Risk (An Update for the UK AI Safety Summit)", "label": "safe"}
{"id": "766_4", "doc_id": "766", "text": "3. Establishing safety baselines. Only models with a post-mitigation score of \"medium\" or below can be deployed, and only models with a post-mitigation score of \"high\" or below can be developed further (as defined in the Tracked Risk Categories below). In addition, we will ensure Security is appropriately tailored to any model that has a \"high\" or \"critical\" pre-mitigation level of risk (as defined in the Scorecard below) to prevent model exfiltration. We also establish procedural commitments (as defined in Governance below) that further specify how we operationalize all the activities that the Preparedness Framework outlines.", "tags": ["Strategies: Tiering: Tiering based on impact", "Strategies: Tiering", "Strategies: Performance requirements"], "source": "https://cdn.openai.com/openai-preparedness-framework-beta.pdf", "official_name": "OpenAI Preparedness Framework (Beta)", "label": "safe"}
{"id": "766_9", "doc_id": "766", "text": "Tracked Risk Categories\n\nIn this section, we identify the categories of risks that we will be tracking, along with a dedicated workstream for identifying and adding new or nascent categories of risk as they emerge, i.e., \"unknown unknowns.\" Our intent is to \"go deep\" in the tracked categories to ensure we are testing for any possible worst-case scenarios, while also maintaining a broad holistic view of risks via monitoring activities across OpenAI and the \"unknown unknowns\" identification process.\n\nEach of the Tracked Risk Categories comes with a gradation scale. We believe monitoring gradations of risk will enable us to get in front of escalating threats and be able to apply more tailored mitigations. In general, \"low\" on this gradation scale is meant to indicate that the corresponding category of risks is not yet a significant problem, while \"critical\" represents the maximal level of concern.\n\nSpecifically, below, we lay out details for the following Tracked Risk Categories:\n\n• Cybersecurity\n• Chemical, Biological, Nuclear, and Radiological (CBRN) threats\n• Persuasion\n• Model autonomy", "tags": ["Strategies: Tiering", "Strategies: Tiering: Tiering based on impact", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Performance requirements"], "source": "https://cdn.openai.com/openai-preparedness-framework-beta.pdf", "official_name": "OpenAI Preparedness Framework (Beta)", "label": "safe"}
{"id": "766_11", "doc_id": "766", "text": "Cybersecurity\n\nCybersecurity is focused on risks related to use of the model for cyber-exploitation to disrupt confidentiality, integrity, and/or availability of computer systems.\n\nRisk level: Low\nDefinition: Model assistance for cyberattacks, if any, is meaningful only for non-programming uses, e.g., translation, targeted distillation of confidential information, and/or data analysis AND model cannot meaningfully execute elements of cyber operations, e.g., developing a known exploit into an attack, black-box exploitation, translating reasoning about defenses into action, or lateral goal-directed movement on hardened targets.\nRationale for threshold: Non-programming tasks represent a sizable bottleneck for threat actors; however, the skills involved are not terribly specialized and the benefit comes often from intersectional skills (i.e., proficiency in English and a high level understanding of political or technical systems to summarize emails). We exclude vulnerability discovery because the harm at this stage is not identification of individual vulnerabilities, but moreso the implementation and deployment of an exploit against a target.\n\nRisk level: Medium\nDefinition: Model increases the productivity of operators by an efficiency threshold (e.g. >2x time saved) on key cyber operation tasks, such as developing a known exploit into an attack, black-box exploitation, goal-directed lateral movement, identification of valuable information, remaining undetected, or reacting to defense.\nRationale for threshold: The execution of a cyberattack is a scarce skill that is generally learned from experience. A model's ability to substitute in for or accelerate a professional would improve the quality or quantity of attacks. Note that this productivity speedup also includes low? level (potentially state) actors using known methods, e.g. Russian disinformation groups, who use purchased capabilities, phishing, and otherwise known methods.\n\nRisk level: High\nDefinition: Tool-augmented model can identify and develop proofs-of-concept for high-value exploits against hardened targets without human intervention, potentially involving novel exploitation techniques, OR provided with a detailed strategy, the model can end? to-end execute cyber operations involving the above tasks without human intervention.\nRationale for threshold: High-value exploits are generally against hardened platforms, scarce, and easily productionizable (e.g., a zero-click iOS exploit). While weaker vulnerability detection capabilities (e.g. those already cheaply available and fairly liquid on exploit markets) do not meaningfully address a bottleneck for attackers, such advanced capability would unblock many and be usable by actors of many skill levels. End-to-end execution would dramatically accelerate the quality or quantity of professional cyberattacks.\n\nRisk level: Critical\nDefinition: Tool-augmented model can identify and develop functional zero-day exploits of all severity levels, across all software projects, without human intervention OR model can devise and execute end-to-end novel strategies for cyberattacks against hardened targets given only a high level desired goal.\nRationale for threshold: Finding and executing end-to-end exploits for all software would lead to catastrophe from unilateral actors, from hacking military or industrial systems as well as OpenAI infrastructure. Novel cyber operations generally pose the most serious threat, as they are unpredictable and scarce. They may involve, e.g., novel zero-days or methods of command-and-control.", "tags": ["Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to infrastructure", "Strategies: Performance requirements", "Strategies: Tiering", "Strategies: Tiering: Tiering based on impact", "Applications: Security"], "source": "https://cdn.openai.com/openai-preparedness-framework-beta.pdf", "official_name": "OpenAI Preparedness Framework (Beta)", "label": "safe"}
{"id": "766_16", "doc_id": "766", "text": "Scorecard\n\nAs a part of our Preparedness Framework, we will maintain a dynamic (i.e., frequently updated) Scorecard that is designed to track our current pre-mitigation model risk across each of the risk categories, as well as the post-mitigation risk. The Scorecard will be regularly updated by the Preparedness team to help ensure it reflects the latest research and findings. Sources that inform the updates to the Scorecard will also include tracking observed misuse, and other community red-teaming and input on our frontier models from other teams (e.g., Policy Research, Safety Systems, Superalignment).", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation"], "source": "https://cdn.openai.com/openai-preparedness-framework-beta.pdf", "official_name": "OpenAI Preparedness Framework (Beta)", "label": "safe"}
{"id": "766_19", "doc_id": "766", "text": "Evaluating post-mitigation risk\n\nTo verify if mitigations have sufficiently and dependently reduced the resulting post? mitigation risk, we will also run evaluations on models after they have safety mitigations in place, again attempting to verify and test the possible \"worst known case\" scenario for these systems. As part of our baseline commitments, we are aiming to keep post-mitigation risk at \"medium\" risk or below.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Strategies: Evaluation: Impact assessment", "Strategies: Performance requirements"], "source": "https://cdn.openai.com/openai-preparedness-framework-beta.pdf", "official_name": "OpenAI Preparedness Framework (Beta)", "label": "safe"}
{"id": "766_21", "doc_id": "766", "text": "Mitigations\n\nA central part of meeting our safety baselines is implementing mitigations to address various types of model risk. Our mitigation strategy will involve both containment measures, which help reduce risks related to possession of a frontier model, as well as deployment mitigations, which help reduce risks from active use of a frontier model. As a result, these mitigations might span increasing compartmentalization, restricting deployment to trusted users, implementing refusals, redacting training data, or alerting distribution partners.\n\nIllustrative Scorecard\n\nNote: Below is only an illustrative template version of what the Scorecard might look like; all specifics are purely for illustrative purposes and do not reflect the results from real evaluations.\n\n[Omitted from plaintext version.]", "tags": ["Strategies: Input controls", "Risk factors: Security", "Strategies: Disclosure", "Risk factors: Security: Dissemination", "Strategies: Input controls: Data circulation"], "source": "https://cdn.openai.com/openai-preparedness-framework-beta.pdf", "official_name": "OpenAI Preparedness Framework (Beta)", "label": "safe"}
{"id": "766_24", "doc_id": "766", "text": "Restricting deployment\n\nOnly models with a post-mitigation score of \"medium\" or below can be deployed. In other words, if we reach (or are forecasted to reach) at least \"high\" pre-mitigation risk in any of the considered categories, we will not continue with deployment of that model (by the time we hit \"high\" pre-mitigation risk) until there are reasonably mitigations in place for the relevant post? mitigation risk level to be back at most to \"medium\" level. (Note that a potentially effective mitigation in this context could be restricting deployment to trusted parties.)\n\nRestricting development\n\nOnly models with a post-mitigation score of \"high\" or below can be developed further. In other words, if we reach (or are forecasted to reach) \"critical\" pre-mitigation risk along any risk category, we commit to ensuring there are sufficient mitigations in place for that model (by the time we reach that risk level in our capability development, let alone deployment) for the overall post-mitigation risk to be back at most to \"high\" level. Note that this should not preclude safety-enhancing development. We would also focus our efforts as a company towards solving these safety challenges and only continue with capabilities-enhancing development if we can reasonably assure ourselves (via the operationalization processes) that it is safe to do so.\n\nAdditionally, to protect against \"critical\" pre-mitigation risk, we need dependable evidence that the model is sufficiently aligned that it does not initiate \"critical\"-risk-level tasks unless explicitly instructed to do so.", "tags": ["Strategies: Tiering", "Strategies: Tiering: Tiering based on impact", "Strategies: Performance requirements", "Risk factors: Safety"], "source": "https://cdn.openai.com/openai-preparedness-framework-beta.pdf", "official_name": "OpenAI Preparedness Framework (Beta)", "label": "safe"}
{"id": "766_30", "doc_id": "766", "text": "• Accountability:\n\na. reviews: Scorecard evaluations (and corresponding mitigations) will be audited by qualified, independent third-parties to ensure accurate reporting of results, either by reproducing findings or by reviewing methodology to ensure soundness, at a cadence specified by the SAG and/or upon the request of OpenAI Leadership or the BoD.\n\nb. External access: We will also continue to enable external research and government access for model releases to increase the depth of red-teaming and testing of frontier model capabilities.\n\nc. Safety drills: A critical part of this process is to be prepared if fast-moving emergency scenarios arise, including what default organizational response might look like (including how to stress-test against the pressures of our business or our culture). While the Preparedness team and SAG will of course work hard on forecasting and preparing for risks, safety drills can help the organization build \"muscle memory\" by practicing and coming up with the right \"default\" responses for some of the foreseeable scenarios. Therefore, the SAG will call for safety drills at a recommended minimum yearly basis.\n\nd. For each of these accountability measures, decision-making will follow the standard process (i.e., SAG chair synthesizes final recommendation, which OpenAI leadership makes a final decision on, that the BoD can overrule if needed).\n\n\nExample Scenarios\n\n[Omitted from plaintext version.]", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Adversarial testing", "Strategies: Evaluation: External auditing"], "source": "https://cdn.openai.com/openai-preparedness-framework-beta.pdf", "official_name": "OpenAI Preparedness Framework (Beta)", "label": "safe"}
{"id": "769_2", "doc_id": "769", "text": "4. Be accountable to people.\nWe will design AI systems that provide appropriate opportunities for feedback, relevant explanations, and appeal. Our AI technologies will be subject to appropriate human direction and control.\n\n\n5. Incorporate privacy design principles.\nWe will incorporate our privacy principles in the development and use of our AI technologies. We will give opportunity for notice and consent, encourage architectures with privacy safeguards, and provide appropriate transparency and control over the use of data.\n\n\n6. Uphold high standards of scientific excellence.\nTechnological advancement is rooted in the scientific method and a commitment to open inquiry, intellectual rigor, integrity, and collaboration. AI tools have the potential to unlock new realms of scientific research and knowledge in critical domains like biology, chemistry, medicine, and environmental sciences. We aspire to high standards of scientific excellence as we work to progress AI development.\n\n\n7. Be made available for uses that accord with these principles.\nMany technologies have multiple uses. We will work to limit potentially harmful or abusive applications. As we develop and deploy AI technologies, we will evaluate likely uses in light of the following factors:\na) Primary purpose and use: the primary purpose and likely use of a technology and application, including how closely the solution is related to or adaptable to a harmful use\nb) Nature and uniqueness: whether we are making available technology that is unique or more generally available\nc) Scale: whether the use of this technology will have significant impact\nd) Nature of Google’s involvement: whether we are providing general-purpose tools, integrating tools for customers, or developing custom solutions", "tags": ["Risk factors: Bias", "Risk factors: Privacy", "Risk factors: Transparency", "Strategies: Evaluation: Impact assessment", "Strategies: Input controls: Data use", "Strategies: Tiering: Tiering based on impact", "Strategies: Tiering: Tiering based on generality"], "source": "https://ai.google/static/documents/EN-AI-Principles.pdf", "official_name": "Google AI Principles", "label": "safe"}
{"id": "772_1", "doc_id": "772", "text": "[Introductory material omitted. Figures omitted throughout.]\n\nPart 1: Foundational Information\n\n[Omitted.]\n\n\n2.\tAudience\n\n[Omitted.]\n\n\n3.\tAI Risks and Trustworthiness\n\nFor AI systems to be trustworthy, they often need to be responsive to a multiplicity of criteria that are of value to interested parties. Approaches which enhance AI trustworthiness can reduce negative AI risks. This Framework articulates the following characteristics of trustworthy AI and offers guidance for addressing them. Characteristics of trustworthy AI systems include: valid and reliable, safe, secure and resilient, accountable and transparent, explainable and interpretable, privacy-enhanced, and fair with harmful bias managed. Creating trustworthy AI requires balancing each of these characteristics based on the AI system’s context of use. While all characteristics are socio-technical system attributes, accountability and transparency also relate to the processes and activities internal to an AI system and its external setting. Neglecting these characteristics can increase the probability and magnitude of negative consequences.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Privacy", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Transparency", "Risk factors: Reliability: Robustness", "Risk factors: Bias"], "source": "https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf", "official_name": "NIST AI Risk Management Framework 1.0", "label": "safe"}
{"id": "772_3", "doc_id": "772", "text": "There are multiple approaches for enhancing contextual awareness in the AI lifecycle. For example, subject matter experts can assist in the evaluation of TEVV findings and work with product and deployment teams to align TEVV parameters to requirements and deployment conditions. When properly resourced, increasing the breadth and diversity of input from interested parties and relevant AI actors throughout the AI lifecycle can enhance opportunities for informing contextually sensitive evaluations, and for identifying AI system benefits and positive impacts. These practices can increase the likelihood that risks arising in social contexts are managed appropriately.\n\nUnderstanding and treatment of trustworthiness characteristics depends on an AI actor’s particular role within the AI lifecycle. For any given AI system, an AI designer or developer may have a different perception of the characteristics than the deployer.\n\nTrustworthiness characteristics explained in this document influence each other. Highly secure but unfair systems, accurate but opaque and uninterpretable systems, and inaccurate but secure, privacy-enhanced, and transparent systems are all undesirable. A comprehensive approach to risk management calls for balancing tradeoffs among the trustworthiness characteristics. It is the joint responsibility of all AI actors to determine whether AI technology is an appropriate or necessary tool for a given context or purpose, and how to use it responsibly. The decision to commission or deploy an AI system should be based on a contextual assessment of trustworthiness characteristics and the relative risks, impacts, costs, and benefits, and informed by a broad set of interested parties.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Impact assessment"], "source": "https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf", "official_name": "NIST AI Risk Management Framework 1.0", "label": "safe"}
{"id": "772_4", "doc_id": "772", "text": "3.1\tValid and Reliable\n\nValidation is the “confirmation, through the provision of objective evidence, that the requirements for a specific intended use or application have been fulfilled” (Source: ISO 9000:2015). Deployment of AI systems which are inaccurate, unreliable, or poorly generalized to data and settings beyond their training creates and increases negative AI risks and reduces trustworthiness.\n\nReliability is defined in the same standard as the “ability of an item to perform as required, without failure, for a given time interval, under given conditions” (Source: ISO/IEC TS 5723:2022). Reliability is a goal for overall correctness of AI system operation under the conditions of expected use and over a given period of time, including the entire lifetime of the system.\n\nAccuracy and robustness contribute to the validity and trustworthiness of AI systems, and can be in tension with one another in AI systems.", "tags": ["Risk factors: Reliability", "Risk factors: Reliability: Robustness"], "source": "https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf", "official_name": "NIST AI Risk Management Framework 1.0", "label": "safe"}
{"id": "772_5", "doc_id": "772", "text": "Accuracy is defined by ISO/IEC TS 5723:2022 as “closeness of results of observations, computations, or estimates to the true values or the values accepted as being true.” Measures of accuracy should consider computational-centric measures (e.g., false positive and false negative rates), human-AI teaming, and demonstrate external validity (generalizable beyond the training conditions). Accuracy measurements should always be paired with clearly defined and realistic test sets – that are representative of conditions of expected use – and details about test methodology; these should be included in associated documentation. Accuracy measurements may include disaggregation of results for different data segments.\n\nRobustness or generalizability is defined as the “ability of a system to maintain its level of performance under a variety of circumstances” (Source: ISO/IEC TS 5723:2022). Robustness is a goal for appropriate system functionality in a broad set of conditions and circumstances, including uses of AI systems not initially anticipated. Robustness requires not only that the system perform exactly as it does under expected uses, but also that it should perform in ways that minimize potential harms to people if it is operating in an unexpected setting.\n\nValidity and reliability for deployed AI systems are often assessed by ongoing testing or monitoring that confirms a system is performing as intended. Measurement of validity, accuracy, robustness, and reliability contribute to trustworthiness and should take into consideration that certain types of failures can cause greater harm. AI risk management efforts should prioritize the minimization of potential negative impacts, and may need to include human intervention in cases where the AI system cannot detect or correct errors.", "tags": ["Risk factors: Reliability", "Risk factors: Reliability: Robustness", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation: Conformity assessment"], "source": "https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf", "official_name": "NIST AI Risk Management Framework 1.0", "label": "safe"}
{"id": "772_7", "doc_id": "772", "text": "3.3\tSecure and Resilient\n\nAI systems, as well as the ecosystems in which they are deployed, may be said to be resilient if they can withstand unexpected adverse events or unexpected changes in their environment or use – or if they can maintain their functions and structure in the face of internal and external change and degrade safely and gracefully when this is necessary (Adapted from: ISO/IEC TS 5723:2022). Common security concerns relate to adversarial examples, data poisoning, and the exfiltration of models, training data, or other intellectual property through AI system endpoints. AI systems that can maintain confidentiality, integrity, and availability through protection mechanisms that prevent unauthorized access and use may be said to be secure. Guidelines in the NIST Cybersecurity Framework and Risk Management Framework are among those which are applicable here.\n\nSecurity and resilience are related but distinct characteristics. While resilience is the ability to return to normal function after an unexpected adverse event, security includes resilience but also encompasses protocols to avoid, protect against, respond to, or recover from attacks. Resilience relates to robustness and goes beyond the provenance of the data to encompass unexpected or adversarial use (or abuse or misuse) of the model or data.", "tags": ["Risk factors: Reliability", "Risk factors: Reliability: Robustness", "Risk factors: Security"], "source": "https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf", "official_name": "NIST AI Risk Management Framework 1.0", "label": "safe"}
{"id": "772_16", "doc_id": "772", "text": "GOVERN 1:\nPolicies, processes, procedures, and practices across the organization related to the mapping, measuring, and managing of AI risks are in place, transparent, and implemented effectively.\n\nGOVERN 1.1: Legal and regulatory requirements involving AI are understood, managed, and documented.\n\nGOVERN 1.2: The characteristics of trustworthy AI are integrated into organizational policies, processes, procedures, and practices.\n\nGOVERN 1.3: Processes, procedures, and practices are in place to determine the needed level of risk management activities based on the organization’s risk tolerance.\n\nGOVERN 1.4: The risk management process and its outcomes are established through transparent policies, procedures, and other controls based on organizational risk priorities.\n\nGOVERN 1.5: Ongoing monitoring and periodic review of the risk management process and its outcomes are planned and organizational roles and responsibilities clearly defined, including determining the frequency of periodic review.\n\nGOVERN 1.6: Mechanisms are in place to inventory AI systems and are resourced according to organizational risk priorities.\n\nGOVERN 1.7: Processes and procedures are in place for decommissioning and phasing out AI systems safely and in a manner that does not increase risks or decrease the organization’s trustworthiness.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Post-market monitoring", "Risk factors: Transparency"], "source": "https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf", "official_name": "NIST AI Risk Management Framework 1.0", "label": "safe"}
{"id": "772_24", "doc_id": "772", "text": "MAP 1:\nContext is established and understood.\n\nMAP 1.1: Intended purposes, potentially beneficial uses, contextspecific laws, norms and expectations, and prospective settings in which the AI system will be deployed are understood and documented. Considerations include: the specific set or types of users along with their expectations; potential positive and negative impacts of system uses to individuals, communities, organizations, society, and the planet; assumptions and related limitations about AI system purposes, uses, and risks across the development or product AI lifecycle; and related TEVV and system metrics.\n\nMAP 1.2: Interdisciplinary AI actors, competencies, skills, and capacities for establishing context reflect demographic diversity and broad domain and user experience expertise, and their participation is documented. Opportunities for interdisciplinary collaboration are prioritized.\n\nMAP 1.3: The organization’s mission and relevant goals for AI technology are understood and documented.\n\nMAP 1.4: The business value or context of business use has been clearly defined or – in the case of assessing existing AI systems – re-evaluated.\n\nMAP 1.5: Organizational risk tolerances are determined and documented.\n\nMAP 1.6: System requirements (e.g., “the system shall respect the privacy of its users”) are elicited from and understood by relevant AI actors. Design decisions take socio-technical implications into account to address AI risks.", "tags": ["Strategies: Convening", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Performance requirements"], "source": "https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf", "official_name": "NIST AI Risk Management Framework 1.0", "label": "safe"}
{"id": "772_29", "doc_id": "772", "text": "5.3\tMeasure\n\nThe MEASURE function employs quantitative, qualitative, or mixed-method tools, techniques, and methodologies to analyze, assess, benchmark, and monitor AI risk and related impacts. It uses knowledge relevant to AI risks identified in the MAP function and informs the MANAGE function. AI systems should be tested before their deployment and regularly while in operation. AI risk measurements include documenting aspects of systems’ functionality and trustworthiness.\n\nMeasuring AI risks includes tracking metrics for trustworthy characteristics, social impact, and human-AI configurations. Processes developed or adopted in the MEASURE function should include rigorous software testing and performance assessment methodologies with associated measures of uncertainty, comparisons to performance benchmarks, and formalized reporting and documentation of results. Processes for independent review can improve the effectiveness of testing and can mitigate internal biases and potential conflicts of interest.\n\nWhere tradeoffs among the trustworthy characteristics arise, measurement provides a traceable basis to inform management decisions. Options may include recalibration, impact mitigation, or removal of the system from design, development, production, or use, as well as a range of compensating, detective, deterrent, directive, and recovery controls.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Post-market monitoring"], "source": "https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf", "official_name": "NIST AI Risk Management Framework 1.0", "label": "safe"}
{"id": "772_32", "doc_id": "772", "text": "MEASURE 2:\nAI systems are evaluated for trustworthy characteristics.\n\nMEASURE 2.1: Test sets, metrics, and details about the tools used during TEVV are documented.\n\nMEASURE 2.2: Evaluations involving human subjects meet applicable requirements (including human subject protection) and are representative of the relevant population.\n\nMEASURE 2.3: AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for conditions similar to deployment setting(s). Measures are documented.\n\nMEASURE 2.4: The functionality and behavior of the AI system and its components – as identified in the MAP function – are monitored when in production.\n\nMEASURE 2.5: The AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability beyond the conditions under which the technology was developed are documented.\n\nMEASURE 2.6: The AI system is evaluated regularly for safety risks – as identified in the MAP function. The AI system to be deployed is demonstrated to be safe, its residual negative risk does not exceed the risk tolerance, and it can fail safely, particularly if made to operate beyond its knowledge limits. Safety metrics reflect system reliability and robustness, real-time monitoring, and response times for AI system failures.", "tags": ["Risk factors: Reliability", "Risk factors: Reliability: Robustness", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment"], "source": "https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf", "official_name": "NIST AI Risk Management Framework 1.0", "label": "safe"}
{"id": "772_34", "doc_id": "772", "text": "MEASURE 3:\nMechanisms for tracking identified AI risks over time are in place.\n\nMEASURE 3.1: Approaches, personnel, and documentation are in place to regularly identify and track existing, unanticipated, and emergent AI risks based on factors such as intended and actual performance in deployed contexts.\n\nMEASURE 3.2: Risk tracking approaches are considered for settings where AI risks are difficult to assess using currently available measurement techniques or where metrics are not yet available.\n\nMEASURE 3.3: Feedback processes for end users and impacted communities to report problems and appeal system outcomes are established and integrated into AI system evaluation metrics.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Strategies: Convening"], "source": "https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf", "official_name": "NIST AI Risk Management Framework 1.0", "label": "safe"}
{"id": "785_2", "doc_id": "785", "text": "Outcome 1. Organisations effectively identify, assess and manage risks when developing and deploying their frontier AI models and systems. They will:\nI. Assess the risks posed by their frontier models or systems across the AI lifecycle, including before deploying that model or system, and, as appropriate, before and during training. Risk assessments should consider model capabilities and the context in which they are developed and deployed, as well as the efficacy of implemented mitigations to reduce the risks associated with their foreseeable use and misuse. They should also consider results from internal and external evaluations as appropriate, such as by independent third-party evaluators, their home governments, and other bodies their governments deem appropriate.\nII. Set out thresholds at which severe risks posed by a model or system, unless adequately mitigated, would be deemed intolerable. Assess whether these thresholds have been breached, including monitoring how close a model or system is to such a breach. These thresholds should be defined with input from trusted actors, including organisations’ respective home governments as appropriate. They should align with relevant international agreements to which their home governments are party. They should also be accompanied by an explanation of how thresholds were decided upon, and by specific examples of situations where the models or systems would pose intolerable risk.\nIII. Articulate how risk mitigations will be identified and implemented to keep risks within defined thresholds, including safety and security-related risk mitigations such as modifying system behaviours and implementing robust security controls for unreleased model weights.\nIV. Set out explicit processes they intend to follow if their model or system poses risks that meet or exceed the pre-defined thresholds. This includes processes to further develop and deploy their systems and models only if they assess that residual risks would stay below the thresholds. In the extreme, organisations commit not to develop or deploy a model or system at all, if mitigations cannot be applied to keep risks below the thresholds.\nV. Continually invest in advancing their ability to implement commitments i-iv, including risk assessment and identification, thresholds definition, and mitigation effectiveness. This should include processes to assess and monitor the adequacy of mitigations, and identify additional mitigations as needed to ensure risks remain below the pre-defined thresholds. They will contribute to and take into account emerging best practice, international standards, and science on AI risk identification, assessment, and mitigation.", "tags": ["Risk factors: Safety", "Risk factors: Security", "Risk factors: Security: Dissemination", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Strategies: Performance requirements"], "source": "https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024/frontier-ai-safety-commitments-ai-seoul-summit-2024", "official_name": "Frontier AI Safety Commitments, AI Seoul Summit 2024", "label": "safe"}
{"id": "807_2", "doc_id": "807", "text": "(b) Content.—The agreement entered into pursuant to subsection (a) shall provide for the study of the following:\n(1) The use of innovative technology (including artificial intelligence) in maternal health care, including the extent to which such technology has affected racial or ethnic biases in maternal health care.\n(2) The use of patient monitoring devices (including pulse oximeter devices) in maternal health care, including the extent to which such devices have affected racial or ethnic biases in maternal health care.\n(3) Best practices for reducing and preventing racial or ethnic biases in the use of innovative technology and patient monitoring devices in maternity care.\n(4) Best practices in the use of innovative technology and patient monitoring devices for pregnant and postpartum individuals from racial and ethnic minority groups.\n(5) Best practices with respect to privacy and security safeguards in such use.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Harms: Harm to health/safety", "Risk factors: Bias", "Risk factors: Privacy", "Risk factors: Security", "Risk factors: Safety", "Applications: Medicine, life sciences and public health", "Applications: Security"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/3305", "official_name": "Black Maternal Health Omnibus Act, Sec. 904 (Report on the Use of Technology in Maternity Care)", "label": "safe"}
{"id": "865_3", "doc_id": "865", "text": "SEC. 2. COMMISSION OF EXPERTS ON CHILD EXPLOITATION AND ARTIFICIAL INTELLIGENCE.\n\n\n(a) Establishment.—There is established a commission, to be known as the “Commission of Experts on Child Exploitation and Artificial Intelligence” (in this section referred to as the “Commission”), which shall investigate and make recommendations on solutions to improve the ability of a law enforcement agency to prevent, detect, and prosecute child exploitation crimes committed using artificial intelligence.", "tags": ["Risk factors: Safety", "Harms: Violation of civil or human rights, including privacy", "Strategies: New institution", "Applications: Government: judicial and law enforcement"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/8005", "official_name": "Child Exploitation and Artificial Intelligence Expert Commission Act of 2024", "label": "safe"}
{"id": "865_11", "doc_id": "865", "text": "(j) Definitions.—In this section:\n\n\n(1) ARTIFICIAL INTELLIGENCE.—The term “artificial intelligence” includes a machine-based system that uses data, references, or machine or human-based inputs to—\n\n\n(A) create a visual depiction, including an image or video;\n\n\n(B) recognize a pattern; or\n\n\n(C) make a recommendation, prediction, or decision.\n\n\n(2) CHILD EXPLOITATION CRIME.—The term “child exploitation crime” means a crime under any Federal, State, local, or Tribal law relating to the sexual exploitation of children.\n\n\n(3) LAW ENFORCEMENT AGENCY.—The term “law enforcement agency” means any Federal, State, Tribal, or local governmental agency that is engaged in or supervises the prevention, detection, investigation, or prosecution of a infraction of criminal law.\n\n\n(4) STATE.—The term “State” means each of the several States, the District of Columbia, and any commonwealth, territory, or possession of the United States.", "tags": ["Applications: Government: judicial and law enforcement"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/8005", "official_name": "Child Exploitation and Artificial Intelligence Expert Commission Act of 2024", "label": "safe"}
{"id": "874_1", "doc_id": "874", "text": "H. R. 7913\n\n\nTo require a notice be submitted to the Register of Copyrights with respect to copyrighted works used in building generative AI systems, and for other purposes.\n\n\nIN THE HOUSE OF REPRESENTATIVES\nApril 9, 2024\nMr. Schiff introduced the following bill; which was referred to the Committee on the Judiciary\n\n\nA BILL\nTo require a notice be submitted to the Register of Copyrights with respect to copyrighted works used in building generative AI systems, and for other purposes.\n\n\nBe it enacted by the Senate and House of Representatives of the United States of America in Congress assembled,", "tags": ["Risk factors: Transparency", "Harms: Harm to property", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Incentives: Civil liability", "Incentives: Fines", "Strategies: Disclosure", "Strategies: Disclosure: In standard form"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7913", "official_name": "Generative AI Copyright Disclosure Act of 2024", "label": "safe"}
{"id": "874_4", "doc_id": "874", "text": "(b) Civil consequence.—\n\n\n(1) ASSESSMENT.—Any person described under paragraph (1) of subsection (a) that fails to comply with a requirement under such subsection shall be assessed a civil consequence in an amount not less than $5,000.\n\n\n(2) REGULATIONS.— Not later than 180 days after the date on which this Act takes effect, the Register shall issue regulations to implement the requirement under paragraph (1).\n\n\n(c) Database.—The Register shall establish and maintain a publicly available online database that contains each notice filed under subsection (a)(1).", "tags": ["Incentives: Civil liability", "Incentives: Fines", "Strategies: Disclosure", "Strategies: Disclosure: In standard form"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7913", "official_name": "Generative AI Copyright Disclosure Act of 2024", "label": "safe"}
{"id": "874_5", "doc_id": "874", "text": "(d) Definitions.—In this section:\n\n\n(1) ARTIFICIAL INTELLIGENCE.—The term “Artificial Intelligence” means an automated system designed to perform a task typically associated with human intelligence or cognitive function.\n\n\n(2) COPYRIGHTED WORK.—The term “copyrighted work” means a work protected in the United States under a law relating to copyrights.\n\n\n(3) GENERATIVE AI MODEL.—The term “generative AI model” means a combination of computer code and numerical values designed to use Artificial Intelligence to generate outputs in the form of expressive material such as text, images, audio, or video.\n\n\n(4) GENERATIVE AI SYSTEM.—The term “generative AI system” means a software product or service that—\n\n\n(A) substantially incorporates one or more generative AI models; and\n\n\n(B) is designed for use by consumers.\n\n\n(5) REGISTER.—The term “Register” means the Register of Copyrights.\n\n\n(6) TRAINING DATASET.—The term “training dataset” means a collection of individual units of material (including a combination of text, images, audio, or other categories of expressive material, as well as annotations describing the material) used to train a generative AI model.\n\n\n(e) Effective Date.—This Act shall take effect on the date that is 180 days after the date of the enactment of this Act.", "tags": ["Risk factors: Transparency", "Harms: Harm to property", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Incentives: Civil liability", "Incentives: Fines", "Strategies: Disclosure", "Strategies: Disclosure: In standard form"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7913", "official_name": "Generative AI Copyright Disclosure Act of 2024", "label": "safe"}
{"id": "886_8", "doc_id": "886", "text": "SEC. 5. ENFORCEMENT.\nThe support pool Administrator may establish, by regulation, consequences for—\n(1) a infraction of a prohibition under this Act by a service provider; or\n(2) a failure to comply with a requirement under this Act by a service provider.", "tags": ["Incentives: Fines", "Incentives: Civil liability", "Applications: Broadcasting and media production"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7763/text", "official_name": "Living Wage for Musicians Act of 2024", "label": "safe"}
{"id": "889_6", "doc_id": "889", "text": "SEC. 5. ENFORCEMENT.\n\n\n(a) Unfair And Deceptive Acts Or Practices.—A infraction of a regulation promulgated under this Act shall be treated as a infraction of a rule defining an unfair or deceptive act or practice prescribed under section 18(a)(1)(B) of the Federal Trade Commission Act (15 U.S.C. 57a(a)(1)(B)).\n\n\n(b) Powers Of The Commission.—\n\n\n(1) IN GENERAL.—The Commission shall enforce regulations promulgated under this Act in the same manner, by the same means, and with the same jurisdiction, powers, and duties as though all applicable terms and provisions of the Federal Trade Commission Act (15 U.S.C. 41 et seq.) were incorporated into and made a part of such regulations.\n\n\n(2) PRIVILEGES AND IMMUNITIES.—Any person that violates a regulation promulgated under this Act shall be subject to the consequences, and entitled to the privileges and immunities, provided in the Federal Trade Commission Act (15 U.S.C. 41 et seq.).\n\n\n(3) REGULATIONS.—The Commission shall, pursuant to section 553 of title 5, United States Code, promulgate such regulations as the Commission determines necessary to carry out the provisions of this Act.\n\n\n(4) AUTHORITY PRESERVED.—Nothing in this Act shall be construed to limit the authority of the Commission under any other provision of law.", "tags": ["Incentives: Fines", "Incentives: Civil liability", "Risk factors: Privacy", "Risk factors: Security"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/3975", "official_name": "AI CONSENT Act", "label": "safe"}
{"id": "901_3", "doc_id": "901", "text": "“(a) Offense.—Except as provided in subsection (b), whoever knowingly or recklessly produces or distributes, or causes to be produced or distributed, in or affecting interstate or foreign commerce or using any means, channel, facility, or instrumentality of interstate or foreign commerce, a digital forgery of an identifiable individual, without the consent of the identifiable individual shall be fined under this title, imprisoned not more than 5 years, or both.", "tags": ["Incentives: Criminal liability", "Incentives: Fines", "Incentives: Imprisonment"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7567/text", "official_name": "Protect Victims of Digital Exploitation and Manipulation Act of 2024", "label": "safe"}
{"id": "901_4", "doc_id": "901", "text": "“(b) Exceptions.—\n“(1) IN GENERAL.—This section must avoid apply with respect to a distribution made in good faith—\n“(A) to a law enforcement officer or agency;\n“(B) as part of a legal proceeding;\n“(C) as part of medical education, diagnosis, or treatment; or\n“(D) in the reporting or investigation of—\n“(i) unlawful content; or\n“(ii) unsolicited or unwelcome conduct.\n“(2) SERVICE PROVIDERS.—This section must avoid apply to any provider of a communications service with regard to content provided by another information content provider unless the provider of the communications service knowingly or recklessly distributes content in infraction of this section.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Incentives: Criminal liability", "Incentives: Fines", "Incentives: Imprisonment", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7567/text", "official_name": "Protect Victims of Digital Exploitation and Manipulation Act of 2024", "label": "safe"}
{"id": "901_6", "doc_id": "901", "text": "“(d) Extraterritoriality.—There is extraterritorial Federal jurisdiction over an offense under this section if the alleged offender or the identifiable individual is a national of the United States (as defined in section 1101(a)(22) of title 8).”.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Incentives: Criminal liability", "Incentives: Fines", "Incentives: Imprisonment", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7567/text", "official_name": "Protect Victims of Digital Exploitation and Manipulation Act of 2024", "label": "safe"}
{"id": "901_8", "doc_id": "901", "text": "SEC. 3. SEVERABILITY.\nThe provisions of this Act shall be severable. If any provision of this Act, or any application thereof, is found unconstitutional, that finding must avoid affect any provision or application of the Act not so adjudicated.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Incentives: Criminal liability", "Incentives: Fines", "Incentives: Imprisonment", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7567/text", "official_name": "Protect Victims of Digital Exploitation and Manipulation Act of 2024", "label": "safe"}
{"id": "904_6", "doc_id": "904", "text": "(b) Enforcement.—\n(1) IN GENERAL.—Section 309(a)(4)(C)(i) of the Federal Election Campaign Act of 1971 (52 U.S.C. 30109(a)(4)(C))(i)) is amended—\n(A) in the matter before subclause (I), by inserting “or a qualified disclaimer requirement” after “a qualified disclosure requirement”; and\n(B) in subclause (II)—\n(i) by striking “a civil money consequence in an amount determined, for violations of each qualified disclosure requirement” and inserting “a civil money consequence—\n“(aa) for violations of each qualified disclosure requirement, in an amount determined”;\n(ii) by striking the period at the end and inserting “; and”; and\n(iii) by adding at the end the following new item:\n“(bb) for violations of each qualified disclaimer requirement, in an amount which is determined under a schedule of consequences which is established and published by the Commission and which takes into account the existence of previous violations by the person and how broadly the communication is distributed and such other factors as the Commission considers appropriate, provided that any such civil consequence must avoid exceed $50,000 per covered communication.”.", "tags": ["Incentives: Civil liability", "Incentives: Fines", "Strategies: Disclosure"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/3875/text", "official_name": "AI Transparency in Elections Act of 2024", "label": "safe"}
{"id": "904_7", "doc_id": "904", "text": "(2) FAILURE TO RESPOND.—Section 309(a)(4)(C)(ii) of such Act (52 U.S.C. 30109(a)(4)(C)(ii)) is amended by striking the period at the end and inserting “, except that in the case of a infraction of a qualified disclaimer requirement, failure to timely respond after the Commission has notified the person of an alleged infraction under subsection (a)(1) shall constitute the person’s admission of the factual allegations of the complaint.”.", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/3875/text", "official_name": "AI Transparency in Elections Act of 2024", "label": "safe"}
{"id": "904_10", "doc_id": "904", "text": "(5) TIME OF JUDICIAL REVIEW.—Section 309(a)(8)(A) of the Federal Election Campaign Act of 1971 (52 U.S.C. 30109(a)(8)(A)) is amended by inserting “(45-day period in the case of any complaint alleging a infraction of section 318(e)(2))” after “120-day period”.", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/3875/text", "official_name": "AI Transparency in Elections Act of 2024", "label": "safe"}
{"id": "906_4", "doc_id": "906", "text": "“§ 3591. Purposes\n\n\n“The purposes of this subchapter, with respect to the design, development, acquisition, use, management, and oversight of artificial intelligence in the Federal Government, are to ensure the following:\n\n\n“(1) Actions that are consistent with the Constitution and any other applicable law and policy, including those addressing freedom of speech, privacy, civil rights, civil liberties, and an open and transparent Government.\n\n\n“(2) Any such action is purposeful and performance-driven, including ensuring the following:\n\n\n“(A) Such action promotes the consistent and systemic treatment of all individuals in a fair, just, and impartial manner.\n\n\n“(B) The public benefits of such action significantly outweigh the risks.\n\n\n“(C) The risks and operations of such action do not unfairly and disproportionately benefit or harm an individual or subgroup of the public.\n\n\n“(D) The risk of such action is assessed and responsibly managed, including before the use of artificial intelligence.\n\n\n“(3) Any application of artificial intelligence is consistent with the use cases for which the artificial intelligence was trained, and the deployers of such application promote verifiably accurate, ethical, reliable, and effective use.\n\n\n“(4) The safety, security, and resiliency of artificial intelligence applications, including resilience when confronted with any systematic vulnerability, adversarial manipulation, and other malicious exploitation.\n\n\n“(5) The purpose, operations, risks, and outcomes of artificial intelligence applications are sufficiently explainable and understandable, to the extent practicable, by subject matter experts, users, impacted parties, and others, as appropriate.\n\n\n“(6) Such action is responsible and accountable, including by ensuring the following:\n\n\n“(A) Human roles and responsibilities are clearly defined, understood, and appropriately assigned.\n\n\n“(B) Artificial intelligence is used in a manner consistent with the purposes described in this section and the purposes for which each use of artificial intelligence is intended.\n\n\n“(C) Such action, as well as relevant inputs and outputs of artificial intelligence applications, are well documented and accountable.\n\n\n“(7) Responsible management and oversight by ensuring the following:\n\n\n“(A) Artificial intelligence applications are regularly tested against the purposes described in this section.\n\n\n“(B) Mechanisms are maintained to supersede, disengage, or deactivate applications of artificial intelligence that demonstrate performance or outcomes that are inconsistent with the intended use or this subchapter.\n\n\n“(C) Engagement with impacted communities.\n\n\n“(8) Transparency in publicly disclosing relevant information regarding the use of artificial intelligence to appropriate stakeholders, to the extent practicable and in accordance with any applicable law and policy, including with respect to the protection of privacy, civil liberties, and of sensitive law enforcement, national security, trade secrets or proprietary information, and other protected information.\n\n\n“(9) Accountability for the following:\n\n\n“(A) Implementing and enforcing appropriate safeguards necessary to comply with the purposes described in this section and the requirements of this subchapter, for the proper use and functioning of the applications of artificial intelligence.\n\n\n“(B) Monitoring, auditing, and documenting adherence with those safeguards, as appropriate.\n\n\n“(C) Providing appropriate training to all agency personnel responsible for the design, development, acquisition, use, management, and oversight of artificial intelligence.", "tags": ["Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Reliability", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Reliability: Robustness", "Risk factors: Interpretability and explainability", "Risk factors: Bias", "Applications: Government: other applications/unspecified"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7532", "official_name": "Federal AI Governance and Transparency Act", "label": "safe"}
{"id": "987_7", "doc_id": "987", "text": "Security Mitigations\n\nThe table below describes levels of security mitigations that may be applied to model weights to prevent their exfiltration. This is an impotant measure because the release of model weights may enable the removal of any safeguards trained into or deployed with the model, and hence access (including by bad actors) to any critical capabilities.\n\n0: Status quo - Industry standard development and enterprise controls. E.g., multi-factor authentication, basic access control mechanisms, secure software development standards, red-team tests.\n\n1: Controlled access - Access Control List hygiene. Non-forgeable lineage of models. Approximately RAND L3.5.\nLimited access to raw representations of the most valuable models, including isolation of development models from production models. Specific measures include model and checkpoint storage lockdown, SLSA Build L3 for model provenance, and hardening of ML platforms and tools.\n\n2: Lockdown of unilateral access - Significant restrictions of unilateral access of model weights. Approximately RAND L3-L4.\nChanges to ML platforms and tools to disallow unilateral access to raw model representations outside the core research team, with exceptions granted on the basis of business need.\n\n3: High-trust developer environments - For developers with unilateral access to raw models, protection against exfiltration and account compromise. Approximately RAND L4.\nModels can be accessed only through high-trust developer environments (HTDE), hardened, tamper-resistant workstations with enhanced logging.\n \n4: Advanced protection - Model weights are generally not accessible to humans, even non-unilaterally. Hardened software platforms and confidential-compute hardware make it difficult even for well-funded adversaries to find and exploit vulnerabilities. Approximately RAND L5.\nMinimal trusted computing base (TCB). TPUs with confidential compute capabilities. Dedicated hardware pods for training and serving high-value models.", "tags": ["Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Security: Dissemination", "Strategies: Evaluation: Adversarial testing", "Strategies: Evaluation", "Strategies: Tiering", "Strategies: Tiering: Tiering based on impact", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Performance requirements", "Risk factors: Transparency", "Strategies: Input controls", "Strategies: Input controls: Compute use", "Strategies: Evaluation: Impact assessment"], "source": "https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/introducing-the-frontier-safety-framework/fsf-technical-report.pdf", "official_name": "Google DeepMind Frontier Safety Framework Version 1.0", "label": "safe"}
{"id": "987_8", "doc_id": "987", "text": "Deployment Mitigations\n\nThis table below describes levels of deployment mitigations that may be applied to models and their descendants to manage access to and limit the expression of critical capabilities in deployment. Critical capabilities may have closely associated positive capabilities, misuse of critical capabilities may be more or less difficult to distinguish from beneficial uses, and the overall risks of misuse may differ by deployment context. Because of this, the mitigation options listed below are illustrative and will need to be tailored to different use cases and risks.\n\n0: Status quo - Safety finetuning of models and filters against general misuse and harmful model behavior.\n\n1: Mitigations targeting the critical capability - Use of the full suite of mitigations to prevent the inappropriate access of critical capabilities.\nApplication, where appropriate, of the full suite of prevailing industry safeguards targeting the specific capability, including safety charge-tuning, misuse filtering and detection, and response protocols. Periodic red-teaming to assess the adequacy of mitigations.\n\n2: Safety case with red team validation - Targeted safeguards, aimed at keeping numbers of incidents below a prespecified amount, with pre-deployment validation by a red-team.\nA robustness target is set based on a safety case considering factors like the critical capability and deployment context. Afterwards, similar mitigations as Level 1 are applied, but deployment takes place only after the robustness of safeguards has been demonstrated to meet the target.\nSome protection against inappropriate internal access of the critical capability, such as automated monitoring and logging of large-scale internal deployments, Security Level 2.\n\n3: Prevention of access - Mitigations that allow for high levels of confidence that capabilities cannot be accessed at all.\nTechnical options for this level of deployment safety are currently an open research problem.\nHighly restricted and monitored internal use, alongside high security.", "tags": ["Strategies: Evaluation: Adversarial testing", "Strategies: Evaluation", "Risk factors: Reliability", "Risk factors: Reliability: Robustness", "Strategies: Evaluation: Conformity assessment", "Risk factors: Security", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering", "Strategies: Tiering: Tiering based on impact", "Strategies: Performance requirements", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation: Impact assessment"], "source": "https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/introducing-the-frontier-safety-framework/fsf-technical-report.pdf", "official_name": "Google DeepMind Frontier Safety Framework Version 1.0", "label": "safe"}
{"id": "990_1", "doc_id": "990", "text": "SEC. 102. SANCTIONS WITH RESPECT TO COMMUNIST CHINESE MILITARY AND SURVEILLANCE COMPANIES.\n\n\n(a) In General.—Not later than 180 days after the date of the enactment of this Act, the President shall impose the sanctions described in subsection (e) with respect to any foreign person determined by the Secretary of the Treasury, in consultation with the Secretary of State and, as the Secretary of the Treasury determines appropriate, the Secretary of Defense, to knowingly engage in significant operations in the defense and related materiel sector or the surveillance technology sector of the economy of the People’s Republic of China.", "tags": ["Strategies: Input controls"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7476", "official_name": "Countering Communist China Act, Title I (\"Matters Related to Trade, Investment, and Economic Relations\"), Sec. 102 (\"Sanctions with Respect to Communist Chinese Military and Surveillance Companies\")", "label": "safe"}
{"id": "990_2", "doc_id": "990", "text": "(b) Annual Determination And Report.—Not less frequently than annually, the Secretary of the Treasury shall—\n\n\n(1) undertake the determination described under subsection (a) with respect to foreign persons listed in the Annex to Executive Order 14032 (as amended by any revision to such Annex); and\n\n\n(2) submit a report explaining the results of the determination to the appropriate congressional committees.", "tags": ["Strategies: Input controls", "Strategies: Input controls", "Strategies: Input controls", "Incentives: Criminal liability", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7476", "official_name": "Countering Communist China Act, Title I (\"Matters Related to Trade, Investment, and Economic Relations\"), Sec. 102 (\"Sanctions with Respect to Communist Chinese Military and Surveillance Companies\")", "label": "safe"}
{"id": "990_3", "doc_id": "990", "text": "(c) Assessment.—For the purpose of making the determination described under subsection (a), the Secretary of the Treasury, in consultation with the Secretary of State, the Secretary of Commerce, and the Secretary of Defense, shall—\n\n\n(1) assess whether, under existing authorities, sanctions should be imposed with respect to the activities of—\n\n\n(A) foreign persons listed on the Military End User List (Supplement No. 7 to part 744 of the Export Administration Regulations) that are located in the People’s Republic of China;\n\n\n(B) foreign persons listed by the Department of Commerce on the Denied Persons List or the Entity List (Supplement No. 4 to part 744 of the Export Administration Regulations) that are located in the People’s Republic of China; or\n\n\n(C) foreign persons listed pursuant to section 1260H of the William M. (Mac) Thornberry National Defense Authorization Act for Fiscal Year 2021 (10 U.S.C. 113 note); and\n\n\n(2) submit a report to the appropriate congressional committees summarizing such assessment, which shall include an explanation of why the sanctions described under subsection (e) may not be applicable to foreign persons included on the lists described under paragraph (1).", "tags": ["Strategies: Input controls", "Strategies: Input controls", "Strategies: Input controls", "Incentives: Criminal liability", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7476", "official_name": "Countering Communist China Act, Title I (\"Matters Related to Trade, Investment, and Economic Relations\"), Sec. 102 (\"Sanctions with Respect to Communist Chinese Military and Surveillance Companies\")", "label": "safe"}
{"id": "990_4", "doc_id": "990", "text": "(d) Consideration Of Certain Activities.—For the purpose of making the determination described under subsection (a), the Secretary of the Treasury may, to the extent practicable, focus particular attention on foreign persons engaging in any of the following:\n\n\n(1) Artificial intelligence, machine learning, autonomy, and related advances.\n\n\n(2) High-performance computing, semiconductors, and advanced computer hardware and software.\n\n\n(3) Quantum information science and technology.\n\n\n(4) Robotics, automation, and advanced manufacturing.\n\n\n(5) Advanced communications technology and immersive technology.\n\n\n(6) Biotechnology, medical technology, genomics, and synthetic biology.\n\n\n(7) Data storage, data management, and cybersecurity, including biometrics.\n\n\n(8) Advanced materials science, including composites and 2D materials.", "tags": ["Strategies: Input controls"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7476", "official_name": "Countering Communist China Act, Title I (\"Matters Related to Trade, Investment, and Economic Relations\"), Sec. 102 (\"Sanctions with Respect to Communist Chinese Military and Surveillance Companies\")", "label": "safe"}
{"id": "990_5", "doc_id": "990", "text": "(e) Sanctions Described.—The President shall exercise all of the powers granted to the President under the International Emergency Economic Powers Act (50 U.S.C. 1701 et seq.) to the extent necessary to block and restrict all transactions in property and interests in property of a foreign person if such property and interests in property—\n\n\n(1) are in the United States;\n\n\n(2) come within the United States; or\n\n\n(3) come within the possession or control of a United States person.", "tags": ["Strategies: Input controls"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7476", "official_name": "Countering Communist China Act, Title I (\"Matters Related to Trade, Investment, and Economic Relations\"), Sec. 102 (\"Sanctions with Respect to Communist Chinese Military and Surveillance Companies\")", "label": "safe"}
{"id": "990_6", "doc_id": "990", "text": "(f) Implementation.—The President may exercise all authorities provided under sections 203 and 205 of the International Emergency Economic Powers Act (50 U.S.C. 1702 and 1704) to carry out this section.", "tags": ["Strategies: Input controls", "Strategies: Input controls", "Strategies: Input controls", "Incentives: Criminal liability", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7476", "official_name": "Countering Communist China Act, Title I (\"Matters Related to Trade, Investment, and Economic Relations\"), Sec. 102 (\"Sanctions with Respect to Communist Chinese Military and Surveillance Companies\")", "label": "safe"}
{"id": "990_7", "doc_id": "990", "text": "(g) consequences.—The consequences set forth in section 206 of the International Emergency Economic Powers Act (50 U.S.C. 1705) apply to violations of any license, order, or regulation issued under this section.", "tags": ["Incentives: Criminal liability", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7476", "official_name": "Countering Communist China Act, Title I (\"Matters Related to Trade, Investment, and Economic Relations\"), Sec. 102 (\"Sanctions with Respect to Communist Chinese Military and Surveillance Companies\")", "label": "safe"}
{"id": "990_8", "doc_id": "990", "text": "(h) Waiver.—The President may waive the application of sanctions under this section, for renewable periods of one year, if the President certifies in writing to the appropriate congressional committees that the waiver is in the national interest of the United States, with an explanation of the reasons therefor. In lieu of the imposition of such sanctions, the President shall restrict the purchase or sale of any publicly traded securities, or any publicly traded securities that are derivative of such securities, issued by any person with respect to which sanctions were waived.", "tags": ["Strategies: Input controls", "Strategies: Input controls", "Strategies: Input controls", "Incentives: Criminal liability", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7476", "official_name": "Countering Communist China Act, Title I (\"Matters Related to Trade, Investment, and Economic Relations\"), Sec. 102 (\"Sanctions with Respect to Communist Chinese Military and Surveillance Companies\")", "label": "safe"}
{"id": "990_9", "doc_id": "990", "text": "(i) Exceptions.—\n\n\n(1) INTELLIGENCE AND LAW ENFORCEMENT ACTIVITIES.—Sanctions under this section must avoid apply with respect to—\n\n\n(A) any activity subject to the reporting requirements under title V of the National Security Act of 1947 (50 U.S.C. 3091 et seq.); or\n\n\n(B) any authorized intelligence or law enforcement activities of the United States.\n\n\n(2) UNITED STATES GOVERNMENT ACTIVITIES.—Nothing in this section shall restrict transactions for the conduct of the official business of the Federal Government by employees, grantees, or contractors thereof.\n\n\n(3) HUMANITARIAN ACTIVITIES.—The President may not impose sanctions under this section with respect to any person for conducting or facilitating a transaction for the sale of agricultural commodities, food, medicine, or medical devices or for the provision of humanitarian assistance.", "tags": ["Strategies: Input controls", "Strategies: Input controls", "Strategies: Input controls", "Incentives: Criminal liability", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7476", "official_name": "Countering Communist China Act, Title I (\"Matters Related to Trade, Investment, and Economic Relations\"), Sec. 102 (\"Sanctions with Respect to Communist Chinese Military and Surveillance Companies\")", "label": "safe"}
{"id": "990_10", "doc_id": "990", "text": "(j) Exception Relating To Importation Of Goods.—\n\n\n(1) IN GENERAL.—The authorities and requirements to impose sanctions authorized under this section must avoid include the authority or requirement to impose sanctions on the importation of goods.\n\n\n(2) GOOD DEFINED.—In this subsection, the term “good” means any article, natural or manmade substance, material, supply, or manufactured product, including inspection and test equipment, and excluding technical data.", "tags": ["Strategies: Input controls", "Strategies: Input controls", "Strategies: Input controls", "Incentives: Criminal liability", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7476", "official_name": "Countering Communist China Act, Title I (\"Matters Related to Trade, Investment, and Economic Relations\"), Sec. 102 (\"Sanctions with Respect to Communist Chinese Military and Surveillance Companies\")", "label": "safe"}
{"id": "990_11", "doc_id": "990", "text": "(k) Definitions.—In this section—\n\n\n(1) the term “appropriate congressional committees” means—\n\n\n(A) the Committee on Foreign Affairs and the Committee on Financial Services of the House of Representatives; and\n\n\n(B) the Committee on Foreign Relations and the Committee on Banking, Housing, and Urban Affairs of the Senate;\n\n\n(2) the term “foreign person” means an individual or entity that is not a United States person;\n\n\n(3) the term “United States person” means—\n\n\n(A) a United States citizen or an alien lawfully admitted for permanent residence to the United States;\n\n\n(B) an entity organized under the laws of the United States or of any jurisdiction within the United States, including a foreign branch of such an entity; or\n\n\n(C) a person in the United States; and\n\n\n(4) the term “knowingly” with respect to conduct, a circumstance, or a result, means that a person has actual knowledge, or should have known, of the conduct, the circumstance, or the result.", "tags": ["Strategies: Input controls", "Strategies: Input controls", "Strategies: Input controls", "Incentives: Criminal liability", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7476", "official_name": "Countering Communist China Act, Title I (\"Matters Related to Trade, Investment, and Economic Relations\"), Sec. 102 (\"Sanctions with Respect to Communist Chinese Military and Surveillance Companies\")", "label": "safe"}
{"id": "993_1", "doc_id": "993", "text": "SEC. 701. MODIFICATION TO USE OF EMERGENCY SANCTIONS AUTHORITIES REGARDING COMMUNIST CHINESE MILITARY COMPANIES.\n\n\n(a) In General.—Section 1237(a)(1) of the Strom Thurmond National Defense Authorization Act for Fiscal Year 1999 (50 U.S.C. 1701 note) is amended—\n\n\n(1) by striking “may exercise” and inserting “shall exercise”;\n\n\n(2) by striking clause (ii);\n\n\n(3) in the matter preceding clause (i), by striking “that—” and inserting “that is engaged in providing commercial services, manufacturing, producing, or exporting and—”;\n\n\n(4) in clause (i), by striking “; and” and inserting “; or”; and\n\n\n(5) by adding at the end the following new clause:\n\n\n“(ii) (I) is owned or controlled by, or affiliated with, the Chinese Communist Party or any person who has ever been a delegate of a National People’s Congress of the Chinese Communist Party; and\n\n\n“(II) is engaged in significant investment in the sectors of fifth-generation wireless communications, artificial intelligence, advanced computing, ‘big data’ analytics, autonomy, robotics, directed energy, hypersonics, or biotechnology.”.", "tags": ["Strategies: Input controls"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7476", "official_name": "Countering Communist China Act, Title VII (\"Matters Related to Defense\"), Sec. 701 (\"Modifications to Use of Emergency Sanctions Authorities Regarding Communist Chinese Military Companies\")", "label": "safe"}
{"id": "993_2", "doc_id": "993", "text": "(b) Extension Of List Requirement.—Notwithstanding section 1061(i)(6) of the National Defense Authorization Act for Fiscal Year 2017 (10 U.S.C. 111 note), the submission required by subsection (b) of section 1237 of the Strom Thurmond National Defense Authorization Act for Fiscal Year 1999—\n\n\n(1) must avoid terminate on December 31, 2021; and\n\n\n(2) shall continue in effect until December 31, 2026.", "tags": ["Strategies: Input controls"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7476", "official_name": "Countering Communist China Act, Title VII (\"Matters Related to Defense\"), Sec. 701 (\"Modifications to Use of Emergency Sanctions Authorities Regarding Communist Chinese Military Companies\")", "label": "safe"}
{"id": "995_3", "doc_id": "995", "text": "[Introductory material omitted.]\n\nBLUEPRINT FOR AN AI BILL OF RIGHTS\n\nSAFE AND EFFECTIVE SYSTEMS\nYou should be protected from unsafe or ineffective systems.   Automated systems should be developed with consultation from diverse communities, stakeholders, and domain experts to identify concerns, risks, and potential impacts of the system. Systems should undergo pre-deployment testing, risk identification and mitigation, and ongoing monitoring that demonstrate they are safe and effective based on their intended use, mitigation of unsafe outcomes including those beyond the intended use, and adherence to domain-specific standards. Outcomes of these protective measures should include the possibility of not deploying the system or removing a system from use. Automated systems should not be designed with an intent or reasonably foreseeable possibility of endangering your safety or the safety of your community. They should be designed to proactively protect you from harms stemming from unintended, yet foreseeable, uses or impacts of automated systems. You should be protected from inappropriate or irrelevant data use in the design, development, and deployment of automated systems, and from the compounded harm of its reuse. Independent evaluation and reporting that confirms that the system is safe and effective, including reporting of steps taken to mitigate potential harms, should be performed and the results made public whenever possible.", "tags": ["Strategies: Convening", "Risk factors: Safety", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: External auditing", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Risk factors: Reliability", "Strategies: Evaluation: Post-market monitoring", "Strategies: Performance requirements"], "source": "https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf", "official_name": "Blueprint for an AI Bill of Rights", "label": "safe"}
{"id": "995_4", "doc_id": "995", "text": "ALGORITHMIC DISCRIMINATION PROTECTIONS\nYou should not face discrimination by algorithms and systems should be used and designed in an equitable way. Algorithmic discrimination occurs when automated systems contribute to unjustified  different treatment or impacts disfavoring people based on their race, color, ethnicity, sex (including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual orientation), religion, age, national origin, disability, veteran status, genetic information, or any other classification protected by law. Depending on the specific circumstances, such algorithmic discrimination may violate legal protections. Designers, developers, and deployers of automated systems should take proactive and continuous measures to protect individuals and communities from algorithmic discrimination and to use and design systems in an equitable way. This protection should include proactive equity assessments as part of the system design, use of representative data and protection against proxies for demographic features, ensuring accessibility for people with disabilities in design and development, pre-deployment and ongoing disparity testing and mitigation, and clear organizational oversight. Independent evaluation and plain language reporting in the form of an algorithmic impact assessment, including disparity testing results and mitigation information, should be performed and made public whenever possible to confirm these protections.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Risk factors: Bias", "Risk factors: Transparency", "Harms: Discrimination", "Strategies: Performance requirements"], "source": "https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf", "official_name": "Blueprint for an AI Bill of Rights", "label": "safe"}
{"id": "995_5", "doc_id": "995", "text": "DATA  PRIVACY\nYou should be protected from abusive data practices via built-in protections and you should have agency over how data about you is used. You should be protected from violations of privacy through design choices that ensure such protections are included by default, including ensuring that data collection conforms to reasonable expectations and that only data strictly necessary for the specific context is collected.  Designers, developers, and deployers of automated systems should seek your permission and respect your decisions regarding collection, use, access, transfer, and deletion of your data in appropriate ways and to the greatest extent possible; where not possible, alternative privacy by design safeguards should be used. Systems should not employ user experience and design decisions that obfuscate user choice or burden users with defaults that are privacy invasive. Consent should only be used to justify collection of data in cases where it can be appropriately and meaningfully given. Any consent requests should be brief, be understandable in plain language, and give you agency over data collection and the specific context of use; current hard-to-understand notice-and-choice practices for broad uses of data should be changed. Enhanced protections and restrictions for data and inferences related to sensitive domains, including health, work, education, criminal justice, and finance, and for data pertaining to youth should put you first. In sensitive domains, your data and related inferences should only be used for necessary functions, and you should be protected by ethical review and use prohibitions. You and your communities should be free from unchecked surveillance; surveillance technologies should be subject to heightened oversight that includes at least pre-deployment assessment of their potential harms and scope limits to protect privacy and civil liberties. Continuous surveillance and monitoring should not be used in education, work, housing, or in other contexts where the use of such surveillance technologies is likely to limit rights, opportunities, or access. Whenever possible, you should have access to reporting that confirms your data decisions have been respected and provides an assessment of the potential impact of surveillance technologies on your rights, opportunities, or access.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Risk factors: Transparency", "Risk factors: Privacy", "Risk factors: Safety", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Risk factors: Security", "Risk factors: Security: Dissemination", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Input controls", "Strategies: Input controls: Data circulation", "Strategies: Input controls: Data use"], "source": "https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf", "official_name": "Blueprint for an AI Bill of Rights", "label": "safe"}
{"id": "995_9", "doc_id": "995", "text": "FROM PRINCIPLES TO PRACTICE: A TECHNICAL COMPANION TO THE BLUEPRINT\n\n[Table of contents omitted.]\n\nUSING  THIS  TECHNICAL  COMPANION\n\nThe Blueprint for an AI Bill of Rights is a set of five principles and associated practices to help guide the design, use, and deployment of automated systems to protect the rights of the American public in the age of artificial intelligence. This technical companion considers each principle in the Blueprint for an AI Bill of Rights and provides examples and concrete steps for communities, industry, governments, and others to take in order to build these protections into policy, practice, or the technological design process. \nTaken together, the technical protections and practices laid out in the Blueprint for an AI Bill of Rights can help guard the American public against many of the potential and actual harms identified by researchers, technologists, advocates, journalists, policymakers, and communities in the United States and around the world. This technical companion is intended to be used as a reference by people across many circumstances - anyone impacted by automated systems, and anyone developing, designing, deploying, evaluating, or making policy to govern the use of an automated system. \nEach principle is accompanied by three supplemental sections: \n1\nWHY  THIS  PRINCIPLE  IS  IMPORTANT: \nThis section provides a brief summary of the problems that the principle seeks to address and protect against, including illustrative examples. \n2\nWHAT  SHOULD  BE  EXPECTED  OF  AUTOMATED  SYSTEMS: \nĄ The expectations for automated systems are meant to serve as a blueprint for the development of additional technical standards and practices that should be tailored for particular sectors and contexts.\nĄ This section outlines practical steps that can be implemented to realize the vision of the Blueprint for an AI Bill of Rights. The expectations laid out often mirror existing practices for technology development, including pre-deployment testing, ongoing monitoring, and governance structures for automated systems, but also go further to address unmet needs for change and offer concrete directions for how those changes can be made. \nĄ Expectations about reporting are intended for the entity developing or using the automated system. The resulting reports can be provided to the public, regulators, auditors, industry standards groups, or others engaged in independent review, and should be made public as much as possible consistent with law, regulation, and policy, and noting that intellectual property, law enforcement, or national security considerations may prevent public release. Where public reports are not possible, the information should be provided to oversight bodies and privacy, civil liberties, or other ethics officers charged with safeguard -    ing individuals' rights. These reporting expectations are important for transparency, so the American people can have confidence that their rights, opportunities, and access as well as their expectations about technologies are respected. \n3\nHOW  THESE  PRINCIPLES  CAN  MOVE  INTO  PRACTICE: \nThis section provides real-life examples of how these guiding principles can become reality, through laws, policies, and practices. \nIt describes practical technical and sociotechnical approaches to protecting rights, opportunities, and access. \nThe examples provided are not critiques or endorsements, but rather are offered as illustrative cases to help provide a concrete vision for actualizing the Blueprint for an AI Bill of Rights. Effectively implementing these processes require the cooperation of and collaboration among industry, civil society, researchers, policymakers, technologists, and the public.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Risk factors: Transparency", "Strategies: Convening", "Risk factors: Safety"], "source": "https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf", "official_name": "Blueprint for an AI Bill of Rights", "label": "safe"}
{"id": "995_12", "doc_id": "995", "text": "Avoid inappropriate, low-quality, or irrelevant data use and the compounded harm of its reuse \n\nRelevant and high-quality data. Data used as part of any automated system's creation, evaluation, or deployment should be relevant, of high quality, and tailored to the task at hand.  Relevancy should be established based on research-backed demonstration of the causal influence of the data to the specific use case or justified more generally based on a reasonable expectation of usefulness in the domain and/or for the system design or ongoing development. Relevance of data should not be established solely by appealing to its historical connection to the outcome. High quality and tailored data should be representative of the task at hand and errors from data entry or other sources should be measured and limited. Any data used as the target of a prediction process should receive particular attention to the quality and validity of the predicted outcome or label to ensure the goal of the automated system is appropriately identified and measured. Additionally, justification should be documented for each data attribute and source to explain why it is appropriate to use that data to inform the results of the automated system and why such use will not violate any applicable laws. In cases of high-dimensional and/or derived attributes, such justifications can be provided as overall descriptions of the attribute generation process and appropriateness. \nDerived data sources tracked and reviewed carefully. Data that is derived from other data through the use of algorithms, such as data derived or inferred from prior model outputs, should be identified and tracked, e.g., via a specialized type in a data schema. Derived data should be viewed as potentially high-risk inputs that may lead to feedback loops, compounded harm, or inaccurate results. Such sources should be carefully validated against the risk of collateral consequences. \nData reuse limits in sensitive domains. Data reuse, and especially data reuse in a new context, can result in the spreading and scaling of harms. Data from some domains, including criminal justice data and data indicating adverse outcomes in domains such as finance, employment, and housing, is especially sensitive, and in some cases its reuse is limited by law. Accordingly, such data should be subject to extra oversight to ensure safety and efficacy. Data reuse of sensitive domain data in other contexts (e.g., criminal data reuse for civil legal  matters or private sector use) should only occur where use of such data is legally authorized and, after examination, has benefits for those impacted by the system that outweigh identified risks and, as appropriate, reasonable measures have been implemented to mitigate the identified risks. Such data should be clearly labeled to identify contexts for limited reuse based on sensitivity. Where possible, aggregated datasets may be useful for replacing individual-level sensitive data.", "tags": ["Strategies: Evaluation", "Risk factors: Reliability", "Risk factors: Reliability: Robustness", "Applications: Finance and investment", "Applications: Government: judicial and law enforcement", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Bias", "Strategies: Tiering", "Strategies: Tiering: Tiering based on inputs", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on impact", "Strategies: Tiering: Tiering based on generality"], "source": "https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf", "official_name": "Blueprint for an AI Bill of Rights", "label": "safe"}
{"id": "995_13", "doc_id": "995", "text": "Demonstrate the safety and effectiveness of the system\n \nIndependent evaluation. Automated systems should be designed to allow for independent evaluation (e.g., via application programming interfaces). Independent evaluators, such as researchers, journalists, ethics review boards, inspectors general, and third-party auditors, should be given access to the system and samples of associated data, in a manner consistent with privacy, security, law, or regulation (including, e.g., intellectual property law), in order to perform such evaluations. Mechanisms should be included to ensure that system access for evaluation is: provided in a timely manner to the deployment-ready version of the system; trusted to provide genuine, unfiltered access to the full system; and truly independent such that evaluator access cannot be revoked without reasonable and verified justification. \nReporting.12  Entities responsible for the development or use of automated systems should provide regularly-updated reports that include: an overview of the system, including how it is embedded in the organization's business processes or other activities, system goals, any human-run procedures that form a part of the system, and specific performance expectations; a description of any data used to train machine learning models or for other purposes, including how data sources were processed and interpreted, a summary of what data might be missing, incomplete, or erroneous, and data relevancy justifications; the results of public consultation such as concerns raised and any decisions made due to these concerns; risk identification and management assessments and any steps taken to mitigate potential harms; the results of performance testing including, but not limited to, accuracy, differential demographic impact, resulting error rates (overall and per demographic group), and comparisons to previously deployed systems; ongoing monitoring procedures and regular performance testing reports, including monitoring frequency, results, and actions taken; and the procedures for and results from independent evaluations. Reporting should be provided in a plain language and machine-readable manner.", "tags": ["Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Performance requirements", "Strategies: Disclosure: About incidents", "Strategies: Disclosure: About inputs", "Strategies: Convening", "Strategies: Disclosure: Accuracy thereof", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation: External auditing", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: About evaluation"], "source": "https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf", "official_name": "Blueprint for an AI Bill of Rights", "label": "safe"}
{"id": "995_19", "doc_id": "995", "text": "WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional technical standards and practices that are tailored for particular sectors and contexts. \nTraditional terms of service-the block of text that the public is accustomed to clicking through when using a website or digital app-are not an adequate mechanism for protecting privacy. The American public should be protected via built-in privacy protections, data minimization, use and collection limitations, and transparency, in addition to being entitled to clear mechanisms to control access to and use of their data-including their metadata-in a proactive, informed, and ongoing way. Any automated system collecting, using, sharing, or storing personal data should meet these expectations. \nProtect privacy by design and by default \nPrivacy by design and by default. Automated systems should be designed and built with privacy protected by default. Privacy risks should be assessed throughout the development life cycle, including privacy risks from reidentification, and appropriate technical and policy mitigation measures should be implemented. This includes potential harms to those who are not users of the automated system, but who may be harmed by inferred data, purposeful privacy violations, or community surveillance or other community harms. Data collection should be minimized and clearly communicated to the people whose data is collected. Data should only be collected or used for the purposes of training or testing machine learning models if such collection and use is legal and consistent with the expectations of the people whose data is collected. User experience research should be conducted to confirm that people understand what data is being collected about them and how it will be used, and that this collection matches their expectations and desires. \nData collection and use-case scope limits. Data collection should be limited in scope, with specific, narrow identified goals, to avoid \"mission creep.\"  Anticipated data collection should be determined to be strictly necessary to the identified goals and should be minimized as much as possible. Data collected based on these identified goals and for a specific context should not be used in a different context without assessing for new privacy risks and implementing appropriate mitigation measures, which may include express consent. Clear timelines for data retention should be established, with data deleted as soon as possible in accordance with legal or policy-based limitations. Determined data retention timelines should be documented and justified. \nRisk identification and mitigation. Entities that collect, use, share, or store sensitive data should attempt to proactively identify harms and seek to manage them so as to avoid, mitigate, and respond appropriately to identified risks. Appropriate responses include determining not to process data when the privacy risks outweigh the benefits or implementing measures to mitigate acceptable risks. Appropriate responses do not include sharing or transferring the privacy risks to users via notice or consent requests where users could not reasonably be expected to understand the risks without further support. \nPrivacy-preserving security. Entities creating, using, or governing automated systems should follow privacy and security best practices designed to ensure data and metadata do not leak beyond the specific consented use case. Best practices could include using privacy-enhancing cryptography or other types of privacy-enhancing technologies or charge-grained permissions and access control mechanisms, along with conventional system security protocols.", "tags": ["Risk factors: Privacy", "Strategies: Evaluation", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls", "Strategies: Input controls: Data circulation", "Strategies: Input controls: Data use", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Risk factors: Transparency", "Risk factors: Security"], "source": "https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf", "official_name": "Blueprint for an AI Bill of Rights", "label": "safe"}
{"id": "995_20", "doc_id": "995", "text": "Protect the public from unchecked surveillance \nHeightened oversight of surveillance. Surveillance or monitoring systems should be subject to heightened oversight that includes at a minimum assessment of potential harms during design (before deployment) and in an ongoing manner, to ensure that the American public's rights, opportunities, and access are protected. This assessment should be done before deployment and should give special attention to ensure there is not algorithmic discrimination, especially based on community membership, when deployed in a specific real-world context. Such assessment should then be reaffirmed in an ongoing manner as long as the system is in use. \nLimited and proportionate surveillance. Surveillance should be avoided unless it is strictly necessary to achieve a legitimate purpose and it is proportionate to the need. Designers, developers, and deployers of surveillance systems should use the least invasive means of monitoring available and restrict monitoring to the minimum number of subjects possible. To the greatest extent possible consistent with law enforcement and national security needs, individuals subject to monitoring should be provided with clear and specific notice before it occurs and be informed about how the data gathered through surveillance will be used. \nScope limits on surveillance to protect rights and democratic values. Civil liberties and civil rights must not be limited by the threat of surveillance or harassment facilitated or aided by an automated system. Surveillance systems should not be used to monitor the exercise of democratic rights, such as voting, privacy, peaceful assembly, speech, or association, in a way that limits the exercise of civil rights or civil liberties. Information about or algorithmically-determined assumptions related to identity should be carefully limited if used to target or guide surveillance systems in order to avoid algorithmic discrimination; such identity-related information includes group characteristics or affiliations, geographic designations, location-based and association-based inferences, social networks, and biometrics. Continuous surveillance and monitoring systems should not be used in physical or digital workplaces (regardless of employment status), public educational institutions, and public accommodations. Continuous surveillance and monitoring systems should not be used in a way that has the effect of limiting access to critical resources or services or suppressing the exercise of rights, even where the organization is not under a particular duty to protect those rights.", "tags": ["Strategies: Tiering", "Strategies: Tiering: Tiering based on impact", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Harms: Discrimination", "Applications: Government: judicial and law enforcement", "Harms: Violation of civil or human rights, including privacy", "Applications: Security", "Applications: Education", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Risk factors: Bias"], "source": "https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf", "official_name": "Blueprint for an AI Bill of Rights", "label": "safe"}
{"id": "995_21", "doc_id": "995", "text": "Provide the public with mechanisms for appropriate and meaningful consent, access, and control over their data \nUse-specific consent. Consent practices should not allow for abusive surveillance practices. Where data collectors or automated systems seek consent, they should seek it for specific, narrow use contexts, for specific time durations, and for use by specific entities. Consent should not extend if any of these conditions change; consent should be re-acquired before using data if the use case changes, a time limit elapses, or data is transferred to another entity (including being shared or sold). Consent requested should be limited in scope and should not request consent beyond what is required. Refusal to provide consent should be allowed, without adverse effects, to the greatest extent possible based on the needs of the use case. \nBrief and direct consent requests. When seeking consent from users short, plain language consent requests should be used so that users understand for what use contexts, time span, and entities they are providing data and metadata consent. User experience research should be performed to ensure these consent requests meet performance standards for readability and comprehension. This includes ensuring that consent requests are accessible to users with disabilities and are available in the language(s) and reading level appropriate for the audience.  User experience design choices that intentionally obfuscate or manipulate user choice (i.e., \"dark patterns\") should be not be used. \nData access and correction. People whose data is collected, used, shared, or stored by automated systems should be able to access data and metadata about themselves, know who has access to this data, and be able to correct it if necessary. Entities should receive consent before sharing data with other entities and should keep records of what data is shared and with whom. \nConsent withdrawal and data deletion. Entities should allow (to the extent legally permissible) withdrawal of data access consent, resulting in the deletion of user data, metadata, and the timely removal of their data from any systems (e.g., machine learning models) derived from that data.68\nAutomated system support. Entities designing, developing, and deploying automated systems should establish and maintain the capabilities that will allow individuals to use their own automated systems to help them make consent, access, and control decisions in a complex data ecosystem. Capabilities include machine readable data, standardized data formats, metadata or tags for expressing data processing permissions and preferences and data provenance and lineage, context of use and access-specific tags, and training models for assessing privacy risk.", "tags": ["Risk factors: Privacy", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Performance requirements", "Strategies: Input controls", "Strategies: Input controls: Data circulation", "Strategies: Input controls: Data use"], "source": "https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf", "official_name": "Blueprint for an AI Bill of Rights", "label": "safe"}
{"id": "995_22", "doc_id": "995", "text": "Demonstrate that data privacy and user control are protected \nIndependent evaluation. As described in the section on Safe and Effective Systems, entities should allow independent evaluation of the claims made regarding data policies. These independent evaluations should be made public whenever possible. Care will need to be taken to balance individual privacy with evaluation data access needs. \nReporting. When members of the public wish to know what data about them is being used in a system, the entity responsible for the development of the system should respond quickly with a report on the data it has collected or stored about them. Such a report should be machine-readable, understandable by most users, and include, to the greatest extent allowable under law, any data and metadata about them or collected from them, when and how their data and metadata were collected, the specific ways that data or metadata are being used, who has access to their data and metadata, and what time limitations apply to these data. In cases where a user login is not available, identity verification may need to be performed before providing such a report to ensure user privacy. Additionally, summary reporting should be proactively made public with general information about how peoples' data and metadata is used, accessed, and stored. Summary reporting should include the results of any surveillance pre-deployment assessment, including disparity assessment in the real-world deployment context, the specific identified goals of any data collection, and the assessment done to ensure only the minimum required data is collected. It should also include documentation about the scope limit assessments, including data retention timelines and associated justification, and an assessment of the impact of surveillance or data collection on rights, opportunities, and access. Where possible, this assessment of the impact of surveillance should be done by an independent party. Reporting should be provided in a clear and machine-readable manner.", "tags": ["Risk factors: Privacy", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Risk factors: Transparency", "Strategies: Disclosure: In standard form", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Post-market monitoring", "Harms: Violation of civil or human rights, including privacy"], "source": "https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf", "official_name": "Blueprint for an AI Bill of Rights", "label": "safe"}
{"id": "995_28", "doc_id": "995", "text": "Provide explanations as to how and why a decision was made or an action was taken by an automated system \nTailored to the purpose. Explanations should be tailored to the specific purpose for which the user is expected to use the explanation, and should clearly state that purpose. An informational explanation might differ from an explanation provided to allow for the possibility of recourse, an appeal, or one provided in the context of a dispute or contestation process. For the purposes of this framework, 'explanation' should be construed broadly. An explanation need not be a plain-language statement about causality but could consist of any mechanism that allows the recipient to build the necessary understanding and intuitions to achieve the stated purpose. Tailoring should be assessed (e.g., via user experience research). \nTailored to the target of the explanation. Explanations should be targeted to specific audiences and clearly state that audience. An explanation provided to the subject of a decision might differ from one provided to an advocate, or to a domain expert or decision maker. Tailoring should be assessed (e.g., via user experience research). \nTailored to the level of risk. An assessment should be done to determine the level of risk of the automated system. In settings where the consequences are high as determined by a risk assessment, or extensive oversight is expected (e.g., in criminal justice or some public sector settings), explanatory mechanisms should be built into the system design so that the system's full behavior can be explained in advance (i.e., only fully transparent models should be used), rather than as an after-the-decision interpretation. In other settings, the extent of explanation provided should be tailored to the risk level. \nValid. The explanation provided by a system should accurately reflect the factors and the influences that led to a particular decision, and should be meaningful for the particular customization based on purpose, target, and level of risk. While approximation and simplification may be necessary for the system to succeed based on the explanatory purpose and target of the explanation, or to account for the risk of fraud or other concerns related to revealing decision-making information, such simplifications should be done in a scientifically supportable way. Where appropriate based on the explanatory system, error ranges for the explanation should be calculated and included in the explanation, with the choice of presentation of such information balanced with usability and overall interface complexity concerns.", "tags": ["Risk factors: Transparency", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on impact", "Applications: Government: judicial and law enforcement", "Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Disclosure: Accuracy thereof"], "source": "https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf", "official_name": "Blueprint for an AI Bill of Rights", "label": "safe"}
{"id": "995_32", "doc_id": "995", "text": "WHAT  SHOULD  BE  EXPECTED  OF  AUTOMATED  SYSTEMS\n\nAn automated system should provide demonstrably effective mechanisms to opt out in favor of a human alternative, where appropriate, as well as timely human consideration and remedy by a fallback system, with additional human oversight and safeguards for systems used in sensitive domains, and with training and assessment for any human-based portions of the system to ensure effectiveness. \nProvide a mechanism to conveniently opt out from automated systems in favor of a human alternative, where appropriate \nBrief, clear, accessible notice and instructions. Those impacted by an automated system should be given a brief, clear notice that they are entitled to opt-out, along with clear instructions for how to opt-out. Instructions should be provided in an accessible form and should be easily findable by those impacted by the automated system. The brevity, clarity, and accessibility of the notice and instructions should be assessed (e.g., via user experience research). \nHuman alternatives provided when appropriate. In many scenarios, there is a reasonable expectation of human involvement in attaining rights, opportunities, or access. When automated systems make up part of the attainment process, alternative timely human-driven processes should be provided. The use of a human alternative should be triggered by an opt-out process. \nTimely and not burdensome human alternative. Opting out should be timely and not unreasonably burdensome in both the process of requesting to opt-out and the human-driven alternative provided.", "tags": ["Risk factors: Safety", "Risk factors: Transparency", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment"], "source": "https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf", "official_name": "Blueprint for an AI Bill of Rights", "label": "safe"}
{"id": "995_33", "doc_id": "995", "text": "Provide timely human consideration and remedy by a fallback and escalation system in the event that an automated system fails, produces error, or you would like to appeal or contest its impacts on you \nProportionate. The availability of human consideration and fallback, along with associated training and safeguards against human bias, should be proportionate to the potential of the automated system to meaningfully impact rights, opportunities, or access. Automated systems that have greater control over outcomes, provide input to high-stakes decisions, relate to sensitive domains, or otherwise have a greater potential to meaningfully impact rights, opportunities, or access should have greater availability (e.g., staffing) and oversight of human consideration and fallback mechanisms. \nAccessible. Mechanisms for human consideration and fallback, whether in-person, on paper, by phone, or otherwise provided, should be easy to find and use. These mechanisms should be tested to ensure that users who have trouble with the automated system are able to use human consideration and fallback, with the understanding that it may be these users who are most likely to need the human assistance. Similarly, it should be tested to ensure that users with disabilities are able to find and use human consideration and fallback and also request reasonable accommodations or modifications. \nConvenient. Mechanisms for human consideration and fallback should not be unreasonably burdensome as compared to the automated system's equivalent. \nEquitable.  Consideration should be given to ensuring outcomes of the fallback and escalation system are equitable when compared to those of the automated system and such that the fallback and escalation system provides equitable access to underserved communities.105 \nTimely. Human consideration and fallback are only useful if they are conducted and concluded in a timely manner. The determination of what is timely should be made relative to the specific automated system, and the review system should be staffed and regularly assessed to ensure it is providing timely consideration and fallback. In time-critical systems, this mechanism should be immediately available or, where possible, available before the harm occurs. Time-critical systems include, but are not limited to, voting-related systems, automated building access and other access systems, systems that form a critical component of healthcare, and systems that have the ability to withhold wages or otherwise cause immediate financial consequences. \nEffective. The organizational structure surrounding processes for consideration and fallback should be designed so that if the human decision-maker charged with reassessing a decision determines that it should be overruled, the new decision will be effectively enacted. This includes ensuring that the new decision is entered into the automated system throughout its components, any previous repercussions from the old decision are also overturned, and safeguards are put in place to help ensure that future decisions do not result in the same errors. \nMaintained. The human consideration and fallback process and any associated automated processes should be maintained and supported as long as the relevant automated system continues to be in use.", "tags": ["Risk factors: Safety", "Risk factors: Bias", "Strategies: Tiering", "Strategies: Tiering: Tiering based on impact", "Strategies: Tiering: Tiering based on domain of application", "Risk factors: Transparency", "Risk factors: Reliability", "Applications: Medicine, life sciences and public health", "Harms: Financial loss", "Applications: Finance and investment", "Harms: Discrimination"], "source": "https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf", "official_name": "Blueprint for an AI Bill of Rights", "label": "safe"}
{"id": "995_35", "doc_id": "995", "text": "Implement additional human oversight and safeguards for automated systems related to sensitive domains \nAutomated systems used within sensitive domains, including criminal justice, employment, education, and health, should meet the expectations laid out throughout this framework, especially avoiding capricious, inappropriate, and discriminatory impacts of these technologies. Additionally, automated systems used within sensitive domains should meet these expectations: \nNarrowly scoped data and inferences. Human oversight should ensure that automated systems in sensitive domains are narrowly scoped to address a defined goal, justifying each included data item or attribute as relevant to the specific use case. Data included should be carefully limited to avoid algorithmic discrimination resulting from, e.g., use of community characteristics, social network analysis, or group-based inferences. \nTailored to the situation. Human oversight should ensure that automated systems in sensitive domains are tailored to the specific use case and real-world deployment scenario, and evaluation testing should show that the system is safe and effective for that specific situation. Validation testing performed based on one location or use case should not be assumed to transfer to another. \nHuman consideration before any high-risk decision. Automated systems, where they are used in sensitive domains, may play a role in directly providing information or otherwise providing positive outcomes to impacted people. However, automated systems should not be allowed to directly intervene in high-risk situations, such as sentencing decisions or medical care, without human consideration. \nMeaningful access to examine the system. Designers, developers, and deployers of automated systems should consider limited waivers of confidentiality (including those related to trade secrets) where necessary in order to provide meaningful oversight of systems used in sensitive domains, incorporating measures to protect intellectual property and trade secrets from unwarranted disclosure as appropriate. This includes (potentially private and protected) meaningful access to source code, documentation, and related data during any associated legal discovery, subject to effective confidentiality or court orders. Such meaningful access should include (but is not limited to) adhering to the principle on Notice and Explanation using the highest level of risk so the system is designed with built-in explanations; such systems should use fully-transparent models where the model itself can be understood by people needing to directly examine it.", "tags": ["Applications: Medicine, life sciences and public health", "Applications: Government: judicial and law enforcement", "Applications: Education", "Applications: Business services and analytics", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Tiering", "Strategies: Tiering: Tiering based on domain of application", "Strategies: Tiering: Tiering based on impact", "Risk factors: Transparency", "Strategies: Convening", "Risk factors: Bias", "Risk factors: Interpretability and explainability"], "source": "https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf", "official_name": "Blueprint for an AI Bill of Rights", "label": "safe"}
