{"id": "1194_3", "doc_id": "1194", "text": "Emergency Preparedness Agreements and Institutions\n\nStates should agree on technical and institutional measures required to prepare for advanced AI systems, regardless of their development timescale. To facilitate these agreements, we need an international body to bring together AI safety authorities, fostering dialogue and collaboration in the development and auditing of AI safety regulations across different jurisdictions. This body would ensure states adopt and implement a minimal set of effective safety preparedness measures, including model registration, disclosure, and tripwires.\n\nOver time, this body could also set standards for and commit to using verification methods to enforce domestic implementations of the Safety Assurance Framework. These methods can be mutually enforced through encouragements and consequence mechanisms, such as conditioning access to markets on adherence with global standards. Experts and safety authorities should establish incident reporting and contingency plans, and regularly update the list of verified practices to reflect current scientific understanding. This body will be a critical initial coordination mechanism. In the long run, however, states will need to go further to ensure truly global governance of risks from advanced AI.", "tags": ["Risk factors: Safety", "Harms: Harm to health/safety", "Strategies: Convening", "Strategies: Evaluation", "Incentives: Access to business opportunities", "Strategies: Disclosure", "Strategies: Disclosure: About incidents", "Strategies: New institution", "Strategies: Licensing, registration, and certification"], "source": "https://idais.ai/dialogue/idais-venice/", "official_name": "Consensus Statement on AI Safety as a Global Public Good", "label": "safe"}
{"id": "1194_4", "doc_id": "1194", "text": "Safety Assurance Framework\n\nFrontier AI developers must demonstrate to domestic authorities that the systems they develop or deploy will not cross red lines such as those defined in the IDAIS-Beijing consensus statement.\n\nTo implement this, we need to build further scientific consensus on risks and red lines. Additionally, we should set early-warning thresholds: levels of model capabilities indicating that a model may cross or come close to crossing a red line. This approach builds on and harmonizes the existing patchwork of voluntary commitments such as responsible scaling policies. Models whose capabilities fall below early-warning thresholds require only limited testing and evaluation, while more rigorous assurance mechanisms are needed for advanced AI systems exceeding these early-warning thresholds.\n\nAlthough testing can alert us to risks, it only gives us a coarse-grained understanding of a model. This is insufficient to provide safety guarantees for advanced AI systems. Developers should submit a high-confidence safety case, i.e., a quantitative analysis that would convince the scientific community that their system design is safe, as is common practice in other safety-critical engineering disciplines. Additionally, safety cases for sufficiently advanced systems should discuss organizational processes, including encouragements and accountability structures, to favor safety.", "tags": ["Risk factors: Safety", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Tiering", "Strategies: Tiering: Tiering based on impact"], "source": "https://idais.ai/dialogue/idais-venice/", "official_name": "Consensus Statement on AI Safety as a Global Public Good", "label": "safe"}
{"id": "1379_5", "doc_id": "1379", "text": "2\tA Party must avoid be required to apply this Convention to activities within the lifecycle of artificial intelligence systems related to the protection of its national security interests, with the understanding that such activities are conducted in a manner consistent with applicable international law, including international human rights law obligations, and with respect for its democratic institutions and processes.\n\n3\tWithout prejudice to Article 13 and Article 25, paragraph 2, this Convention must avoid apply to exploratory work activities regarding artificial intelligence systems not yet made available for use, unless testing or similar activities are undertaken in such a way that they have the potential to interfere with human rights, democracy and the rule of law.\n\n4\tMatters relating to national defence do not fall within the scope of this Convention.", "tags": ["Strategies: Performance requirements"], "source": "https://rm.coe.int/1680afae3c", "official_name": "Council of Europe Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law", "label": "safe"}
{"id": "1396_2", "doc_id": "1396", "text": "Section 1: Principles for responsible stewardship of trustworthy AI\n\nII. RECOMMENDS that Members and non-Members adhering to this Recommendation (hereafter the “Adherents”) promote and implement the following principles for responsible stewardship of trustworthy AI, which are relevant to all stakeholders.\n\nIII. CALLS ON all AI actors to promote and implement, according to their respective roles, the following principles for responsible stewardship of trustworthy AI.\n\nIV. UNDERLINES that the following principles are complementary and should be considered as a whole.\n\n1.1. Inclusive growth, sustainable development and well-being\n\nStakeholders should proactively engage in responsible stewardship of trustworthy AI in pursuit of beneficial outcomes for people and the planet, such as augmenting human capabilities and enhancing creativity, advancing inclusion of underrepresented populations, reducing economic, social, gender and other inequalities, and protecting natural environments, thus invigorating inclusive growth, well-being, sustainable development and environmental sustainability.\n\n1.2. Respect for the rule of law, human rights and democratic values, including fairness and privacy\n\na) AI actors should respect the rule of law, human rights, democratic and human-centred values throughout the AI system lifecycle. These include non-discrimination and equality, freedom, dignity, autonomy of individuals, privacy and data protection, diversity, fairness, social justice, and internationally recognised labour rights. This also includes addressing misinformation and disinformation amplified by AI, while respecting freedom of expression and other rights and freedoms protected by applicable international law.\n\nb) To this end, AI actors should implement mechanisms and safeguards, such as capacity for human agency and oversight, including to address risks arising from uses outside of intended purpose, intentional misuse, or unintentional misuse in a manner appropriate to the context and consistent with the state of the art.", "tags": ["Risk factors: Reliability", "Harms: Detrimental content", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Privacy", "Harms: Discrimination"], "source": "https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449", "official_name": "Recommendation of the Council on Artificial Intelligence (5/2/2024)", "label": "safe"}
{"id": "1396_3", "doc_id": "1396", "text": "1.3. Transparency and explainability\n\nAI Actors should commit to transparency and responsible disclosure regarding AI systems. To this end, they should provide meaningful information, appropriate to the context, and consistent with the state of art:\n\ni. to foster a general understanding of AI systems, including their capabilities and limitations,\n\nii. to make stakeholders aware of their interactions with AI systems, including in the workplace,\n\niii. where feasible and useful, to provide plain and easy-to-understand information on the sources of data/input, factors, processes and/or logic that led to the prediction, content, recommendation or decision, to enable those affected by an AI system to understand the output, and,\n\niv. to provide information that enable those adversely affected by an AI system to challenge its output.\n\n1.4. Robustness, security and safety\n\na) AI systems should be robust, secure and safe throughout their entire lifecycle so that, in conditions of normal use, foreseeable use or misuse, or other adverse conditions, they function appropriately and do not pose unreasonable safety and/or security risks.\n\nb) Mechanisms should be in place, as appropriate, to ensure that if AI systems risk causing undue harm or exhibit undesired behaviour, they can be overridden, repaired, and/or decommissioned safely as needed.\n\nc) Mechanisms should also, where technically feasible, be in place to bolster information integrity while ensuring respect for freedom of expression.", "tags": ["Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About inputs", "Risk factors: Interpretability and explainability", "Risk factors: Reliability", "Risk factors: Reliability: Robustness", "Risk factors: Safety", "Risk factors: Security", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure: In standard form"], "source": "https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449", "official_name": "Recommendation of the Council on Artificial Intelligence (5/2/2024)", "label": "safe"}
{"id": "1407_1", "doc_id": "1407", "text": "An act relating to artificial intelligence use in political advertising; creating s. 106.145, F.S.; providing a definition; requiring certain political advertisements, electioneering communications, or other miscellaneous advertisements to include a specified disclaimer; specifying requirements for the disclaimer; providing for criminal and civil consequences; authorizing any person to file certain complaints; providing for expedited hearings; providing an effective date.\n\n\nBe It Enacted by the Legislature of the State of Florida:", "tags": ["Harms: Detrimental content", "Risk factors: Transparency", "Strategies: Disclosure", "Incentives: Criminal liability"], "source": "https://www.flsenate.gov/Session/Bill/2024/919/?Tab=BillText", "official_name": "Florida House Bill 919", "label": "safe"}
{"id": "1407_3", "doc_id": "1407", "text": "(4)(a) In addition to any civil consequences provided by law, a person identified pursuant to another disclaimer required under this chapter as paying for, sponsoring, or approving a political advertisement, an electioneering communication, or other miscellaneous advertisement of a political nature which is required to contain the disclaimer prescribed in this section and who fails to include the required disclaimer commits a misdemeanor of the first degree, punishable as provided in s. 775.082 or s. 775.083.\n(b) Any person may file a complaint with the Florida Elections Commission pursuant to s. 106.25 alleging a infraction of this section. The commission shall adopt rules to provide an\n66 expedited hearing of complaints filed under this section, or, in cases referred to the Division of Administrative Hearings pursuant to s. 106.25(5), the director shall assign an administrative law judge to provide an expedited hearing.\n\n\nSection 2. This act shall take effect July 1, 2024.", "tags": ["Incentives: Criminal liability"], "source": "https://www.flsenate.gov/Session/Bill/2024/919/?Tab=BillText", "official_name": "Florida House Bill 919", "label": "safe"}
{"id": "170_1", "doc_id": "170", "text": "§ 870.2785 Software for optical camera-based measurement of pulse rate, heart rate, breathing rate, and/or respiratory rate.\n\n(a) Identification.\n\nThe device uses software algorithms to analyze video signal and estimate pulse rate, heart rate, breathing rate, and/or respiratory rate. This device is not intended to independently direct therapy.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2023/01/31/2023-01967/medical-devices-cardiovascular-devices-classification-of-the-software-for-optical-camera-based", "official_name": "21 CFR § 870.2785 (\"Software for optical camera-based measurement of pulse rate, heart rate, breathing rate, and/or respiratory rate\")", "label": "safe"}
{"id": "170_2", "doc_id": "170", "text": "(b) Classification.\n\nClass II (special controls). The special controls for this device are:\n\n(1) A software description and the results of verification and validation testing based on a comprehensive hazard analysis and risk assessment must include:\n\n(i) A full characterization of the software technical parameters, including algorithms;\n\n(ii) If required image acquisition hardware is not included with the device, full specifications of the hardware requirements and testing to demonstrate the specified hardware ensures adequate data for validated and accurate measurements;\n\n(iii) A description of the expected impact of all applicable sensor acquisition hardware characteristics and associated hardware specifications;\n\n(iv) A description of all mitigations for user error or failure of any subsystem components (including signal detection, signal analysis, data display, and storage) on output accuracy; and\n\n(v) Software documentation must include a cybersecurity vulnerability and management process to assure software functionality.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2023/01/31/2023-01967/medical-devices-cardiovascular-devices-classification-of-the-software-for-optical-camera-based", "official_name": "21 CFR § 870.2785 (\"Software for optical camera-based measurement of pulse rate, heart rate, breathing rate, and/or respiratory rate\")", "label": "safe"}
{"id": "170_3", "doc_id": "170", "text": "(2) Clinical data must be provided. This assessment must fulfill the following:\n\n(i) The clinical data must be representative of the intended use population for the device. Any selection criteria or sample limitations must be fully described and justified.\n\n(ii) The assessment must demonstrate output consistency using the expected range of data sources and data quality encountered in the intended use population and environment.\n\n(iii) The assessment must compare device output with a clinically accurate patient-contacting relevant comparator device in an accurate and reproducible manner.\n\n(3) A human factors and usability engineering assessment must be provided that evaluates the risk of improper measurement.\n\n(4) Labeling must include:\n\n(i) A description of what the device measures and outputs to the user;\n\n(ii) Warnings identifying sensor acquisition factors or subject conditions or characteristics (garment types/textures, motion, etc.) that may impact measurement results;\n\n(iii) Guidance for interpretation of the measurements, including a statement that the output is adjunctive to other physical vital sign parameters and patient information;\n\n(iv) The expected performance of the device for all intended use populations and environments; and\n\n(v) Robust instructions to ensure correct system setup.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2023/01/31/2023-01967/medical-devices-cardiovascular-devices-classification-of-the-software-for-optical-camera-based", "official_name": "21 CFR § 870.2785 (\"Software for optical camera-based measurement of pulse rate, heart rate, breathing rate, and/or respiratory rate\")", "label": "safe"}
{"id": "173_1", "doc_id": "173", "text": "§ 882.1491 Pediatric Autism Spectrum Disorder diagnosis aid.\n\n(a) Identification.\n\nA pediatric Autism Spectrum Disorder diagnosis aid is a prescription device that is intended for use as an aid in the diagnosis of Autism Spectrum Disorder in pediatric patients.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/12/30/2022-28430/medical-devices-neurological-devices-classification-of-the-pediatric-autism-spectrum-disorder", "official_name": "21 CFR § 882.1491 (\"Pediatric Autism Spectrum Disorder diagnosis aid\")", "label": "safe"}
{"id": "173_2", "doc_id": "173", "text": "(b) Classification.\n\nClass II (special controls). The special controls for this device are:\n\n(1) Clinical performance testing must demonstrate that the device performs as intended under anticipated conditions of use, including an evaluation of sensitivity, specificity, positive predictive value, and negative predictive value using a reference method of diagnosis and assessment of patient behavioral symptomology.\n\n(2) Software verification, validation, and hazard analysis must be provided. Software documentation must include a detailed, technical description of the algorithm(s) used to generate device output(s), and a cybersecurity assessment of the impact of threats and vulnerabilities on device functionality and user(s).\n\n(3) Usability assessment must demonstrate that the intended user(s) can safely and correctly use the device.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/12/30/2022-28430/medical-devices-neurological-devices-classification-of-the-pediatric-autism-spectrum-disorder", "official_name": "21 CFR § 882.1491 (\"Pediatric Autism Spectrum Disorder diagnosis aid\")", "label": "safe"}
{"id": "173_3", "doc_id": "173", "text": "(4) Labeling must include:\n\n(i) Instructions for use, including a detailed description of the device, compatibility information, and information to facilitate clinical interpretation of all device outputs; and\n\n(ii) A summary of any clinical testing conducted to demonstrate how the device functions as an interpretation of patient behavioral symptomology associated with Autism Spectrum Disorder. The summary must include the following:\n\n(A) A description of each device output and clinical interpretation;\n\n(B) Any performance measures, including sensitivity, specificity, positive predictive value (PPV) and negative predictive value (NPV);\n\n(C) A description of how the cutoff values used for categorical classification of diagnoses were determined; and\n\n(D) Any expected or observed adverse events and complications.\n\n(iii) A statement that the device is not intended for use as a stand-alone diagnostic.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Risk factors: Transparency", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Strategies: Licensing, registration, and certification", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2022/12/30/2022-28430/medical-devices-neurological-devices-classification-of-the-pediatric-autism-spectrum-disorder", "official_name": "21 CFR § 882.1491 (\"Pediatric Autism Spectrum Disorder diagnosis aid\")", "label": "safe"}
{"id": "182_1", "doc_id": "182", "text": "§ 892.2060 Radiological computer-assisted diagnostic software for lesions suspicious of cancer.\n\n(a) Identification.\n\nA radiological computer-assisted diagnostic software for lesions suspicious of cancer is an image processing prescription device intended to aid in the characterization of lesions as suspicious for cancer identified on acquired medical images such as magnetic resonance, mammography, radiography, or computed tomography. The device characterizes lesions based on features or information extracted from the images and provides information about the lesion(s) to the user. Diagnostic and patient management decisions are made by the clinical user.", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2020/01/22/2020-00497/medical-devices-radiology-devices-classification-of-the-radiological-computer-assisted-diagnostic", "official_name": "21 CFR § 892.2060 (\"Radiological computer-assisted diagnostic software for lesions suspicious of cancer\")", "label": "safe"}
{"id": "182_2", "doc_id": "182", "text": "(b) Classification.\n\nClass II (special controls). The special controls for this device are:\n\n(1) Design verification and validation must include:\n\n(i) A detailed description of the image analysis algorithms including, but not limited to, a detailed description of the algorithm inputs and outputs, each major component or block, and algorithm limitations.\n\n(ii) A detailed description of pre-specified performance testing protocols and dataset(s) used to assess whether the device will improve reader performance as intended.\n\n(iii) Results from performance testing protocols that demonstrate that the device improves reader performance in the intended use population when used in accordance with the instructions for use. The performance assessment must be based on appropriate diagnostic accuracy measures (e.g., receiver operator characteristic plot, sensitivity, specificity, predictive value, and diagnostic likelihood ratio). The test dataset must contain sufficient numbers of cases from important cohorts (e.g., subsets defined by clinically relevant confounders, effect modifiers, concomitant diseases, and subsets defined by image acquisition characteristics) such that the performance estimates and confidence intervals of the device for these individual subsets can be characterized for the intended use population and imaging equipment.\n\n(iv) Standalone performance testing protocols and results of the device.\n\n(v) Appropriate software documentation (e.g., device hazard analysis; software requirements specification document; software design specification document; traceability analysis; and description of verification and validation activities including system level test protocol, pass/fail criteria, results, and cybersecurity).", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2020/01/22/2020-00497/medical-devices-radiology-devices-classification-of-the-radiological-computer-assisted-diagnostic", "official_name": "21 CFR § 892.2060 (\"Radiological computer-assisted diagnostic software for lesions suspicious of cancer\")", "label": "safe"}
{"id": "182_3", "doc_id": "182", "text": "(2) Labeling must include:\n\n(i) A detailed description of the patient population for which the device is indicated for use.\n\n(ii) A detailed description of the intended reading protocol.\n\n(iii) A detailed description of the intended user and recommended user training.\n\n(iv) A detailed description of the device inputs and outputs.\n\n(v) A detailed description of compatible imaging hardware and imaging protocols.\n\n(vi) Warnings, precautions, and limitations, including situations in which the device may fail or may not operate at its expected performance level (e.g., poor image quality or for certain subpopulations), as applicable.\n\n(vii) Detailed instructions for use.\n\n(viii) A detailed summary of the performance testing, including: Test methods, dataset characteristics, results, and a summary of sub-analyses on case distributions stratified by relevant confounders (e.g., lesion and organ characteristics, disease stages, and imaging equipment).", "tags": ["Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Harms: Harm to health/safety", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Applications: Medicine, life sciences and public health"], "source": "https://www.federalregister.gov/documents/2020/01/22/2020-00497/medical-devices-radiology-devices-classification-of-the-radiological-computer-assisted-diagnostic", "official_name": "21 CFR § 892.2060 (\"Radiological computer-assisted diagnostic software for lesions suspicious of cancer\")", "label": "safe"}
{"id": "1972_1", "doc_id": "1972", "text": "[footnotes omitted]\nCalifornia Attorney General’s Legal Advisory on the Application\nof Existing California Law to Artificial Intelligence in Healthcare\nThe California Attorney General’s Office (AGO) issues this advisory to provide guidance to healthcare providers,\ninsurers, vendors, investors, and other healthcare entities that develop, sell, and use artificial intelligence (AI) and\nother automated decision systems1\n about their obligations under California law, including under the state’s consumer\nprotection, civil rights, competition, and data privacy laws.2", "tags": ["Risk factors: Bias", "Harms: Discrimination", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Interpretability and explainability", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Government: benefits and welfare", "Strategies: Performance requirements", "Strategies: Disclosure: Accuracy thereof", "Strategies: Disclosure", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Privacy", "Harms: Detrimental content", "Harms: Financial loss", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Risk factors: Reliability", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Applications: Government: benefits and welfare", "Strategies: Input controls: Data use", "Strategies: Performance requirements", "Harms: Discrimination", "Risk factors: Bias", "Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Bias", "Applications: Government: benefits and welfare", "Risk factors: Privacy", "Strategies: Input controls", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Government: benefits and welfare", "Applications: Medicine, life sciences and public health", "Strategies: Input controls: Data circulation", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls", "Strategies: Input controls: Data circulation", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls: Data use", "Harms: Harm to health/safety", "Risk factors: Safety"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Final%20Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence%20in%20Healthcare.pdf", "official_name": "Legal Advisory on the Application of Existing California Law to Artificial Intelligence in Healthcare (California Attorney General)", "label": "safe"}
{"id": "1972_2", "doc_id": "1972", "text": "Artificial Intelligence in the Healthcare Sector\nAI systems are already widespread within healthcare. As of May 2024, the federal Food and Drug Administration\n(FDA) had authorized for medical use 981 artificial intelligence or machine learning software devices, and counting.3\nThese and other AI systems are being used to guide medical diagnosis and treatment decisions. Hospitals and\ninsurers routinely use non-FDA-approved AI systems for tasks such as appointment scheduling, medical risk\nassessment, and bill processing.\nAI tools have the potential to help improve patient and population health, increase health equity, reduce\nadministrative burdens, and facilitate appropriate information sharing. At the same time, AI risks causing\ndiscrimination, denials of needed care and other misallocations of healthcare resources, and interference with\npatient autonomy and privacy. For example, AI models trained on data that reflect existing biases in healthcare\ndelivery can exacerbate health inequity.4\nMany patients are not aware of when and how AI systems are used in\nconnection with their healthcare. Moreover, AI systems are novel and complex. Their inner workings are often not\nunderstood by the healthcare providers using AI, let alone patients receiving care.", "tags": ["Risk factors: Bias", "Harms: Discrimination", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Interpretability and explainability", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Government: benefits and welfare", "Strategies: Performance requirements", "Strategies: Disclosure: Accuracy thereof", "Strategies: Disclosure", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Privacy", "Harms: Detrimental content", "Harms: Financial loss", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Risk factors: Reliability", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Applications: Government: benefits and welfare", "Strategies: Input controls: Data use", "Strategies: Performance requirements", "Harms: Discrimination", "Risk factors: Bias", "Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Bias", "Applications: Government: benefits and welfare", "Risk factors: Privacy", "Strategies: Input controls", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Government: benefits and welfare", "Applications: Medicine, life sciences and public health", "Strategies: Input controls: Data circulation", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls", "Strategies: Input controls: Data circulation", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls: Data use", "Harms: Harm to health/safety", "Risk factors: Safety"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Final%20Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence%20in%20Healthcare.pdf", "official_name": "Legal Advisory on the Application of Existing California Law to Artificial Intelligence in Healthcare (California Attorney General)", "label": "safe"}
{"id": "1972_3", "doc_id": "1972", "text": "Healthcare-related entities that develop, sell, or use AI systems must ensure that their systems comply with laws\nprotecting consumers. This requires understanding how AI systems are trained, what information the systems\nconsider, and how the systems generate output. Developers, researchers, providers, insurers, and related\norganizations should ensure that AI systems are tested, validated, and audited to ensure that their use is safe,\nethical, and lawful, and reduces, rather than replicates or exaggerates, human error and biases. They should also be\ntransparent with patients about whether patient information is being used to train AI and how providers are using AI\nto make decisions affecting health and healthcare. \nFor example, it may be unlawful in California to:\n• Deny health insurance claims using AI or other automated decisionmaking systems in a manner that\noverrides doctors’ views about necessary treatment.\n• Use generative AI or other automated decisionmaking tools to draft patient notes, communications,\nor medical orders that include erroneous or misleading information, including information based on\nstereotypes relating to race or other protected classifications.\n• Determine patient access to healthcare using AI or other automated decisionmaking systems that make\npredictions based on patients’ past healthcare claims data, resulting in disadvantaged patients or groups that\nhave a history of lack of access to healthcare being denied services on that basis while patients/groups with\nrobust past access being provided enhanced services.\n• Double-book a patient’s appointment, or create other administrative barriers, because AI or other\nautomated decisionmaking systems predict that patient is the “type of person” more likely to miss an\nappointment.\n• Conduct cost/benefit analysis of medical treatments for patients with disabilities using AI or other\nautomated decisionmaking systems that are based on stereotypes that undervalue the lives of people with\ndisabilities.", "tags": ["Risk factors: Bias", "Harms: Discrimination", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Interpretability and explainability", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Government: benefits and welfare", "Strategies: Performance requirements"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Final%20Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence%20in%20Healthcare.pdf", "official_name": "Legal Advisory on the Application of Existing California Law to Artificial Intelligence in Healthcare (California Attorney General)", "label": "safe"}
{"id": "1972_4", "doc_id": "1972", "text": "The AGO recognizes that the California Legislature and regulatory agencies continue to develop laws and regulations\naddressing emerging technology. This advisory provides guidance on the application of existing California law to\nAI use in healthcare. This advisory does not encompass all possible laws that may apply to health AI, including\napplicable federal requirements, such as the FDA’s regulation of software as a medical device and research into\nAI in medicine; the Federal Trade Commission Act; the U.S. Department of Health and Human Services (HHS)\nAssistant Secretary for Technology Policy and Office of the National Coordinator for Health Information Technology\nstandards and final rule applying Section 1557 (the Affordable Care Act’s non-discrimination mandate) to automated\npatient care decision support tools, and its guidance to Medicare Advantage plans on use of AI and other forms\nof automated decisionmaking; the National Institute of Standards and Technology’s draft AI risk management\nframework; and the Biden administration Executive Order on AI and draft guidelines of the Office of Management\nand Budget on AI.", "tags": ["Risk factors: Bias", "Harms: Discrimination", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Interpretability and explainability", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Government: benefits and welfare", "Strategies: Performance requirements", "Strategies: Disclosure: Accuracy thereof", "Strategies: Disclosure", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Privacy", "Harms: Detrimental content", "Harms: Financial loss", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Risk factors: Reliability", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Applications: Government: benefits and welfare", "Strategies: Input controls: Data use", "Strategies: Performance requirements", "Harms: Discrimination", "Risk factors: Bias", "Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Bias", "Applications: Government: benefits and welfare", "Risk factors: Privacy", "Strategies: Input controls", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Government: benefits and welfare", "Applications: Medicine, life sciences and public health", "Strategies: Input controls: Data circulation", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls", "Strategies: Input controls: Data circulation", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls: Data use", "Harms: Harm to health/safety", "Risk factors: Safety"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Final%20Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence%20in%20Healthcare.pdf", "official_name": "Legal Advisory on the Application of Existing California Law to Artificial Intelligence in Healthcare (California Attorney General)", "label": "safe"}
{"id": "1972_7", "doc_id": "1972", "text": "California’s professional licensing laws provide additional standards to which licensed medical professionals must\nadhere. (Bus. & Prof. Code, Division 2 (commencing with Section 500).) Only human physicians (and other medical\nprofessionals) are licensed to practice medicine in California; California law does not allow delegation of the practice\nof medicine to AI. Licensed physicians may violate conflict of interest law if they or their family member have a\nfinancial interest in AI services and must disclose any financial conflict when consulting with AI organizations. (Lab.\nCode, § 139.3, subds. (a), (e).) Furthermore, using AI or other automated decision tools to make decisions about\npatients’ medical treatment, or to override licensed care providers’ determinations about what a patient’s medical\nneeds are, may violate California’s block on the practice of medicine by corporations and other “artificial legal entities”\n(Bus. & Prof. Code, § 2400 et seq.),6\n in addition to constituting an “unlawful” or “unfair” business practice under the\nUnfair Competition Law.\nRecent amendments to the Knox-Keene Act and California Insurance Code limit health care service plans’ ability to\nuse AI or other automated decision systems to deny coverage. (See Sen. Bill No. 1120 (2023-2024).) When employed\nfor utilization review or management purposes, a plan cannot use these types of tools to “deny, delay, or modify\nhealth care services based, in whole or in part, on medical necessity.” (Health & Saf. Code, § 1367.01, subd. (k)(1);\nIns. Code, § 10123.135, subd. (j)(2).) Instead, plans must ensure that AI and other software:\n• Does not supplant a licensed health care provider’s decisionmaking;\n• Bases decisions on individual enrollees’ own medical history and clinical circumstances;\n• Does not discriminate, and is applied fairly and equitably;\n• Is open to inspection and review by relevant state agencies;\n• Is periodically reviewed and revised to maximize accuracy and reliability;\n• Does not use patient data beyond its intended and stated purpose; and\n• Does not directly or indirectly cause harm to the plan enrollee.\n(Health & Saf. Code, § 1367.01, subd. (k)(1)(A-K); Ins. Code, § 10123.135, subd. (j)(1)(A-K).)", "tags": ["Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Risk factors: Reliability", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Applications: Government: benefits and welfare", "Strategies: Input controls: Data use", "Strategies: Performance requirements"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Final%20Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence%20in%20Healthcare.pdf", "official_name": "Legal Advisory on the Application of Existing California Law to Artificial Intelligence in Healthcare (California Attorney General)", "label": "safe"}
{"id": "1972_8", "doc_id": "1972", "text": "B. California Anti-Discrimination Laws\nCalifornia law prohibits discrimination by any entity or individual receiving “any state support,” including an “entity\nprincipally engaged in the business of providing […] health care.” (Gov. Code, § 11135; Cal. Code Regs., tit. 2, §\n14020, subd. (m)(6)(B); see also id. at (ii) [covered programs or activities include provision of health services].)\nDiscrimination is restricted based on any or a combination of the following classifications: “sex, race, color, religion,\nancestry, national origin, ethnic group identification, age, mental disability, physical disability, medical condition,\ngenetic information, marital status, or sexual orientation.” (Gov. Code, § 11135; Cal. Code Regs., tit. 2, § 14000, subd.\n(e).)\nThis non-discrimination mandate covers healthcare programs or activities broadly because “state support” may come\nin the form of “any payments, subsidies, or other assistance extended to any person, agency or entity providing\ninsurance, including health-related insurance coverage for payments to or on behalf of a person obtaining healthrelated insurance coverage from that entity […].” (Id. § 14020, subd. (ww)(5) (emphasis added).). For example, this\nincludes state Medi-Cal services. And the non-discrimination mandate extends to all “operations of the covered\nentity […] even if only one part of the covered entity receives state support,” including “any service, activity, financial\naid or benefit provided in, at or through a facility that is or was provided by the state or any state agency or with the\naid or benefit of state support or other funds or resources.” (Id. § 14020, subd. (ii)(1-2).)", "tags": ["Risk factors: Bias", "Harms: Discrimination", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Interpretability and explainability", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Government: benefits and welfare", "Strategies: Performance requirements", "Strategies: Disclosure: Accuracy thereof", "Strategies: Disclosure", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Privacy", "Harms: Detrimental content", "Harms: Financial loss", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Risk factors: Reliability", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Applications: Government: benefits and welfare", "Strategies: Input controls: Data use", "Strategies: Performance requirements", "Harms: Discrimination", "Risk factors: Bias", "Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements", "Harms: Discrimination", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Bias", "Applications: Government: benefits and welfare", "Risk factors: Privacy", "Strategies: Input controls", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Government: benefits and welfare", "Applications: Medicine, life sciences and public health", "Strategies: Input controls: Data circulation", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls", "Strategies: Input controls: Data circulation", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls: Data use", "Harms: Harm to health/safety", "Risk factors: Safety"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Final%20Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence%20in%20Healthcare.pdf", "official_name": "Legal Advisory on the Application of Existing California Law to Artificial Intelligence in Healthcare (California Attorney General)", "label": "safe"}
{"id": "1972_9", "doc_id": "1972", "text": "These rules restrict the types of discriminatory practices likely to be caused by AI, including disparate impact\ndiscrimination (also known as “discriminatory effect” or “adverse impact”) and denial of full and equal access.7\n(Cal. Code Regs., tit. 2, § 14027, subd. (b)(3).) For example, an AI system that makes less accurate predictions about\ndemographic groups of people who have historically faced barriers to healthcare (and whose information may\nbe underrepresented in large datasets), though facially neutral, may have a disproportionate negative impact on\nmembers of protected groups.8\n Classifications that are protected under section 11135 may frequently overlap with\nlower income and social marginalization. Even if such models are applied to all patients regardless of race, they may\nstill cause disparate impact discrimination because “identical treatment may be discriminatory.” (Id. § 14025, subd.\n(a)(3).) A disparate impact is permissible only if the covered entity can show that the AI system’s use is necessary\nfor achieving a compelling, legitimate, and nondiscriminatory purpose, and supported by evidence that is not\nhypothetical or speculative. (Id. § 14029, subd. (c)(1, 2).)\nAlthough a policy or tool may be facially neutral, healthcare entities may not simply ignore or avoid data regarding\ninequity relating to race, gender, or another protected classification. To the contrary, recipients of state support\nmay be required or permitted to take ameliorative steps to overcome the effects of past discrimination, or prevent\nnew discrimination.9\n (Id. § 14053; see also id. at § 14003, subd. (b) (California regulations should not be interpreted\nto adversely impact programs or activities that benefit protected subgroups in order to overcome effects of past\nexclusion or reduced access).", "tags": ["Harms: Discrimination", "Risk factors: Bias", "Harms: Violation of civil or human rights, including privacy", "Strategies: Performance requirements"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Final%20Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence%20in%20Healthcare.pdf", "official_name": "Legal Advisory on the Application of Existing California Law to Artificial Intelligence in Healthcare (California Attorney General)", "label": "safe"}
{"id": "1972_11", "doc_id": "1972", "text": "C. California’s Patient Privacy and Autonomy Laws\nVast quantities of patient data underlie the massive growth in the health AI sector. Data is used to build and train\nAI and to render decisions that impact health services. Developers and entities that use AI in healthcare should\ncarefully monitor training data, inputs, and outputs to ensure respect for Californians’ rights to medical privacy.\nCalifornia state medical privacy laws provide protections that are, in some cases, more stringent than federal health\nprivacy laws like HIPAA (the Health Insurance Portability and Accountability Act of 1996, 45 C.F.R. Parts 160 and\n164).11 The Confidentiality of Medical Information Act (CMIA) and the Information Practices Act govern use and\ndisclosure of Californians’ medical information. Covered entities must preserve confidentiality of patients’ medical\ninformation and ensure that patients have access to that information. (Civ. Code, §§ 56.10, 56.26, 1798.25.) Sensitive\ninformation, including mental and behavioral healthcare and reproductive and sexual healthcare (e.g., abortion and\ngender affirming care), receive heightened protections. (Civ. Code, § 56.05, subd. (s).) Medical privacy laws apply\nto governmental healthcare agencies,12 medical providers, and insurance plans, as well as businesses that offer\nsoftware or hardware to consumers for the purposes of managing medical information, diagnosis or treatment, or\nmanagement of medical conditions, via mobile applications or other related devices. (Civ. Code, § 56.06, subds. (a),\n(b).)\nCalifornia law requires that physicians provide information that a reasonable person in the patient’s position would\nneed for informed consent to a proposed course of treatment. (Cal. Code Regs., tit. 9, § 784.29, subd. (a) [patients’\nrights in mental health rehabilitation centers], tit. 22, § 70707 [patient rights in acute care hospitals].) Providers\nshould consider whether this applies to their use of AI tools, as a majority of Californians are currently uncomfortable\nwith use of AI in connection with healthcare.13 If a patient is asked to participate in a medical experiment using AI\nsystems, they are entitled to California’s “Experimental Subject’s Bill of Rights,” including information explaining the\nprocedures to be followed in the medical experiment, and drugs and devices used. (Health & Saf. Code, § 24172.)14", "tags": ["Risk factors: Privacy", "Strategies: Input controls", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Applications: Government: benefits and welfare", "Applications: Medicine, life sciences and public health", "Strategies: Input controls: Data circulation", "Harms: Violation of civil or human rights, including privacy"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Final%20Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence%20in%20Healthcare.pdf", "official_name": "Legal Advisory on the Application of Existing California Law to Artificial Intelligence in Healthcare (California Attorney General)", "label": "safe"}
{"id": "1972_12", "doc_id": "1972", "text": "Significant recent amendments to the CMIA require that providers and electronic health records (EHR) and digital\nhealth companies enable patients to keep their reproductive and sexual health information confidential and separate\nfrom the rest of their medical records.15 (Civ. Code, § 56.101, subds. (a), (c).) They must prevent disclosure, access,\ntransfer, or processing of this information to individuals and entities outside of California. (Id. subd. (c)(1)(D).) As\ndevelopers and users of EHRs and related applications increasingly incorporate AI, they must ensure adherence with\nCMIA and limit access and improper use to sensitive information.\nThe CMIA also imposes independent requirements on healthcare providers, insurers, and others to get patients’\nconsent before disclosing medical information. (Civ. Code, § 56.10, subd. (a).) The Genetic Privacy Information Act\nprovides special protections for individuals’ genetic data, and California healthcare service plans and other entities\nare restricted from disclosing to third parties the results of genetic tests without the patient’s permission. (Civ.\nCode, §§ 56.17, 56.18, et seq.) “Dark patterns”—user interfaces “designed or manipulated with the substantial effect\nof subverting or impairing user autonomy, decisionmaking, or choice,” including those generated by AI—cannot be\nused to obtain patient consent. (Civ. Code, § 56.18, subd. (b)(6).) Under the Patient Access to Health Records Act,\nCalifornia patients and their representatives have the right to obtain their own medical records. (Health & Saf. Code,\n§§ 123110, et seq.) Likewise, the Insurance Information and Privacy Protection Act gives healthcare consumers the\nrights to determine what information has been collected about them, and the reasons for adverse decisions. (Ins. Code, § 791.) Developers and users of AI must have sufficient control over their systems to ensure that Californian\npatients’ rights to privacy and autonomy are not compromised.\n\nApart from these healthcare-specific privacy laws, California has general privacy laws that apply to the use of AI. For\ninformation concerning California state privacy laws and AI, including the constitutional right to privacy that applies\nto both government and private entities (see Hill v. National Collegiate Athletic Assn. (1994) 7 Cal.4th 1, 20) and the\nCalifornia Consumer Privacy Act, see Attorney General Bonta’s recent general consumer legal advisory on AI.", "tags": ["Strategies: Input controls", "Strategies: Input controls: Data circulation", "Risk factors: Privacy", "Harms: Violation of civil or human rights, including privacy", "Strategies: Input controls: Data use"], "source": "https://oag.ca.gov/system/files/attachments/press-docs/Final%20Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence%20in%20Healthcare.pdf", "official_name": "Legal Advisory on the Application of Existing California Law to Artificial Intelligence in Healthcare (California Attorney General)", "label": "safe"}
{"id": "1989_13", "doc_id": "1989", "text": "(5) The business must specifically identify the negative impacts to consumers’ privacy associated with the processing. The business must identify the sources and causes of these negative impacts, and any criteria that the business used to make these determinations. Negative impacts to consumers’ privacy that a business may consider include the following: \n(A) Unauthorized access, destruction, use, modification, or disclosure of personal information; and unauthorized activity resulting in the loss of availability of personal information. \n(B) Discrimination upon the basis of protected classes that would violate federal or state antidiscrimination law. \n(C) Impairing consumers’ control over their personal information, such as by providing insufficient information for consumers to make an informed decision regarding the processing of their personal information, or by interfering with consumers’ ability to make choices consistent with their reasonable expectations. \n(D) Coercing or compelling consumers into allowing the processing of their personal information, such as by conditioning consumers’ acquisition or use of an online service upon their disclosure of personal information that is unnecessary to the expected functionality of the service, or requiring consumers to consent to processing when such consent cannot be freely given. \n(E) Disclosing a consumer’s media consumption (e.g., books they have read or videos they have watched) in a manner that chills or deters their speech, expression, or exploration of ideas. \n(F) Economic harms, including limiting or depriving consumers of economic opportunities; charging consumers higher prices; compensating consumers at lower rates; or imposing additional costs upon consumers, including costs associated with the unauthorized access to consumers’ personal information. \n(G) Physical harms to consumers or to property, including processing that creates the opportunity for physical or sexual violence. \n(H) Reputational harms, including stigmatization, that would negatively impact an average consumer. Examples of processing activities that result in such harms include a mobile dating application’s disclosure of a consumer’s sexual or other preferences in a partner; a business stating or implying that a consumer has committed a crime without verifying this information; or a business processing consumers’ biometric information to create a deepfake of them. (I) Psychological harms, including emotional distress, stress, anxiety, embarrassment, fear, frustration, shame, and feelings of infraction, that would negatively impact an average consumer. Examples of such harms include emotional distress resulting from disclosure of nonconsensual intimate imagery; stress and anxiety from regularly targeting a consumer who visits websites for substance abuse resources with advertisements for alcohol; or emotional distress from disclosing a consumer’s purchase of pregnancy tests or emergency contraception for non-medical purposes.", "tags": ["Strategies: Evaluation", "Risk factors: Privacy", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Harms: Discrimination", "Risk factors: Transparency", "Harms: Violation of civil or human rights, including privacy", "Harms: Financial loss", "Risk factors: Safety", "Harms: Harm to health/safety"], "source": "https://cppa.ca.gov/meetings/materials/20240308_item4_draft_risk.pdf?utm_source=substack&utm_medium=email", "official_name": "California Privacy Protection Agency Draft Risk Assessment and Automated Decisionmaking Technology Regulations ", "label": "safe"}
{"id": "1989_14", "doc_id": "1989", "text": "(6) The business must identify the safeguards that it plans to implement to address the negative impacts identified in subsection (a)(5). The business must specifically identify how these safeguards address the negative impacts identified in subsection (a)(5), including to what extent they eliminate or reduce the negative impacts; and identify any safeguards the business will implement to maintain knowledge of emergent risks and countermeasures. \n(A) Safeguards that a business may consider include the following:\n(i) Encryption, segmentation of information systems, physical and logical access controls, change management, network monitoring and defenses, and data and integrity monitoring; \n(ii) Use of privacy-enhancing technologies, such as trusted execution environments, federated learning, homomorphic encryption, and differential privacy; \n(iii) Consulting external parties, such as those described in section 7151, subsection (b), to ensure that the business maintains current knowledge of emergent privacy risks and countermeasures; and using that knowledge to identify, assess, and mitigate risks to consumers’ privacy; and \n(iv) Evaluating the need for human involvement as part of the business’s use of automated decisionmaking technology, and implementing policies, procedures, and training to address the degree and details of human involvement identified as necessary in that evaluation. \n(B) For uses of automated decisionmaking technology set forth in section 7150, subsection (b)(3), the business must identify the following: \n(i) Whether it evaluated the automated decisionmaking technology to ensure it works as intended for the business’s proposed use and does not discriminate based upon protected classes (“evaluation of the automated decisionmaking technology”); and \n(ii) The policies, procedures, and training the business has implemented or plans to implement to ensure that the automated decisionmaking technology works as intended for the business’s proposed use and does not discriminate based upon protected classes (“accuracy and nondiscrimination safeguards”). For example, if a business determines that the use of low-quality enrollment images creates a high risk of falsepositive matches in its proposed use of facial-recognition technology, the business must identify the policies, procedures, and training it has implemented or plans to implement to ensure that it is using only sufficiently high-quality enrollment images to mitigate that risk\n(iii) Where a business obtains the automated decisionmaking technology from another person, the business must identify the following: \n1. Whether it reviewed that person’s evaluation of the automated decisionmaking technology, and whether that person’s evaluation included any requirements or limitations relevant to the business’s proposed use of the automated decisionmaking technology. \n2. Any accuracy and nondiscrimination safeguards that it implemented or plans to implement.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Convening", "Risk factors: Privacy", "Risk factors: Reliability"], "source": "https://cppa.ca.gov/meetings/materials/20240308_item4_draft_risk.pdf?utm_source=substack&utm_medium=email", "official_name": "California Privacy Protection Agency Draft Risk Assessment and Automated Decisionmaking Technology Regulations ", "label": "safe"}
{"id": "1989_18", "doc_id": "1989", "text": "§ 7155. Timing and Retention Requirements for Risk Assessments. \n\n\n(a) A business must comply with the following timing requirements for conducting and updating its risk assessments: \n(1) A business must conduct and document a risk assessment in accordance with the requirements of this Article before initiating any processing activity identified in section 7150, subsection (b). \n(2) At least once every three years, a business must review, and update as necessary, its risk assessments to ensure that they remain accurate in accordance with the requirements of this Article. \n(3) Notwithstanding subsection (a)(2) of this section, a business must immediately update a risk assessment whenever there is a material change relating to the processing activity. A change relating to the processing activity is material if it diminishes the benefits of the processing activity as set forth in section 7152, subsection (a)(4), creates new negative impacts or increases the magnitude or likelihood of previously identified negative impacts as set forth in section 7152, subsection (a)(5), or diminishes the effectiveness of the safeguards as set forth in section 7152, subsection (a)(6). \nMaterial changes may include, for example, changes to the purpose of the processing; the minimum personal information necessary to achieve the purpose of the processing; or the risks to consumers’ privacy raised by consumers (e.g., numerous consumers complain to a business about the risks that the business’s processing poses to their privacy).", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring"], "source": "https://cppa.ca.gov/meetings/materials/20240308_item4_draft_risk.pdf?utm_source=substack&utm_medium=email", "official_name": "California Privacy Protection Agency Draft Risk Assessment and Automated Decisionmaking Technology Regulations ", "label": "safe"}
{"id": "1989_33", "doc_id": "1989", "text": "(3) For admission, acceptance, or hiring decisions as set forth in section 7200, subsections (a)(1)(A)(i)(1), (a)(1)(A)(ii)(1), if the following are true: \n(A) The automated decisionmaking technology is necessary to achieve, and is used solely for, the business’s assessment of the consumer’s ability to perform at work or in an educational program to determine whether to admit, accept, or hire them; and \n(B) The business has conducted an evaluation of the automated decisionmaking technology to ensure it works as intended for the business’s proposed use and does not discriminate based upon protected classes (“evaluation of the automated decisionmaking technology”), and has implemented policies, procedures, and training to ensure that the automated decisionmaking technology works as intended for the business’s proposed use and does not discriminate based upon protected classes (“accuracy and nondiscrimination safeguards”). \n(i) Alternatively, where a business obtained the automated decisionmaking technology from another person, the business has reviewed that person’s evaluation of the automated decisionmaking technology, including any requirements or limitations relevant to the business’s proposed use of the automated decisionmaking technology; and has implemented accuracy and nondiscrimination safeguards. \n\n\n(4) For allocation/assignment of work and compensation decisions as set forth in section 7200, subsection (a)(1)(A)(ii)(2), if the following are true: \n(A) The automated decisionmaking technology is necessary to achieve, and is used solely for, the business’s allocation/assignment of work or compensation; and \n(B) The business has conducted an evaluation of the automated decisionmaking technology and has implemented accuracy and nondiscrimination safeguards. \n(i) Alternatively, where a business obtained the automated decisionmaking technology from another person, the business has reviewed that person’s evaluation of the automated decisionmaking technology, including any requirements or limitations relevant to the business’s proposed use of the automated decisionmaking technology; and has implemented accuracy and nondiscrimination safeguards.", "tags": ["Risk factors: Reliability", "Risk factors: Bias", "Harms: Discrimination"], "source": "https://cppa.ca.gov/meetings/materials/20240308_item4_draft_risk.pdf?utm_source=substack&utm_medium=email", "official_name": "California Privacy Protection Agency Draft Risk Assessment and Automated Decisionmaking Technology Regulations ", "label": "safe"}
{"id": "2019_9", "doc_id": "2019", "text": "B. Fact Check and review all content generated by AI, especially if it will be used in public communication or decision making.\n\nWhile generative AI can rapidly produce clear prose, the information and content might be inaccurate, outdated, offensive, or simply made up. So, it’s essential to validate that the output of generative AI systems is accurate, properly attributed, free of someone else’s intellectual property, and free of unintended or undesirable instances of bias and potentially offensive or harmful material. It is your responsibility to verify that the information is accurate and appropriate by independently researching claims made by the generative AI tool.", "tags": ["Risk factors: Reliability", "Applications: Government: other applications/unspecified"], "source": "https://www.cityofboise.org/departments/human-resources/employee-policy-handbook/section-400-general-provisions/430q-city-use-of-artificial-intelligence-ai-regulation/", "official_name": "Boise Regulations 4.30q City Use of Artificial Intelligence (AI)", "label": "safe"}
{"id": "2019_14", "doc_id": "2019", "text": "B. Oversight and Evaluation Mechanisms\n\nThe IT department will establish an ongoing evaluation and monitoring process to assess the effectiveness, fairness, and safety of AI systems. Periodic reviews will be conducted to identify and address any issues related to adherence and ethical considerations.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation: Conformity assessment"], "source": "https://www.cityofboise.org/departments/human-resources/employee-policy-handbook/section-400-general-provisions/430q-city-use-of-artificial-intelligence-ai-regulation/", "official_name": "Boise Regulations 4.30q City Use of Artificial Intelligence (AI)", "label": "safe"}
{"id": "2047_6", "doc_id": "2047", "text": "POLICY\nWhen purchasing, configuring, developing, using, or maintaining AI systems, users will:\n1. Uphold the Guiding Principles for AI systems outlined above;\n2. Conduct an AI Review to assess the potential risk of the AI system. The CDPO or designee is responsible for coordinating review of AI systems used by the City as detailed in the AI Handbook;\n3. Obtain technical documentation about AI systems. The Finance Department, or other department overseeing the purchase of an AI system, is responsible for requiring vendors to disclose AI usage and to provide technical documentation (e.g., via the AI FactSheet as defined in the Terms and Definitions section, below) at the request of the CDPO; and\n4. In the event of an incident involving the use of the AI system, follow the City’s AI Incident Response Plan in accordance with the Information and Systems Security Policy. The CISO is responsible for overseeing the security practices of AI systems used by or on behalf the City. \n\n\nAdditionally, Finance is required to ask vendors to disclose the use of AI in procurement solicitations and to comply with the Requirements for AI Systems upon the request of the CDPO or designee.", "tags": ["Strategies: Performance requirements", "Strategies: Disclosure", "Strategies: Disclosure: In deployment"], "source": "https://www.sanjoseca.gov/home/showpublisheddocument/112981/638593035034930000", "official_name": "City of San Jose Artificial Intelligence Policy 1.7.12", "label": "safe"}
{"id": "2047_8", "doc_id": "2047", "text": "Sunset Procedures\n\n\nIf an AI system operated by the City or on its behalf ceases to provide a positive outcome to the\nCity as determined by the staff or CDPO, then the City must halt the use of that system unless express exception is provided by the CIO. If the abrupt cessation of the use of that AI system would significantly disrupt the delivery of services, a gradual phased out approach must be approved by the CIO before sunsetting. All measures to minimize the impact and recovery must be considered in the termination or phase out protocol, including but not limited to:\n• Ownership and future access of data;\n• Portability of the AI model, algorithm, and/or data; and\n• Impact to services, users, and residents.", "tags": ["Strategies: Evaluation", "Strategies: Performance requirements", "Applications: Government: other applications/unspecified"], "source": "https://www.sanjoseca.gov/home/showpublisheddocument/112981/638593035034930000", "official_name": "City of San Jose Artificial Intelligence Policy 1.7.12", "label": "safe"}
{"id": "2047_11", "doc_id": "2047", "text": "VIOLATIONS OF THE AI POLICY\nViolations of any section of the AI Policy, including failure to comply with the AI Handbook, may be subject to disciplinary action, up to and including termination. Violations made by a third party while operating an AI system on behalf of the City may result in a breach of contract and/or pursuit of damages. Infractions that violate local, state, federal or international law may be remanded to the proper authorities.", "tags": ["Incentives: Civil liability"], "source": "https://www.sanjoseca.gov/home/showpublisheddocument/112981/638593035034930000", "official_name": "City of San Jose Artificial Intelligence Policy 1.7.12", "label": "safe"}
{"id": "2097_6", "doc_id": "2097", "text": "Sec. 551.054.  CAPTURE OF BIOMETRIC IDENTIFIERS USING\n \tARTIFICIAL INTELLIGENCE. (a) A government entity in this state\n \tmust avoid develop or deploy an artificial intelligence system\n \tdeveloped with biometric identifiers of individuals and the\n \ttargeted or untargeted gathering of images or other media from the\n \tinternet or any other publicly available source must avoid be\n \tdeployed for the purpose of uniquely identifying a specific\n \tindividual, if it would infringe, constrain, or otherwise chill any\n \tright guaranteed under the United States Constitution, the Texas\n \tConstitution, federal law, or Texas law.\n \t       (b)  An individual is not considered to be informed nor to\n \thave provided consent for such purpose pursuant to Section 503.001,\n \tBusiness and Commerce Code, based solely upon the existence on the\n \tinternet, or other publicly available source, of an image or other\n \tmedia containing one or more biometric identifiers.\n \t       (c)  This section applies to systems designed for government\n \tentities to constrain civil liberties, not any artificial\n \tintelligence system developed or deployed for commercial purposes\n \tor any other government entity purpose.", "tags": ["Strategies: Input controls", "Strategies: Input controls: Data use", "Applications: Government: other applications/unspecified"], "source": "https://www.legis.state.tx.us/BillLookup/Text.aspx?LegSess=89R&Bill=HB149 - wait until docx version is available", "official_name": "Texas Responsible Artificial Intelligence Governance Act (HB 149)", "label": "safe"}
{"id": "2097_13", "doc_id": "2097", "text": "Sec. 551.106.  CIVIL consequence; INJUNCTION. (a) The attorney\n \tgeneral may bring an action in the name of this state to restrain or\n \tenjoin the person from violating this chapter and seek injunctive\n \trelief.\n \t       (b)  The attorney general may recover reasonable attorney's\n \tfees and other reasonable expenses incurred in investigating and\n \tbringing an action under this section.\n \t       (c)  The attorney general may assess and collect an\n \tadministrative charge against a developer or deployer who fails to\n \ttimely cure a infraction or who breaches a written statement\n \tprovided to the attorney general, of not less than $10,000 and not\n \tmore than $12,000 per uncured infraction.\n \t       (d)  The attorney general may assess and collect an\n \tadministrative charge against a developer or deployer who fails to\n \ttimely cure a infraction that is determined to be uncurable, of not\n \tless than $80,000 and not more than $200,000 per infraction after\n \tconviction of such infraction.\n \t       (e)  A developer or deployer who was found in infraction of\n \tand continues to operate with the provisions of this chapter shall\n \tbe assessed an administrative charge of not less than $2,000 and not\n \tmore than $40,000 per day.\n \t       (f)  There is a rebuttable presumption that a developer,\n \tdistributor, or deployer used reasonable care as required under\n \tthis chapter if the developer, distributor, or deployer complied\n \twith their duties in preventing violations under Subchapter B.\n \t       (g)  A developer, distributor, or deployer may seek an\n \texpedited hearing or other process, including a request for\n \tdeclaratory judgment, if the developer, distributor, or deployer\n \tbelieves its actions have not violated this chapter.", "tags": ["Incentives: Civil liability", "Incentives: Fines"], "source": "https://www.legis.state.tx.us/BillLookup/Text.aspx?LegSess=89R&Bill=HB149 - wait until docx version is available", "official_name": "Texas Responsible Artificial Intelligence Governance Act (HB 149)", "label": "safe"}
{"id": "2097_14", "doc_id": "2097", "text": "Sec. 551.107.  ENFORCEMENT ACTIONS BY STATE AGENCIES. (a) A\n \tstate agency may sanction an individual licensed, registered, or\n \tcertified by that agency for violations of Subchapter B, including:\n \t             (1)  the suspension, probation, or revocation of a\n \tlicense, registration, certificate, or other form of permission to\n \tengage in an activity; and\n \t             (2)  monetary consequences up to $100,000.\n \t       (b)  a state agency may not sanction an individual that is\n \tlicensed, registered, or certified by that agency for violations of\n \tSubchapter B until individuals or entities have been sentenced for\n \tviolations of this chapter, and received recommendations from the\n \tattorney general for subsequent enforcement.\n \t       Sec. 551.108.  CONSUMER RIGHTS AND REMEDIES. A consumer may\n \tappeal decision made by an artificial intelligence system which has\n \tan adverse impact on their health, welfare, safety, or fundamental\n \trights, and shall have the right to obtain from the deployer clear\n \tand meaningful explanations of the role of the artificial\n \tintelligence system in the decision-making procedure and the main\n \telements of the decision taken.", "tags": ["Incentives: Fines", "Incentives: Civil liability", "Risk factors: Transparency", "Harms: Harm to health/safety", "Harms: Violation of civil or human rights, including privacy", "Strategies: Disclosure"], "source": "https://www.legis.state.tx.us/BillLookup/Text.aspx?LegSess=89R&Bill=HB149 - wait until docx version is available", "official_name": "Texas Responsible Artificial Intelligence Governance Act (HB 149)", "label": "safe"}
{"id": "2415_4", "doc_id": "2415", "text": "(4) Reasonable steps a developer of a restricted risk covered product is required to take to ensure that children are not able to access the product.\n(5) Requirements for predeployment and postdeployment assessments, including, but not limited to, an assessment of the purpose for which the covered product is intended, technical capabilities, limitations and functionality, specific adverse impacts, internal governance, and the timing for the development and submission to the board of those evaluations and assessments. The board shall also provide guidance consistent with Section 22757.28 to avoid duplication of efforts with respect to any other state or federal laws that require similar documentation.\n(6) Requirements for artificial intelligence information labels to ensure that, for each covered product, the public is able to access baseline information on the covered product, including the covered product’s purpose, a description of how it works, its risk level, potential adverse impacts, and any other information necessary to assess the impact of the system on children.\n(7) Standards for reviews of covered products, including the timing of reviews, qualifications and training of auditors, rules governing auditor independence and oversight, and review reports that auditors are required to provide to the board. The board shall also establish rules for the protection of trade secrets in connection with the performance of reviews.\n(8) The creation of an incident reporting mechanism that enables third parties to report potential incidents of adverse impacts resulting from the use of a covered product directly to a developer or the board.\n(9) The creation of a publicly accessible registry for covered products that contains high-level summaries of review reports, incident reports, system information labels, and any additional information specified by the board.\n(10) (A) Registration fees for developers that do not exceed the reasonable regulatory costs incident to administering this chapter.\n(B) A registration fee described by this paragraph shall be deposited into the LEAD for Kids AI support pool established pursuant to Section 22757.27.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Impact assessment", "Strategies: Disclosure", "Strategies: Disclosure: In deployment"], "source": "https://leginfo.legislature.ca.gov/faces/billCompareClient.xhtml?bill_id=202520260AB1064&showamends=false", "official_name": "Leading Ethical AI Development (LEAD) for Kids Act", "label": "safe"}
{"id": "2415_7", "doc_id": "2415", "text": "(f) (1) On or after July 1, 2028, a developer shall submit a covered product it develops to an independent third party review on a schedule determined by the board according to the risk level posed by the covered product.\n(2) A developer whose covered product is subject to an review shall provide the auditor with all necessary documentation and information for the auditor to perform the review.\n(3) If an auditor discovers substantial noncompliance with this chapter, the auditor shall promptly notify the board.", "tags": ["Strategies: Evaluation: External auditing", "Strategies: Evaluation"], "source": "https://leginfo.legislature.ca.gov/faces/billCompareClient.xhtml?bill_id=202520260AB1064&showamends=false", "official_name": "Leading Ethical AI Development (LEAD) for Kids Act", "label": "safe"}
{"id": "2415_10", "doc_id": "2415", "text": "22757.26. (a) The board may refer violations of this chapter to the Attorney General.\n(b) With respect to violations related to the risk level classification of a covered product, the board may allow the developer to take corrective action if the board determines that the circumstances indicate that the erroneous classification was neither unreasonable nor in bad faith. If the developer fails to do so within 30 days, the board may refer the matter to the Attorney General.\n(c) Upon receiving a referral from the board, the Attorney General may bring an action for all of the following:\n(1) A civil consequence of twenty-five thousand dollars ($25,000) for each infraction.\n(2) Injunctive or declaratory relief.\n(3) Reasonable attorney’s fees.\n(d) A child who suffers actual harm as a result of the use of a covered product, or a parent or guardian acting on behalf of that child, may bring a civil action to recover all of the following:\n(1) Actual damages.\n(2) Punitive damages.\n(3) Reasonable attorney’s fees and costs.\n(4) Injunctive or declaratory relief.\n(5) Any other relief the court deems proper.\n22757.27. (a) There is hereby created in the State Treasury the LEAD for Kids AI support pool into which any civil consequence recovered by the Attorney General pursuant to Section 22757.26 shall be deposited.\n(b) Moneys in the support pool shall be available, only upon appropriation by the Legislature, for the purpose of administering this chapter.", "tags": ["Incentives: Fines"], "source": "https://leginfo.legislature.ca.gov/faces/billCompareClient.xhtml?bill_id=202520260AB1064&showamends=false", "official_name": "Leading Ethical AI Development (LEAD) for Kids Act", "label": "safe"}
{"id": "2667_1", "doc_id": "2667", "text": "The people of the State of California do enact as follows:\n\nSECTION 1. Title 15.3 (commencing with Section 3115) is added to Part 4 of Division 3 of the Civil Code, to read:\nTITLE 15.3. Copyrighted Materials Used for Artificial Intelligence Training\n\n3115. For the purposes of this title, the following definitions apply:\n(a) “Approximate content fingerprint” or “fingerprint” means an abstract representation of digital content that encodes distinctive features of the content and that is all of the following:\n(1) Distinct to the digital content being represented.\n(2) Robust to minor variations in the original digital content.\n(3) Incapable of being used to reconstruct the original digital content.\n(4) Capable of being used to readily identify digital content in a dataset.\n(b) “Artificial intelligence” or “AI” means an engineered or machine-based system that varies in its level of autonomy and that can, for explicit or implicit objectives, infer from the input it receives how to generate outputs that can influence physical or virtual environments.\n(c) “Covered material” means a material registered, preregistered, or indexed with the United States Copyright Office pursuant to Title 17 of the United States Code, Public Law 94-553 (17 U.S.C. Sec. 101 et seq.).\n(d) “Rights owner” means either of the following:\n(1) The owner of a copyright enforceable under the copyright laws of the United States pursuant to Title 17 of the United States Code, Public Law 94-553 (17 U.S.C. Sec. 101 et seq.).\n(2) The owner of a sound recording fixed before February 15, 1972, enforceable under Title 17 of the United States Code (17 U.S.C. Sec. 1401).\n(e) “Developer” means a business, person, partnership, corporation, or other entity that designs, codes, produces, or substantially modifies a GenAI model and that does either of the following:\n(1) Uses the GenAI model commercially in California.\n(2) Makes the GenAI model available to Californians for use.\n(f) “Generative artificial intelligence” or “GenAI” means an artificial intelligence system that can generate derived synthetic content, including text, images, video, and audio, that emulates the structure and characteristics of the system’s training data.", "tags": ["Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Disclosure: In standard form", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: About inputs", "Strategies: Disclosure: In standard form", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: About incidents", "Incentives: Civil liability", "Incentives: Fines", "Strategies: Tiering", "Strategies: Tiering: Tiering based on inputs"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202520260AB412", "official_name": "California AB 412", "label": "safe"}
{"id": "2667_4", "doc_id": "2667", "text": "3118. (a) A rights owner, or any person acting on their behalf, must avoid submit more than one request per calendar quarter to the same developer concerning the same GenAI model, unless the subsequent request includes material new information not available to the rights owner at the time of the prior request.\n(b) A request submitted pursuant to this section may pertain to multiple covered materials.\n\n3119. A rights owner that has complied in good faith with Section 3118 and that is not provided with the information as required by this title may bring a civil action against the developer for any of the following:\n(a) One thousand dollars ($1,000) per infraction or actual damages, whichever is greater.\n(b) Injunctive or declaratory relief.\n(c) Reasonable attorney’s costs and fees.\n(d) Any other relief the court deems appropriate.\n\n3119.5. This title must avoid apply to a GenAI model that is any of the following:\n(a) Trained exclusively using data the developer makes publicly available at no cost to users of the developer’s internet website.\n(b) Developed and used exclusively for noncommercial academic or governmental research.\n(c) Not trained using covered materials.\n(d) Trained exclusively using covered materials for which the developer is the rights owner.", "tags": ["Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: About incidents", "Incentives: Civil liability", "Incentives: Fines", "Strategies: Tiering", "Strategies: Tiering: Tiering based on inputs"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202520260AB412", "official_name": "California AB 412", "label": "safe"}
{"id": "280_1", "doc_id": "280", "text": "SECTION 1. Chapter 25 (commencing with Section 22756) is added to Division 8 of the Business and Professions Code, to read:\n\nCHAPTER  25. Automated Decision Tools\n1. As used in this chapter:\n(a) “Algorithmic discrimination” means the condition in which an automated decision tool contributes to unjustified differential treatment or impacts disfavoring people based on their actual or perceived race, color, ethnicity, sex, religion, age, national origin, limited English proficiency, disability, veteran status, genetic information, reproductive health, or any other classification protected by state law.\n(b) “Artificial intelligence” means a machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations, or decisions influencing a real or virtual environment.\n(c) “Automated decision tool” means a system or service that uses artificial intelligence and has been specifically developed and marketed to, or specifically modified to, make, or be a controlling factor in making, consequential decisions.\n(d) “Consequential decision” means a decision or judgment that has a legal, material, or similarly significant effect on an individual’s life relating to the impact of, access to, or the cost, terms, or availability of, any of the following:\n(1) Employment, workers management, or self-employment, including, but not limited to, all of the following:\n(A) Pay or promotion.\n(B) Hiring or termination.\n(C) Automated task allocation.\n(2) Education and vocational training, including, but not limited to, all of the following:\n(A) Assessment, including, but not limited to, detecting student cheating or plagiarism.\n(B) Accreditation.\n(C) Certification.\n(D) Admissions.\n(E) Financial aid or scholarships.\n(3) Housing or lodging, including rental or short-term housing or lodging.\n(4) Essential utilities, including electricity, heat, water, internet or telecommunications access, or transportation.\n(5) Family planning, including adoption services or reproductive services, as well as assessments related to child protective services.\n(6) Health care or health insurance, including mental health care, dental, or vision.\n(7) Financial services, including a financial service provided by a mortgage company, mortgage broker, or creditor.\n(8) The criminal justice system, including, but not limited to, all of the following:\n(A) Risk assessments for pretrial hearings.\n(B) Sentencing.\n(C) Parole.\n(9) Legal services, including private arbitration or mediation.\n(10) Voting.\n(11) Access to benefits or services or assignment of consequences.\n(e) “Deployer” means a person, partnership, state or local government agency, or corporation that uses an automated decision tool to make a consequential decision.\n(f) “Developer” means a person, partnership, state or local government agency, or corporation that designs, codes, or produces an automated decision tool, or substantially modifies an artificial intelligence system or service for the intended purpose of making, or being a controlling factor in making, consequential decisions, whether for its own use or for use by a third party.\n(g) “Impact assessment” means a documented risk-based evaluation of an automated decision tool that meets the criteria of Section 22756.1.\n(h) “Sex” includes pregnancy, childbirth, and related conditions, gender identity, intersex status, and sexual orientation.\n(i) “Significant update” means a new version, new release, or other update to an automated decision tool that includes changes to its use case, key functionality, or expected outcomes.", "tags": ["Risk factors: Bias", "Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Transparency", "Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About incidents", "Strategies: Disclosure: About inputs", "Strategies: Performance requirements", "Strategies: Tiering", "Strategies: Tiering: Tiering based on planning ability", "Incentives: Civil liability", "Incentives: Fines", "Applications: Finance and investment", "Applications: Business services and analytics", "Applications: Education", "Applications: Government: benefits and welfare", "Applications: Government: judicial and law enforcement", "Applications: Government: other applications/unspecified"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240AB331", "official_name": "An act to add Chapter 25 (commencing with Section 22756) to Division 8 of the Business and Professions Code, relating to artificial intelligence.", "label": "safe"}
{"id": "280_2", "doc_id": "280", "text": "22756.1. (a) On or before January 1, 2025, and annually thereafter, a deployer of an automated decision tool shall perform an impact assessment for any automated decision tool the deployer uses that includes all of the following:\n(1) A statement of the purpose of the automated decision tool and its intended benefits, uses, and deployment contexts.\n(2) A description of the automated decision tool’s outputs and how they are used to make, or be a controlling factor in making, a consequential decision.\n(3) A summary of the type of data collected from natural persons and processed by the automated decision tool when it is used to make, or be a controlling factor in making, a consequential decision.\n(4) A statement of the extent to which the deployer’s use of the automated decision tool is consistent with or varies from the statement required of the developer by Section 22756.3.\n(5) An analysis of potential adverse impacts on the basis of sex, race, color, ethnicity, religion, age, national origin, limited English proficiency, disability, veteran status, or genetic information from the deployer’s use of the automated decision tool.\n(6) A description of the safeguards implemented, or that will be implemented, by the deployer to address any reasonably foreseeable risks of algorithmic discrimination arising from the use of the automated decision tool known to the deployer at the time of the impact assessment.\n(7) A description of how the automated decision tool will be used by a natural person, or monitored when it is used, to make, or be a controlling factor in making, a consequential decision.\n(8) A description of how the automated decision tool has been or will be evaluated for validity or relevance.\n(b) On or before January 1, 2025, and annually thereafter, a developer of an automated decision tool shall complete and document an assessment of any automated decision tool that it designs, codes, or produces that includes all of the following:\n(1) A statement of the purpose of the automated decision tool and its intended benefits, uses, and deployment contexts.\n(2) A description of the automated decision tool’s outputs and how they are used to make, or be a controlling factor in making, a consequential decision.\n(3) A summary of the type of data collected from natural persons and processed by the automated decision tool when it is used to make, or be a controlling factor in making, a consequential decision.\n(4) An analysis of a potential adverse impact on the basis of sex, race, color, ethnicity, religion, age, national origin, limited English proficiency, disability, veteran status, or genetic information from the deployer’s use of the automated decision tool.\n(5) A description of the measures taken by the developer to mitigate the risk known to the developer of algorithmic discrimination arising from the use of the automated decision tool.\n(6) A description of how the automated decision tool can be used by a natural person, or monitored when it is used, to make, or be a controlling factor in making, a consequential decision.\n(c) A deployer or developer shall, in addition to the impact assessment required by subdivisions (a) and (b), perform, as soon as feasible, an impact assessment with respect to any significant update.\n(d) This section does not apply to a deployer with fewer than 25 employees unless, as of the end of the prior calendar year, the deployer deployed an automated decision tool that impacted more than 999 people per year.", "tags": ["Risk factors: Bias", "Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Transparency", "Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About incidents", "Strategies: Disclosure: About inputs", "Strategies: Performance requirements", "Strategies: Tiering", "Strategies: Tiering: Tiering based on planning ability", "Incentives: Civil liability", "Incentives: Fines", "Applications: Finance and investment", "Applications: Business services and analytics", "Applications: Education", "Applications: Government: benefits and welfare", "Applications: Government: judicial and law enforcement", "Applications: Government: other applications/unspecified"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240AB331", "official_name": "An act to add Chapter 25 (commencing with Section 22756) to Division 8 of the Business and Professions Code, relating to artificial intelligence.", "label": "safe"}
{"id": "280_3", "doc_id": "280", "text": "22756.2. (a) (1) A deployer shall, at or before the time an automated decision tool is used to make a consequential decision, notify any natural person that is the subject of the consequential decision that an automated decision tool is being used to make, or be a controlling factor in making, the consequential decision.\n(2) A deployer shall provide to a natural person notified pursuant to this subdivision all of the following:\n(A) A statement of the purpose of the automated decision tool.\n(B) Contact information for the deployer.\n(C) A plain language description of the automated decision tool that includes a description of any human components and how any automated component is used to inform a consequential decision.\n(b) (1) If a consequential decision is made solely based on the output of an automated decision tool, a deployer shall, if technically feasible, accommodate a natural person’s request to not be subject to the automated decision tool and to be subject to an alternative selection process or accommodation.\n(2) After a request pursuant to paragraph (1), a deployer may reasonably request, collect, and process information from a natural person for the purposes of identifying the person and the associated consequential decision. If the person does not provide that information, the deployer must avoid be obligated to provide an alternative selection process or accommodation.", "tags": ["Risk factors: Bias", "Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Transparency", "Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About incidents", "Strategies: Disclosure: About inputs", "Strategies: Performance requirements", "Strategies: Tiering", "Strategies: Tiering: Tiering based on planning ability", "Incentives: Civil liability", "Incentives: Fines", "Applications: Finance and investment", "Applications: Business services and analytics", "Applications: Education", "Applications: Government: benefits and welfare", "Applications: Government: judicial and law enforcement", "Applications: Government: other applications/unspecified"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240AB331", "official_name": "An act to add Chapter 25 (commencing with Section 22756) to Division 8 of the Business and Professions Code, relating to artificial intelligence.", "label": "safe"}
{"id": "280_4", "doc_id": "280", "text": "22756.3. (a) A developer shall provide a deployer with a statement regarding the intended uses of the automated decision tool and documentation regarding all of the following:\n(1) The known limitations of the automated decision tool, including any reasonably foreseeable risks of algorithmic discrimination arising from its intended use.\n(2) A description of the type of data used to program or train the automated decision tool.\n(3) A description of how the automated decision tool was evaluated for validity and explainability before sale or licensing.\n(b) This section does not require the disclosure of trade secrets, as defined in Section 3426.1 of the Civil Code.", "tags": ["Risk factors: Bias", "Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Transparency", "Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About incidents", "Strategies: Disclosure: About inputs", "Strategies: Performance requirements", "Strategies: Tiering", "Strategies: Tiering: Tiering based on planning ability", "Incentives: Civil liability", "Incentives: Fines", "Applications: Finance and investment", "Applications: Business services and analytics", "Applications: Education", "Applications: Government: benefits and welfare", "Applications: Government: judicial and law enforcement", "Applications: Government: other applications/unspecified"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240AB331", "official_name": "An act to add Chapter 25 (commencing with Section 22756) to Division 8 of the Business and Professions Code, relating to artificial intelligence.", "label": "safe"}
{"id": "280_5", "doc_id": "280", "text": "22756.4. (a) (1) A deployer or developer shall establish, document, implement, and maintain a governance program that contains reasonable administrative and technical safeguards to map, measure, manage, and govern the reasonably foreseeable risks of algorithmic discrimination associated with the use or intended use of an automated decision tool.\n(2) The safeguards required by this subdivision shall be appropriate to all of the following:\n(A) The use or intended use of the automated decision tool.\n(B) The deployer’s or developer’s role as a deployer or developer.\n(C) The size, complexity, and resources of the deployer or developer.\n(D) The nature, context, and scope of the activities of the deployer or developer in connection with the automated decision tool.\n(E) The technical feasibility and cost of available tools, assessments, and other means used by a deployer or developer to map, measure, manage, and govern the risks associated with an automated decision tool.\n(b) The governance program required by this section shall be designed to do all of the following:\n(1) (A) Designate at least one employee to be responsible for overseeing and maintaining the governance program and adherence with this chapter.\n(B) (i) An employee designated pursuant to this paragraph shall have the authority to assert to the employee’s employer a good faith belief that the design, production, or use of an automated decision tool fails to comply with the requirements of this chapter.\n(ii) An employer of an employee designated pursuant to this paragraph shall conduct a prompt and complete assessment of any adherence issue raised by that employee.\n(2) Identify and implement safeguards to address reasonably foreseeable risks of algorithmic discrimination resulting from the use or intended use of an automated decision tool.\n(3) If established by a deployer, provide for the performance of impact assessments as required by Section 22756.1.\n(4) If established by a developer, provide for adherence with Sections 22756.2 and 22756.3.\n(5) Conduct an annual and comprehensive review of policies, practices, and procedures to ensure adherence with this chapter.\n(6) Maintain for two years after completion the results of an impact assessment.\n(7) Evaluate and make reasonable adjustments to administrative and technical safeguards in light of material changes in technology, the risks associated with the automated decision tool, the state of technical standards, and changes in business arrangements or operations of the deployer or developer.\n(c) This section does not apply to a deployer with fewer than 25 employees unless, as of the end of the prior calendar year, the deployer deployed an automated decision tool that impacted more than 999 people per year.", "tags": ["Risk factors: Bias", "Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Transparency", "Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About incidents", "Strategies: Disclosure: About inputs", "Strategies: Performance requirements", "Strategies: Tiering", "Strategies: Tiering: Tiering based on planning ability", "Incentives: Civil liability", "Incentives: Fines", "Applications: Finance and investment", "Applications: Business services and analytics", "Applications: Education", "Applications: Government: benefits and welfare", "Applications: Government: judicial and law enforcement", "Applications: Government: other applications/unspecified"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240AB331", "official_name": "An act to add Chapter 25 (commencing with Section 22756) to Division 8 of the Business and Professions Code, relating to artificial intelligence.", "label": "safe"}
{"id": "280_6", "doc_id": "280", "text": "22756.5. A deployer or developer shall make publicly available, in a readily accessible manner, a clear policy that provides a summary of both of the following:\n(a) The types of automated decision tools currently in use or made available to others by the deployer or developer.\n(b) How the deployer or developer manages the reasonably foreseeable risks of algorithmic discrimination that may arise from the use of the automated decision tools it currently uses or makes available to others.", "tags": ["Risk factors: Bias", "Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Transparency", "Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About incidents", "Strategies: Disclosure: About inputs", "Strategies: Performance requirements", "Strategies: Tiering", "Strategies: Tiering: Tiering based on planning ability", "Incentives: Civil liability", "Incentives: Fines", "Applications: Finance and investment", "Applications: Business services and analytics", "Applications: Education", "Applications: Government: benefits and welfare", "Applications: Government: judicial and law enforcement", "Applications: Government: other applications/unspecified"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240AB331", "official_name": "An act to add Chapter 25 (commencing with Section 22756) to Division 8 of the Business and Professions Code, relating to artificial intelligence.", "label": "safe"}
{"id": "280_7", "doc_id": "280", "text": "22756.6. (a) A deployer must avoid use an automated decision tool that results in algorithmic discrimination.\n(b) (1) On and after January 1, 2026, a person may bring a civil action against a deployer for infraction of this section.\n(2) In an action brought pursuant to paragraph (1), the plaintiff shall have the burden of proof to demonstrate that the deployer’s use of the automated decision tool resulted in algorithmic discrimination that caused actual harm to the person bringing the civil action.\n(c) In addition to any other remedy at law, a deployer that violates this section shall be responsible to a prevailing plaintiff for any of the following:\n(1) Compensatory damages.\n(2) Declaratory relief.\n(3) Reasonable attorney’s fees and costs.", "tags": ["Risk factors: Bias", "Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Transparency", "Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About incidents", "Strategies: Disclosure: About inputs", "Strategies: Performance requirements", "Strategies: Tiering", "Strategies: Tiering: Tiering based on planning ability", "Incentives: Civil liability", "Incentives: Fines", "Applications: Finance and investment", "Applications: Business services and analytics", "Applications: Education", "Applications: Government: benefits and welfare", "Applications: Government: judicial and law enforcement", "Applications: Government: other applications/unspecified"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240AB331", "official_name": "An act to add Chapter 25 (commencing with Section 22756) to Division 8 of the Business and Professions Code, relating to artificial intelligence.", "label": "safe"}
{"id": "280_8", "doc_id": "280", "text": "22756.7. (a) Within 60 days of completing an impact assessment required by this chapter, a deployer or a developer shall provide the impact assessment to the Civil Rights Department.\n(b) (1) A deployer or developer who violates this section shall be responsible for an administrative charge of not more than ten thousand dollars ($10,000) per infraction in an administrative enforcement action brought by the Civil Rights Department.\n(2) Each day on which an automated decision tool is used for which an impact assessment has not been submitted pursuant to this section shall give rise to a distinct infraction of this section.\n(c) The Civil Rights Department may share impact assessments with other state entities as appropriate.", "tags": ["Risk factors: Bias", "Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Transparency", "Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About incidents", "Strategies: Disclosure: About inputs", "Strategies: Performance requirements", "Strategies: Tiering", "Strategies: Tiering: Tiering based on planning ability", "Incentives: Civil liability", "Incentives: Fines", "Applications: Finance and investment", "Applications: Business services and analytics", "Applications: Education", "Applications: Government: benefits and welfare", "Applications: Government: judicial and law enforcement", "Applications: Government: other applications/unspecified"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240AB331", "official_name": "An act to add Chapter 25 (commencing with Section 22756) to Division 8 of the Business and Professions Code, relating to artificial intelligence.", "label": "safe"}
{"id": "280_9", "doc_id": "280", "text": "22756.8. (a) (1) Any of the following public attorneys may bring a civil action against a deployer or developer for a infraction of this chapter:\n(A) The Attorney General in the name of the people of the State of California.\n(B) A district attorney, county counsel, or city attorney for the jurisdiction in which the infraction occurred.\n(C) A city prosecutor in any city having a full-time city prosecutor, with the consent of the district attorney.\n(2) A court may award in an action brought pursuant to this subdivision all of the following:\n(A) Injunctive relief.\n(B) Declaratory relief.\n(C) Reasonable attorney’s fees and litigation costs.\n(b) (1) A public attorney, before commencing an action pursuant to this section for injunctive relief, shall provide 45 days’ written notice to a deployer or developer of the alleged violations of this chapter.\n(2) (A) The developer or deployer may cure, within 45 days of receiving the written notice described in paragraph (1), the noticed infraction and provide the person who gave the notice an express written statement, made under consequence of perjury, that the infraction has been cured and that no further violations shall occur.\n(B) If the developer or deployer cures the noticed infraction and provides the express written statement pursuant to subparagraph (A), a claim for injunctive relief must avoid be maintained for the noticed infraction.", "tags": ["Risk factors: Bias", "Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Transparency", "Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About incidents", "Strategies: Disclosure: About inputs", "Strategies: Performance requirements", "Strategies: Tiering", "Strategies: Tiering: Tiering based on planning ability", "Incentives: Civil liability", "Incentives: Fines", "Applications: Finance and investment", "Applications: Business services and analytics", "Applications: Education", "Applications: Government: benefits and welfare", "Applications: Government: judicial and law enforcement", "Applications: Government: other applications/unspecified"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240AB331", "official_name": "An act to add Chapter 25 (commencing with Section 22756) to Division 8 of the Business and Professions Code, relating to artificial intelligence.", "label": "safe"}
{"id": "280_10", "doc_id": "280", "text": "SEC. 2. No reimbursement is required by this act pursuant to Section 6 of Article XIII B of the California Constitution for certain costs that may be incurred by a local agency or school district because, in that regard, this act creates a new crime or infraction, eliminates a crime or infraction, or changes the consequence for a crime or infraction, within the meaning of Section 17556 of the Government Code, or changes the definition of a crime within the meaning of Section 6 of Article XIII B of the California Constitution.\nHowever, if the Commission on State Mandates determines that this act contains other costs mandated by the state, reimbursement to local agencies and school districts for those costs shall be made pursuant to Part 7 (commencing with Section 17500) of Division 4 of Title 2 of the Government Code.", "tags": ["Risk factors: Bias", "Risk factors: Reliability", "Risk factors: Interpretability and explainability", "Risk factors: Transparency", "Harms: Harm to health/safety", "Harms: Financial loss", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About incidents", "Strategies: Disclosure: About inputs", "Strategies: Performance requirements", "Strategies: Tiering", "Strategies: Tiering: Tiering based on planning ability", "Incentives: Civil liability", "Incentives: Fines", "Applications: Finance and investment", "Applications: Business services and analytics", "Applications: Education", "Applications: Government: benefits and welfare", "Applications: Government: judicial and law enforcement", "Applications: Government: other applications/unspecified"], "source": "https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240AB331", "official_name": "An act to add Chapter 25 (commencing with Section 22756) to Division 8 of the Business and Professions Code, relating to artificial intelligence.", "label": "safe"}
{"id": "304_1", "doc_id": "304", "text": "AN ACT concerning employment. \n \nBe it enacted by the People of the State of Illinois, represented in the General Assembly: \n \nSection 1. Short title. This Act may be cited as the Artificial Intelligence Video Interview Act. \n \nSection 5. Disclosure of the use of artificial intelligence analysis. An employer that asks applicants to record video interviews and uses an artificial intelligence analysis of the applicant-submitted videos shall do all of the following when considering applicants for positions based in Illinois before asking applicants to submit video interviews: \n\n(1) Notify each applicant before the interview that artificial intelligence may be used to analyze the applicant's video interview and consider the applicant's fitness for the position. \n\n(2) Provide each applicant with information before the interview explaining how the artificial intelligence works and what general types of characteristics it uses to evaluate applicants. \n\n(3) Obtain, before the interview, consent from the applicant to be evaluated by the artificial intelligence program as described in the information provided. An employer may not use artificial intelligence to evaluate applicants who have not consented to the use of artificial intelligence analysis.", "tags": ["Risk factors: Security", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Input controls", "Strategies: Input controls: Data circulation", "Applications: Business services and analytics"], "source": "https://www.ilga.gov/legislation/fulltext.asp?DocName=&SessionId=108&GA=101&DocTypeId=HB&DocNum=2557&GAID=15&LegID=&SpecSess=&Session=", "official_name": "Artificial Intelligence Video Interview Act", "label": "safe"}
{"id": "304_2", "doc_id": "304", "text": "Section 10. Sharing videos limited. An employer may not share applicant videos, except with persons whose expertise or technology is necessary in order to evaluate an applicant's fitness for a position. \n \nSection 15. Destruction of videos. Upon request from the applicant, employers, within 30 days after receipt of the request, must delete an applicant's interviews and instruct any other persons who received copies of the applicant video interviews to also delete the videos, including all electronically generated backup copies. Any other such person shall comply with the employer's instructions.", "tags": ["Risk factors: Security", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Input controls", "Strategies: Input controls: Data circulation", "Applications: Business services and analytics"], "source": "https://www.ilga.gov/legislation/fulltext.asp?DocName=&SessionId=108&GA=101&DocTypeId=HB&DocNum=2557&GAID=15&LegID=&SpecSess=&Session=", "official_name": "Artificial Intelligence Video Interview Act", "label": "safe"}
{"id": "314_1", "doc_id": "314", "text": "AN ACT concerning employment.\n\n\n   Be it enacted by the People of the State of Illinois, represented in the General Assembly:\n\n\n   Section 5. The Artificial Intelligence Video Interview Act is amended by adding Section 20 as follows:\n\n\n   Sec. 20. Report of demographic data.\n\n   (a) An employer that relies solely upon an artificial intelligence analysis of a video interview to determine whether an applicant will be selected for an in-person interview must collect and report the following demographic data:\n\n       (1) the race and ethnicity of applicants who are and are not afforded the opportunity for an in-person interview after the use of artificial intelligence analysis; and\n\n       (2) the race and ethnicity of applicants who are hired.", "tags": ["Harms: Financial loss", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Applications: Business services and analytics"], "source": "https://www.ilga.gov/legislation/fulltext.asp?DocName=&SessionId=110&GA=102&DocTypeId=HB&DocNum=53&GAID=16&LegID=127865&SpecSess=&Session=", "official_name": "Illinois HB53 2021", "label": "safe"}
{"id": "314_2", "doc_id": "314", "text": "(b) The demographic data collected under subsection (a) must be reported to the Department of Commerce and Economic Opportunity annually by December 31. The report shall include the data collected in the 12-month period ending on November 30 preceding the filing of the report.\n\n\n   (c) The Department must analyze the data reported and report to the Governor and General Assembly by July 1 of each year whether the data discloses a racial bias in the use of artificial intelligence.", "tags": ["Harms: Financial loss", "Harms: Violation of civil or human rights, including privacy", "Harms: Discrimination", "Strategies: Evaluation", "Strategies: Evaluation: External auditing", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Applications: Business services and analytics"], "source": "https://www.ilga.gov/legislation/fulltext.asp?DocName=&SessionId=110&GA=102&DocTypeId=HB&DocNum=53&GAID=16&LegID=127865&SpecSess=&Session=", "official_name": "Illinois HB53 2021", "label": "safe"}
{"id": "442_4", "doc_id": "442", "text": "(b) Issuance Of Rules.—Not later than 180 days after the date of enactment of this Act, the Federal Communications Commission, in consultation with the Federal Trade Commission, shall issue rules that provide that, with respect to any product offered or operated by an entity—\n\n\n(1) the entity may not offer to a minor user of the product any artificial intelligence feature, including an artificial intelligence chat feature, as part of the product unless a parent or guardian of the minor user affirmatively financial support allocations consent to accept that artificial intelligence feature on behalf of the minor user;\n\n\n(2) after granting consent under paragraph (1), a parent or guardian of the applicable minor user may revoke that consent at any time; and\n\n\n(3) with respect to the revocation of consent under paragraph (2), the entity may not charge the parent or guardian revoking consent a fee for the removal by the entity of the applicable artificial intelligence feature.\n\n\n(c) Violations.—A infraction of a rule issued under subsection (b) shall be considered to be a infraction of the Communications Act of 1934 (47 U.S.C. 151 et seq.) or a rule issued under that Act.", "tags": ["Risk factors: Safety", "Harms: Harm to health/safety", "Harms: Detrimental content", "Incentives: Civil liability", "Applications: Consumer goods"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/1626", "official_name": "ASK Act", "label": "safe"}
{"id": "448_2", "doc_id": "448", "text": "SECTION 1. SHORT TITLE.\n\n\nThis Act may be cited as the “AI Disclosure Act of 2023”.\n\n\nSEC. 2. REQUIREMENT TO DISCLOSE USE OF GENERATIVE ARTIFICIAL INTELLIGENCE.\n\n\n(a) Disclaimer Required.—Generative artificial intelligence shall include on any output generated by such artificial intelligence the following: “Disclaimer: this output has been generated by artificial intelligence.”.\n\n\n(b) Enforcement By Federal Trade Commission.—\n\n\n(1) UNFAIR OR DECEPTIVE ACTS OR PRACTICES.—A infraction of subsection (a) or a regulation promulgated under such subsection shall be treated as a infraction of a regulation under section 18(a)(1)(B) of the Federal Trade Commission Act (15 U.S.C. 57a(a)(1)(B)) regarding unfair or deceptive acts or practices.\n\n\n(2) POWERS OF COMMISSION.—The Federal Trade Commission shall enforce subsection (a) and the regulations promulgated under such subsection in the same manner, by the same means, and with the same jurisdiction, powers, and duties as though all applicable terms and provisions of the Federal Trade Commission Act (15 U.S.C. 41 et seq.) were incorporated into and made a part of this Act. Any person who violates such subsection or a regulation promulgated under such subsection shall be subject to the consequences and entitled to the privileges and immunities provided in the Federal Trade Commission Act.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Incentives: Fines", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/3831", "official_name": "AI Disclosure Act of 2023", "label": "safe"}
{"id": "453_5", "doc_id": "453", "text": "SEC. 3. ENHANCED consequences FOR VIOLATIONS INVOLVING AI VOICE OR TEXT MESSAGE IMPERSONATION.\n\n\n(a) In General.—Section 227 of the Communications Act of 1934 (47 U.S.C. 227), as amended by the preceding provisions of this Act, is further amended by adding at the end the following:\n\n\n“(l) Enhanced consequences For Violations Involving AI Voice Or Text Message Impersonation.—In the case of a infraction of this section with respect to which the party making the call or sending the text message uses artificial intelligence to impersonate an individual or entity with the intent to defraud, cause harm, or wrongfully obtain anything of value—\n\n\n“(1) the maximum amount of the forfeiture consequence that may be imposed under subsection (b)(4) or (e)(5)(A) of this section or subsection (b) of section 503 (as the case may be) shall be twice the maximum amount that may be imposed for such infraction under such subsection without regard to this subsection; and\n\n\n“(2) the maximum amount of the criminal charge that may be imposed under subsection (e)(5)(B) of this section or section 501 (as the case may be) shall be twice the maximum amount that may be imposed for such infraction under such subsection or section without regard to this subsection.”.\n\n\n(b) Applicability.—The amendment made by subsection (a) shall apply with respect to violations occurring after the date of the enactment of this Act.", "tags": ["Risk factors: Transparency", "Harms: Financial loss", "Harms: Violation of civil or human rights, including privacy", "Harms: Detrimental content", "Applications: Networking and telecommunications", "Incentives: Fines", "Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/7123", "official_name": "QUIET Act", "label": "safe"}
{"id": "50_7", "doc_id": "50", "text": "e.\tInternational sales or transfers of autonomous and semi-autonomous weapon systems will be approved in accordance with existing technology security and foreign disclosure requirements and processes in accordance with DoDD 5111.21. \n\nf.\tThe design, development, deployment, and use of AI capabilities in autonomous and semi-autonomous weapon systems will be consistent with the DoD AI Ethical Principles and the DoD Responsible Artificial Intelligence Strategy and Implementation Pathway.  The DoD AI Ethical Principles, as adopted in the February 21, 2020 Secretary of Defense Memorandum, are: \n(1)\tResponsible. \nDoD personnel will exercise appropriate levels of judgment and care, while remaining responsible for the development, deployment, and use of AI capabilities. \n(2)\tEquitable. \nThe DoD will take deliberate steps to minimize unintended bias in AI capabilities. \n(3)\tTraceable. \nThe DoD’s AI capabilities will be developed and deployed such that relevant personnel possess an appropriate understanding of the technology, development processes, and operational methods applicable to AI capabilities, including with transparent and auditable methodologies, data sources, and design procedures and documentation. \n(4)\tReliable. \nThe DoD’s AI capabilities will have explicit, well-defined uses, and the safety, security, and effectiveness of such capabilities will be subject to testing and assurance within those defined uses across their entire life cycles. \n(5)\tGovernable. \nThe DoD will design and engineer AI capabilities to fulfill their intended functions while possessing the ability to detect and avoid unintended consequences, and the ability to disengage or deactivate deployed systems that demonstrate unintended behavior.", "tags": ["Applications: Government: military and public safety", "Risk factors: Bias", "Risk factors: Reliability", "Risk factors: Transparency"], "source": "https://www.esd.whs.mil/portals/54/documents/dd/issuances/dodd/300009p.pdf", "official_name": "Department of Defense Directive 3000.09 (\"Autonomy in weapon systems\")", "label": "safe"}
{"id": "517_2", "doc_id": "517", "text": "SEC. 2. PROHIBITION ON DISTRIBUTION OF MATERIALLY DECEPTIVE AI-GENERATED AUDIO OR VISUAL MEDIA PRIOR TO ELECTION.\n(a) In General.—Title III of the Federal Election Campaign Act of 1971 (52 U.S.C. 30101 et seq.) is amended by adding at the end the following new section:\n“SEC. 325. PROHIBITION ON DISTRIBUTION OF MATERIALLY DECEPTIVE AI-GENERATED AUDIO OR VISUAL MEDIA.", "tags": ["Harms: Detrimental content", "Risk factors: Reliability", "Applications: Broadcasting and media production", "Harms: Detrimental content", "Applications: Government: other applications/unspecified", "Applications: Broadcasting and media production", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/2770", "official_name": "Protect Elections from Deceptive AI Act", "label": "safe"}
{"id": "517_3", "doc_id": "517", "text": "“(a) Definitions.—In this section:\n“(1) COVERED INDIVIDUAL.—The term ‘covered individual’ means a candidate for Federal office.\n“(2) DECEPTIVE AI-GENERATED AUDIO OR VISUAL MEDIA.—The term ‘deceptive AI-generated audio or visual media’ means an image, audio, or video that—\n“(A) is the product of artificial intelligence technology that uses machine learning (including deep learning models, natural learning processing, or any other computational processing techniques of similar or greater complexity), that—\n“(i) merges, combines, replaces, or superimposes content onto an image, audio, or video, creating an image, audio, or video that appears authentic; or\n“(ii) generates an inauthentic image, audio, or video that appears authentic; and\n“(B) a reasonable person, having considered the qualities of the image, audio, or video and the nature of the distribution channel in which the image, audio, or video appears—\n“(i) would have a fundamentally different understanding or impression of the appearance, speech, or expressive conduct exhibited in the image, audio, or video than that person would have if that person were hearing or seeing the unaltered, original version of the image, audio, or video; or\n“(ii) would believe that the image, audio, or video accurately exhibits any appearance, speech, or expressive conduct of a person who did not actually exhibit such appearance, speech, or expressive conduct.", "tags": ["Harms: Detrimental content", "Risk factors: Reliability", "Applications: Broadcasting and media production"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/2770", "official_name": "Protect Elections from Deceptive AI Act", "label": "safe"}
{"id": "517_7", "doc_id": "517", "text": "“(d) Civil Action.—\n“(1) INJUNCTIVE OR OTHER EQUITABLE RELIEF.—\n“(A) IN GENERAL.—A covered individual whose voice or likeness appears in, or who is the subject of, a materially deceptive AI-generated audio or visual media, including content distributed as part of a Federal election activity, distributed in infraction of this section may seek injunctive or other equitable relief prohibiting the distribution of materially deceptive AI-generated audio or visual media in infraction of this section.\n“(B) PRECEDENCE.—An action under this paragraph shall be entitled to precedence in accordance with the Federal Rules of Civil Procedure.", "tags": ["Incentives: Civil liability"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/2770", "official_name": "Protect Elections from Deceptive AI Act", "label": "safe"}
{"id": "517_8", "doc_id": "517", "text": "“(2) DAMAGES.—\n“(A) IN GENERAL.—A covered individual whose voice or likeness appears in, or who is the subject of, a materially deceptive AI-generated audio or visual media, including content distributed as part of a Federal election activity, distributed in infraction of this section may bring an action for general or special damages against the person, committee, or other entity that distributed the materially deceptive AI-generated audio or visual media.\n“(B) ATTORNEY'S FEES AND COSTS.—In addition to any damages awarded under subparagraph (A), the court may also award a prevailing party reasonable attorney’s fees and costs.\n“(C) RULE OF CONSTRUCTION.—Nothing in this paragraph shall be construed to limit or preclude a plaintiff from securing or recovering any other available remedy.", "tags": ["Incentives: Fines"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/2770", "official_name": "Protect Elections from Deceptive AI Act", "label": "safe"}
{"id": "517_10", "doc_id": "517", "text": "(b) Severability.—If any provision of this Act, or an amendment made by this Act, or the application of such provision to any person or circumstance, is held to be invalid, the remainder of this Act, or an amendment made by this Act, or the application of such provision to other persons or circumstances, must avoid be affected.", "tags": ["Harms: Detrimental content", "Risk factors: Reliability", "Applications: Broadcasting and media production", "Harms: Detrimental content", "Applications: Government: other applications/unspecified", "Applications: Broadcasting and media production", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://www.congress.gov/bill/118th-congress/senate-bill/2770", "official_name": "Protect Elections from Deceptive AI Act", "label": "safe"}
{"id": "573_6", "doc_id": "573", "text": "SEC. 4. CIVIL consequence.\n\n\n(a) In General.—The Attorney General may bring an action for a civil consequence of not more than $5,000,000 per infraction per day against any social media company that does not submit a report required under section 3.\n\n\n(b) Violations.—A social media company shall be considered in infraction of the provisions of this chapter for each day the social media company does any of the following:\n\n\n(1) Fails to post terms of service in accordance with section 2.\n\n\n(2) Fails to timely submit to the Attorney General a report required pursuant to section 3.\n\n\n(3) Materially omits or misrepresents required information in a report submitted pursuant to section 3.", "tags": ["Applications: Broadcasting and media production", "Incentives: Civil liability", "Incentives: Fines"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/6463", "official_name": "STOP HATE Act of 2023", "label": "safe"}
{"id": "758_3", "doc_id": "758", "text": "Today, these seven leading AI companies are committing to:  \n\n\nEnsuring Products are Safe Before Introducing Them to the Public\n* The companies commit to internal and external security testing of their AI systems before their release. This testing, which will be carried out in part by independent experts, guards against some of the most significant sources of AI risks, such as biosecurity and cybersecurity, as well as its broader societal effects.\n* The companies commit to sharing information across the industry and with governments, civil society, and academia on managing AI risks. This includes best practices for safety, information on attempts to circumvent safeguards, and technical collaboration.\n\n\nBuilding Systems that Put Security First\n* The companies commit to investing in cybersecurity and insider threat safeguards to protect proprietary and unreleased model weights. These model weights are the most essential part of an AI system, and the companies agree that it is vital that the model weights be released only when intended and when security risks are considered.\n* The companies commit to facilitating third-party discovery and reporting of vulnerabilities in their AI systems. Some issues may persist even after an AI system is released and a robust reporting mechanism enables them to be found and fixed quickly.\n\n\nEarning the Public’s Trust\n* The companies commit to developing robust technical mechanisms to ensure that users know when content is AI generated, such as a watermarking system. This action enables creativity with AI to flourish but reduces the dangers of fraud and deception.\n* The companies commit to publicly reporting their AI systems’ capabilities, limitations, and areas of appropriate and inappropriate use. This report will cover both security risks and societal risks, such as the effects on fairness and bias.\n* The companies commit to prioritizing research on the societal risks that AI systems can pose, including on avoiding harmful bias and discrimination, and protecting privacy. The track record of AI shows the insidiousness and prevalence of these dangers, and the companies commit to rolling out AI that mitigates them.   \n* The companies commit to develop and deploy advanced AI systems to help address society’s greatest challenges. From cancer prevention to mitigating climate change to so much in between, AI—if properly managed—can contribute enormously to the prosperity, equality, and security of all.", "tags": ["Strategies: Evaluation: External auditing", "Strategies: Evaluation: Conformity assessment", "Strategies: Evaluation", "Risk factors: Transparency", "Strategies: Disclosure", "Strategies: Convening", "Strategies: Disclosure: About incidents", "Risk factors: Security: Cybersecurity", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure: In deployment", "Risk factors: Bias", "Harms: Violation of civil or human rights, including privacy", "Risk factors: Privacy"], "source": "https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/", "official_name": "Voluntary Commitments from Leading Artificial Intelligence Companies", "label": "safe"}
{"id": "758_5", "doc_id": "758", "text": "Today’s announcement is part of a broader commitment by the Biden-Harris Administration to ensure AI is developed safely and responsibly, and to protect Americans from harm and discrimination.\n\n\n* Earlier this month, Vice President Harris convened consumer protection, labor, and civil rights leaders to discuss risks related to AI and reaffirm the Biden-Harris Administration’s commitment to protecting the American public from harm and discrimination.\n* Last month, President Biden met with top experts and researchers in San Francisco as part of his commitment to seizing the opportunities and managing the risks posed by AI, building on the President’s ongoing engagement with leading AI experts.\n* In May, the President and Vice President convened the CEOs of four American companies at the forefront of AI advancement—Google, Anthropic, Microsoft, and OpenAI—to underscore their responsibility and emphasize the importance of driving responsible, trustworthy, and ethical advancement with safeguards that mitigate risks and potential harms to individuals and our society. At the companies’ request, the White House hosted a subsequent meeting focused on cybersecurity threats and best practices.\n* The Biden-Harris Administration published a landmark Blueprint for an AI Bill of Rights to safeguard Americans’ rights and safety, and U.S. government agencies have ramped up their efforts to protect Americans from the risks posed by AI, including through preventing algorithmic bias in home valuation and leveraging existing enforcement authorities to protect people from unlawful bias, discrimination, and other harmful outcomes.\n* President Biden signed an Executive Order that directs federal agencies to root out bias in the design and use of new technologies, including AI, and to protect the public from algorithmic discrimination.\n* Earlier this year, the National Science Foundation announced a $140 million investment to establish seven new National AI Research Institutes, bringing the total to 25 institutions across the country.\n* The Biden-Harris Administration has also released a National AI exploratory work Strategic Plan to advance responsible AI.\n* The Office of Management and Budget will soon release draft policy guidance for federal agencies to ensure the development, procurement, and use of AI systems is centered around safeguarding the American people’s rights and safety.", "tags": ["Harms: Violation of civil or human rights, including privacy", "Risk factors: Bias", "Harms: Discrimination", "Risk factors: Reliability", "Harms: Harm to health/safety"], "source": "https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/", "official_name": "Voluntary Commitments from Leading Artificial Intelligence Companies", "label": "safe"}
{"id": "771_5", "doc_id": "771", "text": "A3.4 Document an evaluation plan for each of the performance metrics and error types.  Tags: Ongoing Evaluation Checkpoint. \nA3.5 Use the methods defined in requirement A3.4 to conduct evaluations. Document the pre-release results of the evaluations. Determine and document how often ongoing evaluation should be conducted to continue supporting this Goal. \nTags: Ongoing Evaluation Checkpoint. \nA3.6 Provide documentation to customers which describes the systemﾕs:   \n1. intended uses, and \n2. evidence that the system is fit for purpose for each intended use. \nWhen the system is a platform service made available to external customers or partners, include this information in the required Transparency Note. \nTags: Transparency Note. \nA3.7 If an intended use is not supported by evidence, or if evidence comes to light that refutes that the system is fit for purpose for the intended use at any point in the systemﾕs use: \n1. remove the intended use from customer-facing materials and make current customers aware of the issue, take action to close the identified gap, or discontinue the system, 2) revise documentation related to the intended use, and \n2. publish the revised documentation to customers. \nWhen the system is a platform service made available to external customers or partners, include this information in the required Transparency Note. \nA3.8 Communicate with care about system benefits; follow any applicable guidance from your attorney.", "tags": ["Harms: Discrimination", "Strategies: Disclosure: About evaluation", "Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Risk factors: Transparency", "Strategies: Evaluation: Impact assessment", "Strategies: Disclosure: In deployment"], "source": "https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-Responsible-AI-Standard-v2-General-Requirements-3.pdf", "official_name": "Microsoft Responsible AI Standard, v2", "label": "safe"}
{"id": "771_7", "doc_id": "771", "text": "Goal A5: Human oversight and control \n\nMicrosoft AI systems include capabilities that support informed human oversight and control.  \nApplies to: All AI systems. \nRequirements \nA5.1 Identify the stakeholders who are responsible for troubleshooting, managing, operating, overseeing, and controlling the system during and after deployment. Document these stakeholders and their oversight and control responsibilities using the Impact Assessment template.  \nTags: Impact Assessment. \nA5.2 Identify the system elements (including system UX, features, alerting and reporting functions, and educational materials) necessary for stakeholders identified in requirement A5.1 to effectively understand their oversight responsibilities and carry them out.  Stakeholders must be able to understand:  1) the systemﾕs intended uses,  \n1. how to effectively execute interactions with the system,  \n2. how to interpret system behavior,  \n3. when and how to override, intervene, or interrupt the system, and  \n4. how to remain aware of the possible tendency of over-relying on outputs produced by the system (ﾒautomation biasﾓ).  \nDocument the system design elements that will support relevant stakeholders for each oversight and control function.", "tags": ["Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Risk factors: Transparency", "Risk factors: Bias", "Risk factors: Interpretability and explainability", "Strategies: Evaluation: Conformity assessment"], "source": "https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-Responsible-AI-Standard-v2-General-Requirements-3.pdf", "official_name": "Microsoft Responsible AI Standard, v2", "label": "safe"}
{"id": "771_12", "doc_id": "771", "text": "T2.3 Review and update documentation annually or when any of the following events occur: \n1. new uses are added, \n2. functionality changes, \n3. the product moves to a new release stage, \n4. new information about reliable and safe performance becomes known as defined by requirement RS3.3, or 5) new information about system accuracy and performance becomes available. \nWhen the system is a platform service made available to external customers or partners, include this information in the required Transparency Note. \nTags: Transparency Note.", "tags": ["Strategies: Disclosure", "Strategies: Disclosure: In standard form", "Strategies: Evaluation: Post-market monitoring", "Strategies: Evaluation", "Risk factors: Transparency"], "source": "https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-Responsible-AI-Standard-v2-General-Requirements-3.pdf", "official_name": "Microsoft Responsible AI Standard, v2", "label": "safe"}
{"id": "771_16", "doc_id": "771", "text": "F1.5 Evaluate the system according to the defined Responsible Release Criteria.  Tags: Ongoing Evaluation Checkpoint. \nF1.6 Reassess the system design, including the choice of training data, features, objective function, and training algorithm, to pursue the goals of:  \n1. improving performance for any identified demographic group that does not meet any target minimum performance level, and  \n2. minimizing performance differences between identified demographic groups, paying particular attention to those that exceed the target maximum, while recognizing that doing so may appear to affect system performance and that it is seldom clear how to make such tradeoffs.  \nConsult with your attorney to determine your approach to this, including how you will identify and document tradeoffs.  \nTags: Ongoing Evaluation Checkpoint. \nF1.7 Identify and document any justifiable factors, such as circumstantial and other operational factors (e.g., ﾒbackground noiseﾓ for speech recognition systems or ﾒimage resolutionﾓ for facial recognition systems), that account for:  \n1. any inability to meet any target minimum performance level for any identified demographic group, and  2) any remaining performance differences between identified demographic groups. Tags: Ongoing Evaluation Checkpoint. \nF1.8 Document the pre-release results from requirements F1.4, F1.5, and F1.6. Determine and document how often ongoing evaluation should be conducted to continue supporting this Goal. \nTags: Ongoing Evaluation Checkpoint.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Harms: Discrimination", "Risk factors: Bias", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation"], "source": "https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-Responsible-AI-Standard-v2-General-Requirements-3.pdf", "official_name": "Microsoft Responsible AI Standard, v2", "label": "safe"}
{"id": "771_18", "doc_id": "771", "text": "Recommendation F1.5.1 Use the Fairlearn Python toolkitﾕs assessment and mitigation capabilities, if appropriate for the system. \nRecommendation F1.5.2 Use Error Analysis to help understand factors that may account for performance levels and differences, if appropriate for the system. \nRecommendation F1.5.3 Use one or more techniques available as part of the Interpret ML toolkit to help understand factors that may account for performance levels and differences, if appropriate for the system. \nRecommendation F1.6.1 Use the Fairlearn Python toolkitﾕs assessment and mitigation capabilities, if appropriate for the system. \nRecommendation F1.6.2 Be prepared to collect additional training data for identified demographic groups. \nRecommendation F1.7.1 Use Error Analysis to help understand factors that may account for performance levels and differences, if appropriate for the system.  \nRecommendation F1.7.2 Use one or more techniques available as part of the Interpret ML toolkit to help understand factors that may account for performance levels and differences, if appropriate for the system.", "tags": ["Risk factors: Bias", "Risk factors: Interpretability and explainability", "Risk factors: Reliability", "Harms: Discrimination"], "source": "https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-Responsible-AI-Standard-v2-General-Requirements-3.pdf", "official_name": "Microsoft Responsible AI Standard, v2", "label": "safe"}
{"id": "771_20", "doc_id": "771", "text": "F2.3 Define and document the evaluation that you will perform to support this Goal. Include: 1) any system components to be evaluated, in addition to the whole system, \n1. the metrics to be used to evaluate the system components and the whole system, and 3) the data set to be used for this evaluation. \nTags: Ongoing Evaluation Checkpoint. \nF2.4 Define and document Responsible Release Criteria to achieve this Goal, as follows: \nFor each metric, document the target maximum difference (absolute or relative) between the rates at which resources and opportunities are allocated to groups. \nTags: Ongoing Evaluation Checkpoint. \nF2.5 Evaluate the system according to the defined Responsible Release Criteria.  Tags: Ongoing Evaluation Checkpoint. \nF2.6 Reassess the system design, including the choice of training data, features, objective function, and training algorithm, to pursue the goal of minimizing differences between the rates at which resources and opportunities are allocated to identified demographic groups, paying particular attention to those that exceed the target maximum difference, while recognizing that doing so may appear to affect system performance and it is seldom clear how to make such trade-offs.  \nConsult with your attorney to determine your approach to this, including how you will identify and document trade-offs. \nTags: Ongoing Evaluation Checkpoint.", "tags": ["Strategies: Evaluation", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Performance requirements", "Strategies: Evaluation: Impact assessment"], "source": "https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-Responsible-AI-Standard-v2-General-Requirements-3.pdf", "official_name": "Microsoft Responsible AI Standard, v2", "label": "safe"}
{"id": "771_23", "doc_id": "771", "text": "Recommendation F2.5.1 Use the Fairlearn Python toolkitﾕs assessment and mitigation capabilities, if appropriate for the system.  \nRecommendation F2.5.2 Use Error Analysis to help understand factors that may account for differences between the rates at which resources and opportunities are allocated to the identified demographic groups, if appropriate for the system.  \nRecommendation F2.5.3 Use one or more techniques available as part of the Interpret ML toolkit to help understand factors that may account for differences between the rates at which resources and opportunities are allocated to the identified demographic groups, if appropriate for the system. \nRecommendation F2.6.1 Use the Fairlearn Python toolkitﾕs assessment and mitigation capabilities, if appropriate for the system.  \nRecommendation F2.7.1 Use Error Analysis to help understand factors that may account for differences between the rates at which resources and opportunities are allocated to the identified demographic groups, if appropriate for the system.  \nRecommendation F2.7.2 Use Interpret ML to help understand factors that may account for differences between the rates at which resources and opportunities are allocated to the identified demographic groups, if appropriate for the system.", "tags": ["Risk factors: Bias", "Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Evaluation: Conformity assessment", "Harms: Discrimination"], "source": "https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-Responsible-AI-Standard-v2-General-Requirements-3.pdf", "official_name": "Microsoft Responsible AI Standard, v2", "label": "safe"}
{"id": "771_25", "doc_id": "771", "text": "F3.4 Evaluate the system according to the plan defined in requirement F3.3.  Tags: Ongoing Evaluation Checkpoint. \nF3.5 Reassess the system design, including the choice of training data, features, objective function, and training algorithm, to pursue the goal of minimizing the potential for stereotyping, demeaning, and erasing the identified demographic groups.  \nTags: Ongoing Evaluation Checkpoint. \nF3.6 Document the pre-release results from requirements F3.4 and F3.5. Determine and document how often ongoing evaluation should be conducted to continue supporting this goal. \nTags: Ongoing Evaluation Checkpoint. \nF3.7 Publish information for customers about these risks involving identified demographic groups. When the system is a platform service made available to external customers or partners, include this information in the required Transparency Note. \nTags: Transparency Note.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: About evaluation", "Strategies: Disclosure: About incidents"], "source": "https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-Responsible-AI-Standard-v2-General-Requirements-3.pdf", "official_name": "Microsoft Responsible AI Standard, v2", "label": "safe"}
{"id": "771_30", "doc_id": "771", "text": "Goal RS2: Failures and remediations \n\nMicrosoft AI systems are designed to minimize the time to remediation of predictable or known failures. \nApplies to: All AI systems. \nRequirements \nRS2.1 Define predictable failures, including false positive and false negative results for the system as a whole and how they would impact stakeholders for each intended use. Use the Impact Assessment template to document any adverse impacts of these failures on stakeholders. \nTags: Impact Assessment. \nRS2.2 For each case of a predictable failure likely to have an adverse impact on a stakeholder, document the failure management approach: \n1. When possible, design and build the system to avoid this failure. Describe the design solution. Estimate the time range for resolving predictable failures for each designed solution or indicate that the failure will be prevented by design.  \n2. When a failure cannot be prevented by design, build a fallback option that may be used when this failure occurs. Describe the fallback option and document the estimated time required to invoke and use the fallback option. \n3. Provide training and documentation for stakeholders accountable for system oversight that supports their resolution of the failure. Describe the documentation and training.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Impact assessment", "Strategies: Disclosure", "Strategies: Disclosure: About incidents", "Risk factors: Reliability", "Risk factors: Reliability: Robustness"], "source": "https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-Responsible-AI-Standard-v2-General-Requirements-3.pdf", "official_name": "Microsoft Responsible AI Standard, v2", "label": "safe"}
{"id": "771_33", "doc_id": "771", "text": "RS3.3 When new uses, critical operational factors, or changes in the supported range of an operational factor are identified, determine whether any new use or operational factor can be supported with the existing system, will be supported but require additional work, or will not be supported.  \n･ When new uses or operational factors identified are to be supported, evaluate the updated system in accordance with requirement RS1.6, add the new intended use to the Impact Assessment, and publish updated communication in accordance with requirement RS1.9. \n･ When these new uses or operational factor range changes cannot or will not be accommodated to ensure reliable and safe performance of the system update customer documentation described in RS1.9 to include the new use as an unsupported use. \nWhen the system is a platform service made available to external customers or partners, include this information in the required Transparency Note. \nTags: Impact Assessment, Transparency Note. \nRS3.4 When a system is to be used for a Sensitive Use that imposes qualification or quality control requirements beyond the intended uses and/or operational factor ranges, conduct an evaluation specific to this use. If the required Responsible Release Criteria cannot be met, the Office of Responsible AI will review the results and decide how to proceed. Document any changes to the Responsible Release Criteria and document the results of evaluation.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Conformity assessment", "Strategies: Disclosure", "Strategies: Disclosure: In deployment", "Strategies: Evaluation: Post-market monitoring"], "source": "https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-Responsible-AI-Standard-v2-General-Requirements-3.pdf", "official_name": "Microsoft Responsible AI Standard, v2", "label": "safe"}
{"id": "771_34", "doc_id": "771", "text": "RS3.5 Conduct all evaluations tagged as Ongoing Evaluation Checkpoints in other Goals on an ongoing basis. \nRS3.6 If there are targets in Ongoing Evaluation Checkpoints that are no longer satisfied, consult with named reviewers, and in the case of Sensitive Uses, with the Office of Responsible AI, to develop and implement a plan to close any gaps. Document the process, its results, and conclusions. \nRS3.7 If evidence comes to light that refutes the system is fit for purpose for an intended use at any point in the systemﾕs use: \n1. remove the intended use from customer-facing materials and make current customers aware of the issue, take action to close the identified gap, or discontinue the system, 2) revise documentation related to the intended use, and 3) publish the revised documentation to customers. \nWhen the system is a platform service made available to external customers or partners, include this information in the required Transparency Note. \nTags: Transparency Note. \nRS3.8: Review and update documentation required by Goal T2 when any of the following events occur: \n1. new uses are added, \n2. functionality changes, \n3. new information about reliable and safe performance becomes known as defined by requirement RS3.3, or 4) new information about system accuracy and performance becomes available. \nWhen the system is a platform service made available to external customers or partners, include this information in the required Transparency Note. \nTags: Transparency Note. \nRS3.9: Escalate unresolved issues related to supporting the Standard and its requirements to the Office of Responsible AI.", "tags": ["Strategies: Evaluation", "Strategies: Evaluation: Post-market monitoring", "Strategies: Disclosure", "Risk factors: Transparency", "Strategies: Disclosure: About incidents"], "source": "https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-Responsible-AI-Standard-v2-General-Requirements-3.pdf", "official_name": "Microsoft Responsible AI Standard, v2", "label": "safe"}
{"id": "852_1", "doc_id": "852", "text": "H. R. 8315\n\n\nTo amend the Export Control Reform Act of 2018 to prevent foreign adversaries from exploiting United States artificial intelligence and other enabling technologies, and for other purposes.\n\n\nIN THE HOUSE OF REPRESENTATIVES\nMay 8, 2024\nMr. McCaul (for himself, Mr. Moolenaar, Mr. Krishnamoorthi, and Ms. Wild) introduced the following bill; which was referred to the Committee on Foreign Affairs\n\n\nA BILL\nTo amend the Export Control Reform Act of 2018 to prevent foreign adversaries from exploiting United States artificial intelligence and other enabling technologies, and for other purposes.\n\n\nBe it enacted by the Senate and House of Representatives of the United States of America in Congress assembled,", "tags": ["Harms: Harm to health/safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Input controls", "Strategies: Licensing, registration, and certification", "Strategies: Input controls"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/8315", "official_name": "ENFORCE Act", "label": "safe"}
{"id": "852_2", "doc_id": "852", "text": "SECTION 1. SHORT TITLE AND TABLE OF CONTENTS.\n\n\n(a) Short Title.—This Act may be cited as the “Enhancing National Frameworks for Overseas limitation of Critical Exports Act” or “ENFORCE Act”.\n\n\n(b) Table Of Contents.—The table of contents for this Act is as follows:\n\n\nSec. 1. Short title and table of contents.\nSec. 2. Definitions.\nSec. 3. Authority of the President.\nSec. 4. Additional authorities.\nSec. 5. Amendment to International Emergency Economic Powers Act.", "tags": ["Harms: Harm to health/safety", "Risk factors: Security", "Risk factors: Security: Cybersecurity", "Strategies: Input controls", "Strategies: Licensing, registration, and certification", "Strategies: Input controls"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/8315", "official_name": "ENFORCE Act", "label": "safe"}
{"id": "852_4", "doc_id": "852", "text": "SEC. 3. AUTHORITY OF THE PRESIDENT.\n\n\nSection 1753(a) of the Export Control Reform Act of 2018 (50 U.S.C. 4812(a)) is amended by adding at the end the following:\n\n\n“In addition, in order to carry out the policy set forth in paragraphs (1) through (10) of section 1752, the President may control the activities of United States persons, wherever located, relating to specific covered artificial intelligence systems and emerging and foundational technologies that are identified as essential to the national security of the United States pursuant to section 1758(a).”.", "tags": ["Strategies: Input controls"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/8315", "official_name": "ENFORCE Act", "label": "safe"}
{"id": "852_5", "doc_id": "852", "text": "SEC. 4. ADDITIONAL AUTHORITIES.\n\n\nSection 1754(d) of the Export Control Reform Act of 2018 (50 U.S.C. 4813(d)) is amended by adding at the end the following:\n\n\n“(3) ADDITIONAL AUTHORITIES.—In furtherance of section 1753(a), the President may require a United States person, wherever located, to apply for and receive a license from the Department of Commerce for—\n\n\n“(A) the export, reexport, or in-country transfer of items described in paragraph (4), including items that are not subject to control under this subchapter; and\n\n\n“(B) other activities that may support the design, development, production, use, operation, installation, maintenance, repair, overhaul, or refurbishing of, or for the performance of services relating to, any items described in paragraph (4).\n\n\n“(4) ITEMS DESCRIBED.—The items described in this paragraph include—\n\n\n“(A) covered artificial intelligence systems; and\n\n\n“(B) specific emerging and foundational technologies that are identified as essential to the national security of the United States pursuant to section 1758(a).”.", "tags": ["Strategies: Licensing, registration, and certification"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/8315", "official_name": "ENFORCE Act", "label": "safe"}
{"id": "852_6", "doc_id": "852", "text": "SEC. 5. AMENDMENT TO INTERNATIONAL EMERGENCY ECONOMIC POWERS ACT.\n\n\nSection 203(b)(3) of the International Emergency Economic Powers Act (50 U.S.C. 1702(b)(3)) is amended by striking “section 5 of the Export Administration Act of 1979, or under section 6 of such Act to the extent that such controls promote the nonproliferation or antiterrorism policies of the United States” and inserting “section 1754 of the Export Control Reform Act of 2018”.", "tags": ["Strategies: Input controls"], "source": "https://www.congress.gov/bill/118th-congress/house-bill/8315", "official_name": "ENFORCE Act", "label": "safe"}
