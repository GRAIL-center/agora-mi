model_id: "meta-llama/Meta-Llama-3-8B-Instruct"
device: "cuda"
dtype: "bfloat16"
batch_size: 16  # Scaled for A100 80GB
max_length: 1024
seed: 0
layers: [16, 20, 24, 28, 30]  # Llama-3-8B has 32 layers

# SAE (sae-lens)
sae_release: "llama_scope_8b_pt_res"  # Placeholder: check latest SAELens release for Llama 3 SAEs
sae_id_template: "layer_{L}/width_16k/canonical"

# Top features / thresholds
topk: 64
delta_thresh: null
edge_tau: 0.1

# Statistics
bootstrap_B: 2000
perm_N: 10000
fdr_q: 0.05

# Data paths
input_dir: "data/raw/agora"
processed_dir: "data/processed"
results_dir: "results/llama-3-8b-instruct"
logs_dir: "logs/llama-3-8b-instruct"

# Prompt template path
prompt_config: "configs/prompt_templates.yaml"
use_chat_template: true
